{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'joy' is already a single token (ID: 2633)\n",
      "'love' is already a single token (ID: 23205)\n",
      "'anger' is already a single token (ID: 2564)\n",
      "\n",
      "Added 6 new tokens to the tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d1/grad/mha361/anaconda3/envs/memit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "import random\n",
    "import numpy as np\n",
    "import torch  # if you're using PyTorch\n",
    "# import tensorflow as tf  # if you're using TensorFlow\n",
    "\n",
    "# Set random seed\n",
    "seed_value = 42  # or any other integer\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():  # PyTorch-specific\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = '[Label]'\n",
    "\n",
    "# Add the special tokens to the tokenizer\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "special_tokens_dict = {}\n",
    "new_tokens = []\n",
    "label2text = dataset['train'].features['label'].names\n",
    "\n",
    "for label in label2text:\n",
    "    # Create special token format (with and without space)\n",
    "    special_token = f'[{label}]'\n",
    "    special_token_with_space = f'[{label}]'\n",
    "    \n",
    "    # Check if the label is already a single token in the tokenizer\n",
    "    label_tokens = tokenizer.encode(label, add_special_tokens=False)\n",
    "    is_single_token = len(label_tokens) == 1\n",
    "    \n",
    "    if is_single_token:\n",
    "        print(f\"'{label}' is already a single token (ID: {label_tokens[0]})\")\n",
    "    \n",
    "    # Add both versions to new tokens list\n",
    "    new_tokens.extend([special_token])\n",
    "\n",
    "# Add the tokens to the tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"\\nAdded {num_added_tokens} new tokens to the tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "from typing import Dict, Union, Any\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get input IDs and create label masks\n",
    "        input_ids = inputs.get(\"input_ids\")\n",
    "        attention_mask = inputs.get(\"attention_mask\")\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Create label masks for each sequence in the batch\n",
    "        label_masks = []\n",
    "        for sequence in input_ids:\n",
    "            # Find the position of [Label] token\n",
    "            label_start = (sequence == tokenizer.convert_tokens_to_ids(\"[Label]\")).nonzero(as_tuple=True)[0]\n",
    "            if len(label_start) > 0:\n",
    "                # Create mask that's 1 for tokens after [Label] and 0 elsewhere\n",
    "                mask = torch.zeros_like(sequence)\n",
    "                mask[label_start[0]:] = 1\n",
    "                label_masks.append(mask)\n",
    "            else:\n",
    "                # If no [Label] token found, mask everything\n",
    "                label_masks.append(torch.zeros_like(sequence))\n",
    "        \n",
    "        label_masks = torch.stack(label_masks)\n",
    "        \n",
    "        # Shift input_ids and labels for language modeling\n",
    "        labels = input_ids.clone()\n",
    "        labels = labels[:, 1:].contiguous()\n",
    "        label_masks = label_masks[:, 1:].contiguous()\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, :-1, :].contiguous()\n",
    "        \n",
    "        # Compute loss only on label tokens\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # Apply label mask to loss\n",
    "        loss = loss.view(batch_size, -1) * label_masks\n",
    "        loss = loss.sum() / (label_masks.sum() + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd772c74c944381bfcf4eeb267fa60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da364a782624deabaa0c6b2e4fd1fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4614ce6984ba42f1bb3dbe13d37af4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62980888ff714a1181451b22010429d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82da918982e4ce2b7bd8d2c8d8cb5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2926ab34a157490793807f9f97e91e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1454067/1218215732.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weights_path))\n",
      "/u/amo-d1/grad/mha361/anaconda3/envs/memit/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 11:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.004898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.640800</td>\n",
       "      <td>0.003087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.640800</td>\n",
       "      <td>0.002693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_data(examples):\n",
    "    formatted_texts = []\n",
    "    \n",
    "    for text, label in zip(examples['text'], examples['label']):\n",
    "        tok_text = tokenizer.encode(text, max_length=70, truncation=True)\n",
    "        text = tokenizer.decode(tok_text)\n",
    "        label_str = dataset['train'].features['label'].int2str(label)  # Convert label to string\n",
    "        formatted_texts.append(f\"{text}[Label][{label_str}<|endoftext|>]\")\n",
    "    return {'formatted_text': formatted_texts}  # Create a new field for the formatted text\n",
    "\n",
    "# Apply formatting to the dataset\n",
    "formatted_dataset = dataset.map(format_data, batched=True)\n",
    "\n",
    "# Tokenize the formatted dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"formatted_text\"], padding='max_length', max_length = 180, truncation=True)\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Keep the original 'text' and 'label' columns intact\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"text\", \"label\"])\n",
    "\n",
    "from transformers import GPT2LMHeadModel as gt, Trainer, TrainingArguments\n",
    "from models.gpt2 import GPT2LMHeadModel\n",
    "# Load pre-trained GPT-2 model\n",
    "model1 = gt.from_pretrained('gpt2')\n",
    "\n",
    "model1.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model1.config.m_layer = 11\n",
    "import os\n",
    "\n",
    "base_path = os.path.join(\"model_weights\", 'gpt2-emotion-classification')\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "weights_path = os.path.join(base_path, \"weights.pth\")\n",
    "\n",
    "torch.save(model1.state_dict(), weights_path)\n",
    "\n",
    "model = GPT2LMHeadModel(model1.config)\n",
    "\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "# Modified training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-emotion-classification\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize custom trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'].remove_columns(['label', 'text', 'formatted_text']),\n",
    "    eval_dataset=tokenized_dataset['test'].remove_columns(['label', 'text', 'formatted_text']),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "# model.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.ln_f\n",
      "transformer.mask_layer\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for n,m in model.named_modules():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_generate(model, input_ids, attention_mask, max_length, class_token_index):\n",
    "    device = input_ids.device\n",
    "    batch_size = input_ids.shape[0]\n",
    "    \n",
    "    # Initialize the output tensor with the input_ids\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    # Create a tensor to keep track of which sequences have finished generating\n",
    "    finished_sequences = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - input_ids.shape[1]):\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=generated, attention_mask=attention_mask)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Apply greedy decoding (argmax)\n",
    "            next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Check if the class token is generated\n",
    "            class_token_generated = (next_tokens == class_token_index)\n",
    "            finished_sequences = finished_sequences | class_token_generated\n",
    "            \n",
    "            # Append the new tokens\n",
    "            generated = torch.cat([generated, next_tokens.unsqueeze(-1)], dim=-1)\n",
    "            \n",
    "            # Update attention mask\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones((batch_size, 1), dtype=torch.long, device=device)], dim=1)\n",
    "            \n",
    "            # Break if all sequences have finished\n",
    "            if torch.all(finished_sequences):\n",
    "                break\n",
    "    \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nethook\n",
    "def manual_generate(model, tokenizer, input_ids, attention_mask, max_length):\n",
    "    device = input_ids.device\n",
    "    batch_size = input_ids.shape[0]\n",
    "    \n",
    "    # Initialize the output tensor with the input_ids\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    # Create a tensor to keep track of which sequences have finished generating\n",
    "    finished_sequences = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "    \n",
    "    confidences = []\n",
    "    \n",
    "    all_fc_vals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - input_ids.shape[1]):\n",
    "            # Forward pass\n",
    "            with nethook.TraceDict(model, ['transformer.mask_layer']) as ret:\n",
    "                outputs = model(input_ids=generated, attention_mask=attention_mask)\n",
    "                fc1_vals = [\n",
    "                        ret[layer_fc1_vals].output[:,-1,:]#.transpose(0, 1)//works without transpose somehow\n",
    "                        for layer_fc1_vals in ret\n",
    "                    ]\n",
    "                all_fc_vals.append(fc1_vals)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Apply greedy decoding (argmax)\n",
    "            next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # append the confidence of the predicted token\n",
    "            confidences.append(torch.nn.functional.softmax(next_token_logits, dim=-1).max(dim=-1).values)\n",
    "            \n",
    "            # Check if the EOS token is generated\n",
    "            eos_token_generated = (next_tokens == tokenizer.eos_token_id)\n",
    "            finished_sequences = finished_sequences | eos_token_generated\n",
    "            \n",
    "            # Replace next token with EOS token if the sequence is finished\n",
    "            next_tokens = torch.where(finished_sequences, tokenizer.eos_token_id, next_tokens)\n",
    "            \n",
    "            # Append the new tokens\n",
    "            generated = torch.cat([generated, next_tokens.unsqueeze(-1)], dim=-1)\n",
    "            \n",
    "            # Update attention mask\n",
    "            attention_mask = torch.cat([attention_mask, (~finished_sequences).unsqueeze(-1).long()], dim=1)\n",
    "            \n",
    "            # Break if all sequences have finished\n",
    "            if torch.all(finished_sequences):\n",
    "                break\n",
    "    \n",
    "    return generated, torch.stack(confidences, dim=1), all_fc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2000 [00:00<?, ?it/s]/tmp/ipykernel_1960896/603130676.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item['input_ids']).unsqueeze(0).to(device)\n",
      "/tmp/ipykernel_1960896/603130676.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item['attention_mask']).unsqueeze(0).to(device)\n",
      "Evaluating: 100%|██████████| 2000/2000 [24:50<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7980\n",
      "Classification Report:\n",
      "confidence:  tensor(0.7880, device='cuda:0')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.82      0.83      0.83       581\n",
      "       anger       0.77      0.90      0.83       275\n",
      "         joy       0.80      0.80      0.80       695\n",
      "    surprise       0.77      0.76      0.76        66\n",
      "        love       0.71      0.45      0.55       159\n",
      "        fear       0.80      0.84      0.82       224\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.78      0.76      0.77      2000\n",
      "weighted avg       0.80      0.80      0.79      2000\n",
      "\n",
      "\n",
      "Sample of True Labels: [' sadness', ' sadness', ' sadness', ' joy', ' sadness', ' fear', ' anger', ' joy', ' joy', ' anger']\n",
      "Sample of Predicted Labels: [' sadness', ' sadness', ' sadness', ' sadness', ' sadness', ' fear', ' anger', ' joy', ' joy', ' sadness']\n",
      "\n",
      "Sample of reconstructed texts:\n",
      "Sample 0: im feeling rather rotten so im not very ambitious right now [Label]  sadness<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Sample 1: im updating my blog because i feel shitty [Label]  sadness<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Sample 2: i never make her separate from me because i don t ever want her to feel like i m ashamed with her [Label]  sadness<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Sample 3: i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived [Label]  joy<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Sample 4: i was feeling a little vain when i did this one [Label]  sadness<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "Total samples processed: 2000\n",
      "Unique true labels: {' sadness', ' anger', ' joy', ' surprise', ' love', ' fear'}\n",
      "Unique predicted labels: {' sadness', ' anger', ' joy', ' surprise', ' love', ' fear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),\n",
    "        'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),\n",
    "    }\n",
    "\n",
    "def evaluate_gpt2_classification(model, eval_dataset, tokenizer, batch_size=1):\n",
    "    \n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id \n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    confidence = 0\n",
    "    j = 0 \n",
    "    all_hidden = []\n",
    "    for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
    "        input_ids = torch.tensor(item['input_ids']).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(item['attention_mask']).unsqueeze(0).to(device)\n",
    "        \n",
    "        generated_sequences, confidences, fc_vals = manual_generate(model,tokenizer,input_ids,attention_mask,150)\n",
    "        \n",
    "        generated_sequences = generated_sequences[:, input_ids.shape[1]:][0]\n",
    "        \n",
    "        \n",
    "\n",
    "        label_token_ids = tokenizer.encode('[Label]', add_special_tokens=False)\n",
    "        label_len = len(label_token_ids)\n",
    "\n",
    "        label_positions = []\n",
    "\n",
    "        for i in range(len(generated_sequences) - label_len + 1):\n",
    "            if generated_sequences[i:i+label_len].tolist() == label_token_ids:\n",
    "                label_positions.append(i)\n",
    "                break\n",
    "\n",
    "\n",
    "        for pos in label_positions:\n",
    "            predicted_label = tokenizer.decode(generated_sequences[pos+1])\n",
    "            \n",
    "            hidden_dim = fc_vals[pos+1]\n",
    "            confidence += confidences[0][pos+1]\n",
    "            j += 1\n",
    "\n",
    "        \n",
    "        all_hidden.append(hidden_dim[0][0][0])\n",
    "        full_text = tokenizer.decode(input_ids[0])\n",
    "        true_label = full_text.split(\"[Label] \")[1].split(\"<|endoftext|>\")[0]\n",
    "\n",
    "        all_predictions.append(predicted_label)\n",
    "        all_labels.append(true_label)\n",
    "    \n",
    "    if not all_labels or not all_predictions:\n",
    "        print(\"No labels were extracted. Check if '[Label]' token exists in the tokenized text.\")\n",
    "        return 0, \"No labels extracted\", [], []\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = list(set(all_labels + all_predictions))\n",
    "    \n",
    "    # Generate classification report\n",
    "    try:\n",
    "        report = classification_report(all_labels, all_predictions, labels=unique_labels, target_names=unique_labels)\n",
    "    except ValueError as e:\n",
    "        report = f\"Unable to generate classification report: {str(e)}\"\n",
    "    \n",
    "    return accuracy, confidence/j, all_hidden, report, all_labels, all_predictions \n",
    "\n",
    "# Use the function\n",
    "test_dataset = tokenized_dataset['test']\n",
    "accuracy, confidence, all_hidden, report, true_labels, predicted_labels = evaluate_gpt2_classification(model, test_dataset, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(\"confidence: \", confidence)\n",
    "print(report)\n",
    "\n",
    "# If you want to see the actual labels and predictions\n",
    "print(\"\\nSample of True Labels:\", true_labels[:10])\n",
    "print(\"Sample of Predicted Labels:\", predicted_labels[:10])\n",
    "\n",
    "# Check a few samples of the reconstructed text\n",
    "print(\"\\nSample of reconstructed texts:\")\n",
    "for i in range(5):\n",
    "    full_text = tokenizer.decode(test_dataset[i]['input_ids'])\n",
    "    print(f\"Sample {i}: {full_text}\")\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"\\nTotal samples processed: {len(true_labels)}\")\n",
    "print(f\"Unique true labels: {set(true_labels)}\")\n",
    "print(f\"Unique predicted labels: {set(predicted_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 4/250 [00:52<53:31, 13.06s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m     73\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 74\u001b[0m accuracy, report, true_labels, predicted_labels, confidence \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_gpt2_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mevaluate_gpt2_classification\u001b[0;34m(model, eval_dataset, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 24\u001b[0m     generated_sequences, confidences, all_fc_vals \u001b[38;5;241m=\u001b[39m \u001b[43mmanual_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m label_token_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Label]\u001b[39m\u001b[38;5;124m'\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m label_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(label_token_ids)\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mmanual_generate\u001b[0;34m(model, tokenizer, input_ids, attention_mask, max_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length \u001b[38;5;241m-\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m nethook\u001b[38;5;241m.\u001b[39mTraceDict(model, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.mask_layer\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m ret:\n\u001b[0;32m---> 20\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         fc1_vals \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m                 ret[layer_fc1_vals]\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;66;03m#.transpose(0, 1)//works without transpose somehow\u001b[39;00m\n\u001b[1;32m     23\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m layer_fc1_vals \u001b[38;5;129;01min\u001b[39;00m ret\n\u001b[1;32m     24\u001b[0m             ]\n\u001b[1;32m     25\u001b[0m         all_fc_vals\u001b[38;5;241m.\u001b[39mextend(fc1_vals)\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/models/gpt2.py:1346\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1346\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/models/gpt2.py:1168\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m   1158\u001b[0m         hidden_states,\n\u001b[1;32m   1159\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1166\u001b[0m     )\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm_layer):\n\u001b[0;32m-> 1168\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1170\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/models/gpt2.py:898\u001b[0m, in \u001b[0;36mMaskLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 898\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_bound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    899\u001b[0m     upper_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupper_bound\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    900\u001b[0m     replacement_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement_values\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate_gpt2_classification(model, eval_dataset, tokenizer, batch_size=8):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id \n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Create a DataLoader for batch processing\n",
    "    dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "    j = 0\n",
    "    confidence_t = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_sequences, confidences, all_fc_vals = manual_generate(model,tokenizer,input_ids,attention_mask,150)\n",
    "\n",
    "        label_token_ids = tokenizer.encode('[Label]', add_special_tokens=False)\n",
    "        label_len = len(label_token_ids)\n",
    "        endoftext_ids = tokenizer.encode('<|endoftext|>', add_special_tokens=False)\n",
    "        endoftext_len = len(endoftext_ids)\n",
    "\n",
    "        for sequence, orig_sequence, conf, fc_vals in zip(generated_sequences, input_ids, confidences, all_fc_vals[0]):\n",
    "            label_positions = []\n",
    "            endoftext_positions = []\n",
    "\n",
    "            for i in range(len(orig_sequence) - label_len + 1):\n",
    "                if orig_sequence[i:i+label_len].tolist() == label_token_ids:\n",
    "                    label_positions.append(i)\n",
    "\n",
    "            for i in range(len(orig_sequence) - endoftext_len + 1):\n",
    "                if orig_sequence[i:i+endoftext_len].tolist() == endoftext_ids:\n",
    "                    endoftext_positions.append(i)\n",
    "                    break\n",
    "\n",
    "            for pos, end in zip(label_positions, endoftext_positions):\n",
    "                predicted_label = tokenizer.decode(sequence[pos+1:end])\n",
    "                confidence_t += conf[pos+1:end].item()\n",
    "                token_fc = fc_vals[0][pos+1:end]\n",
    "                j += 1\n",
    "                \n",
    "\n",
    "            full_text = tokenizer.decode(orig_sequence)\n",
    "            true_label = full_text.split(\"[Label] \")[1].split(\"<|endoftext|>\")[0]\n",
    "\n",
    "            all_predictions.append(predicted_label)\n",
    "            all_labels.append(true_label)\n",
    "    \n",
    "    if not all_labels or not all_predictions:\n",
    "        print(\"No labels were extracted. Check if '[Label]' token exists in the tokenized text.\")\n",
    "        return 0, \"No labels extracted\", [], []\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    unique_labels = list(set(all_labels + all_predictions))\n",
    "    \n",
    "    try:\n",
    "        report = classification_report(all_labels, all_predictions, labels=unique_labels, target_names=unique_labels)\n",
    "    except ValueError as e:\n",
    "        report = f\"Unable to generate classification report: {str(e)}\"\n",
    "    \n",
    "    return accuracy, report, all_labels, all_predictions, confidence_t/j\n",
    "\n",
    "# Usage\n",
    "test_dataset = tokenized_dataset['test']\n",
    "accuracy, report, true_labels, predicted_labels, confidence = evaluate_gpt2_classification(model, test_dataset, tokenizer, batch_size=8)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "print(report)\n",
    "\n",
    "print(\"\\nSample of True Labels:\", true_labels[:10])\n",
    "print(\"Sample of Predicted Labels:\", predicted_labels[:10])\n",
    "\n",
    "print(\"\\nSample of reconstructed texts:\")\n",
    "for i in range(5):\n",
    "    full_text = tokenizer.decode(test_dataset[i]['input_ids'])\n",
    "    print(f\"Sample {i}: {full_text}\")\n",
    "\n",
    "print(f\"\\nTotal samples processed: {len(true_labels)}\")\n",
    "print(f\"Unique true labels: {set(true_labels)}\")\n",
    "print(f\"Unique predicted labels: {set(predicted_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: ['Company', 'EducationalInstitution', 'Artist', 'Athlete', 'OfficeHolder', 'MeanOfTransportation', 'Building', 'NaturalPlace', 'Village', 'Animal', 'Plant', 'Album', 'Film', 'WrittenWork']\n",
      "'Company' is already a single token (ID: 39154)\n",
      "'Artist' is already a single token (ID: 43020)\n",
      "'Building' is already a single token (ID: 25954)\n",
      "'Animal' is already a single token (ID: 40002)\n",
      "'Film' is already a single token (ID: 39750)\n",
      "\n",
      "Added 14 new tokens to the tokenizer\n",
      "\n",
      "Example tokenization:\n",
      "\n",
      "Tokenizing '[Company]':\n",
      "[50257]\n",
      "\n",
      "Tokenizing '[EducationalInstitution]':\n",
      "[50258]\n",
      "\n",
      "Tokenizing '[Artist]':\n",
      "[50259]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "dataset = load_dataset(\"fancyzhx/dbpedia_14\")\n",
    "dataset = dataset.rename_column(\"content\", \"text\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Get unique labels and their text representations\n",
    "label2text = dataset['train'].features['label'].names\n",
    "print(\"Original labels:\", label2text)\n",
    "\n",
    "# Create special tokens for each label\n",
    "special_tokens_dict = {}\n",
    "new_tokens = []\n",
    "\n",
    "for label in label2text:\n",
    "    # Create special token format (with and without space)\n",
    "    special_token = f'[{label}]'\n",
    "    special_token_with_space = f'[{label}]'\n",
    "    \n",
    "    # Check if the label is already a single token in the tokenizer\n",
    "    label_tokens = tokenizer.encode(label, add_special_tokens=False)\n",
    "    is_single_token = len(label_tokens) == 1\n",
    "    \n",
    "    if is_single_token:\n",
    "        print(f\"'{label}' is already a single token (ID: {label_tokens[0]})\")\n",
    "    \n",
    "    # Add both versions to new tokens list\n",
    "    new_tokens.extend([special_token])\n",
    "\n",
    "# Add the tokens to the tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"\\nAdded {num_added_tokens} new tokens to the tokenizer\")\n",
    "\n",
    "# Print some examples of the new tokens\n",
    "print(\"\\nExample tokenization:\")\n",
    "for label in label2text[:3]:  # Show first 3 labels as examples\n",
    "    special_token = f'[{label}]'\n",
    "    special_token_with_space = f'[{label} ]'\n",
    "    print(f\"\\nTokenizing '{special_token}':\")\n",
    "    print(tokenizer.encode(special_token))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
