{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d1/grad/mha361/anaconda3/envs/ml_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n",
      "Wiki dataset loaded for auxiliary task\n",
      "SQuAD dataset loaded for auxiliary task: 10570 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]\n",
      "/tmp/ipykernel_3666900/1853942014.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Consistency check - First example in dataset_sample: (CNN)The Palestinian Authority officially became t...\n",
      "Consistency check - First example in wiki_sample: Anarchism is a political philosophy and movement t...\n",
      "Consistency check - First example in squad_sample: Which NFL team represented the AFC at Super Bowl 5...\n",
      "Starting base evaluation on main task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.2741: 100%|██████████| 50/50 [37:17<00:00, 44.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting base evaluation on language modeling task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity: 6.2546: 100%|██████████| 200/200 [00:42<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting base evaluation on SQuAD task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA:   0%|          | 0/50 [00:00<?, ?it/s]/u/amo-d1/grad/mha361/anaconda3/envs/ml_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "EM: 0.5200, F1: 0.6063: 100%|██████████| 50/50 [00:21<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.2966: 100%|██████████| 50/50 [37:30<00:00, 45.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from utilities import mask_range_llma, compute_masks, reset_llma, evaluate_llma_summarization, evaluate_llma_language_modeling, evaluate_llma_squad\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Login to Hugging Face (if needed)\n",
    "login(\"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\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Load the CNN/Daily Mail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(\"Dataset loaded:\", dataset)\n",
    "\n",
    "# Load Wiki dataset for auxiliary task\n",
    "wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:500]\")\n",
    "print(\"Wiki dataset loaded for auxiliary task\")\n",
    "\n",
    "# Load SQuAD dataset for auxiliary task\n",
    "squad_dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "print(f\"SQuAD dataset loaded for auxiliary task: {len(squad_dataset)} examples\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "from models.lama import LlamaForCausalLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "model1.config.m_layer = 27\n",
    "\n",
    "import os\n",
    "# Prepare model directory\n",
    "base_path = os.path.join(\"model_weights\", 'llama-summarization')\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "weights_path = os.path.join(base_path, \"weights.pth\")\n",
    "# torch.save(model1.state_dict(), weights_path)\n",
    "model = LlamaForCausalLM(model1.config)\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# Main experiment\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.tensor\")\n",
    "\n",
    "# Setup parameters\n",
    "batch_size = 256\n",
    "mask_layer = 27\n",
    "percent = 0.4\n",
    "\n",
    "# Create tables for all tasks\n",
    "main_results_table = PrettyTable()\n",
    "main_results_table.field_names = [\"Masking Type\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\n",
    "\n",
    "aux_results_table = PrettyTable()\n",
    "aux_results_table.field_names = [\"Masking Type\", \"Perplexity\", \"Perplexity Increase %\"]\n",
    "\n",
    "squad_results_table = PrettyTable()\n",
    "squad_results_table.field_names = [\"Masking Type\", \"Exact Match\", \"F1 Score\", \"EM Drop\", \"F1 Drop\"]\n",
    "\n",
    "# Create smaller datasets for efficiency\n",
    "dataset_sample = dataset['test'].select(range(50))\n",
    "dataset_record = dataset['train'].select(range(50))  # For recording activations\n",
    "wiki_sample = wiki_dataset.select(range(200))  # For language modeling task\n",
    "squad_sample = squad_dataset.select(range(200))  # For question answering task\n",
    "\n",
    "# Consistency check\n",
    "print(f\"Consistency check - First example in dataset_sample: {dataset_sample[0]['article'][:50]}...\")\n",
    "print(f\"Consistency check - First example in wiki_sample: {wiki_sample[0]['text'][:50]}...\")\n",
    "print(f\"Consistency check - First example in squad_sample: {squad_sample[0]['question'][:50]}...\")\n",
    "\n",
    "print(\"Starting base evaluation on main task...\")\n",
    "model = reset_llma(model)\n",
    "base_scores, *_ = evaluate_llma_summarization(model, dataset_sample, tokenizer, max_samples=100)\n",
    "main_results_table.add_row([\n",
    "    \"Base (No Masking)\",\n",
    "    f\"{base_scores['rouge1']:.4f}\",\n",
    "    f\"{base_scores['rouge2']:.4f}\",\n",
    "    f\"{base_scores['rougeL']:.4f}\"\n",
    "])\n",
    "\n",
    "print(\"Starting base evaluation on language modeling task...\")\n",
    "base_aux_scores = evaluate_llma_language_modeling(model, wiki_sample, tokenizer, max_samples=2000)\n",
    "base_perplexity = base_aux_scores[\"perplexity\"]\n",
    "aux_results_table.add_row([\"Base (No Masking)\", f\"{base_perplexity:.4f}\", \"0%\"])\n",
    "\n",
    "print(\"Starting base evaluation on SQuAD task...\")\n",
    "base_squad_scores = evaluate_llma_squad(model, squad_sample, tokenizer, max_samples=50)\n",
    "squad_results_table.add_row([\n",
    "    \"Base (No Masking)\",\n",
    "    f\"{base_squad_scores['exact_match']:.4f}\",\n",
    "    f\"{base_squad_scores['f1']:.4f}\",\n",
    "    \"0.0000\",\n",
    "    \"0.0000\"\n",
    "])\n",
    "\n",
    "print(\"Recording activations...\")\n",
    "*_, fc_vals = evaluate_llma_summarization(model, dataset_record, tokenizer, max_samples=100)\n",
    "\n",
    "# Compute masks\n",
    "mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max, mask_max_random_off, mask_random = compute_masks(fc_vals, percent)\n",
    "\n",
    "# # MAX masking (complete)\n",
    "# print(\"Masking with MAX...\")\n",
    "# model = reset_llma(model)\n",
    "# tao = torch.inf\n",
    "# model = mask_range_llma(model, mask_max_low_std, fc_vals, tao)\n",
    "# t = int(mask_max_low_std.shape[0]-torch.count_nonzero(mask_max_low_std))\n",
    "# print(f\"Total Masked: {t}\")\n",
    "\n",
    "# # Evaluate MAX masking on main task\n",
    "# max_scores, *_ = evaluate_llma_summarization(model, dataset_sample, tokenizer, max_samples=100)\n",
    "# main_results_table.add_row([\n",
    "#     \"MAX Masking\",\n",
    "#     f\"{max_scores['rouge1']:.4f}\",\n",
    "#     f\"{max_scores['rouge2']:.4f}\",\n",
    "#     f\"{max_scores['rougeL']:.4f}\"\n",
    "# ])\n",
    "\n",
    "# # Evaluate MAX masking on language modeling task\n",
    "# max_aux_scores = evaluate_llma_language_modeling(model, wiki_sample, tokenizer, max_samples=200)\n",
    "# max_perplexity = max_aux_scores[\"perplexity\"]\n",
    "# max_perplexity_increase = ((max_perplexity - base_perplexity) / base_perplexity) * 100\n",
    "# aux_results_table.add_row([\n",
    "#     \"MAX Masking\", \n",
    "#     f\"{max_perplexity:.4f}\", \n",
    "#     f\"{max_perplexity_increase:.2f}%\"\n",
    "# ])\n",
    "\n",
    "# # Evaluate MAX masking on SQuAD task\n",
    "# max_squad_scores = evaluate_llma_squad(model, squad_sample, tokenizer, max_samples=50)\n",
    "# em_drop_max = base_squad_scores['exact_match'] - max_squad_scores['exact_match']\n",
    "# f1_drop_max = base_squad_scores['f1'] - max_squad_scores['f1']\n",
    "# squad_results_table.add_row([\n",
    "#     \"MAX Masking\",\n",
    "#     f\"{max_squad_scores['exact_match']:.4f}\",\n",
    "#     f\"{max_squad_scores['f1']:.4f}\",\n",
    "#     f\"{em_drop_max:.4f}\",\n",
    "#     f\"{f1_drop_max:.4f}\"\n",
    "# ])\n",
    "\n",
    "# # Range masking (partial)\n",
    "# print(\"Masking with Range...\")\n",
    "# model = reset_llma(model)\n",
    "# tao = 2.5\n",
    "# model = mask_range_llma(model, mask_max_low_std, fc_vals, tao)\n",
    "# t = int(mask_max_low_std.shape[0]-torch.count_nonzero(mask_max_low_std))\n",
    "# print(f\"Total Masked: {t}\")\n",
    "\n",
    "# # Evaluate Range masking on main task\n",
    "# range_scores, *_ = evaluate_llma_summarization(model, dataset_sample, tokenizer, max_samples=100)\n",
    "# main_results_table.add_row([\n",
    "#     \"Range Masking\",\n",
    "#     f\"{range_scores['rouge1']:.4f}\",\n",
    "#     f\"{range_scores['rouge2']:.4f}\",\n",
    "#     f\"{range_scores['rougeL']:.4f}\"\n",
    "# ])\n",
    "\n",
    "# # Evaluate Range masking on language modeling task\n",
    "# range_aux_scores = evaluate_llma_language_modeling(model, wiki_sample, tokenizer, max_samples=200)\n",
    "# range_perplexity = range_aux_scores[\"perplexity\"]\n",
    "# range_perplexity_increase = ((range_perplexity - base_perplexity) / base_perplexity) * 100\n",
    "# aux_results_table.add_row([\n",
    "#     \"Range Masking\", \n",
    "#     f\"{range_perplexity:.4f}\", \n",
    "#     f\"{range_perplexity_increase:.2f}%\"\n",
    "# ])\n",
    "\n",
    "# # Evaluate Range masking on SQuAD task\n",
    "# range_squad_scores = evaluate_llma_squad(model, squad_sample, tokenizer, max_samples=50)\n",
    "# em_drop_range = base_squad_scores['exact_match'] - range_squad_scores['exact_match']\n",
    "# f1_drop_range = base_squad_scores['f1'] - range_squad_scores['f1']\n",
    "# squad_results_table.add_row([\n",
    "#     \"Range Masking\",\n",
    "#     f\"{range_squad_scores['exact_match']:.4f}\",\n",
    "#     f\"{range_squad_scores['f1']:.4f}\",\n",
    "#     f\"{em_drop_range:.4f}\",\n",
    "#     f\"{f1_drop_range:.4f}\"\n",
    "# ])\n",
    "\n",
    "# # Print results for main task\n",
    "# print(\"\\nResults for Main Task (Summarization):\")\n",
    "# print(main_results_table)\n",
    "# print(f\"Layer: {mask_layer}\")\n",
    "# print(f\"Base ROUGE-1: {base_scores['rouge1']:.4f}\")\n",
    "# print(f\"Range Masking ROUGE-1: {range_scores['rouge1']:.4f}\")\n",
    "# print(f\"MAX Masking ROUGE-1: {max_scores['rouge1']:.4f}\")\n",
    "# print(f\"ROUGE-1 Drop (Range): {base_scores['rouge1'] - range_scores['rouge1']:.4f}\")\n",
    "# print(f\"ROUGE-1 Drop (MAX): {base_scores['rouge1'] - max_scores['rouge1']:.4f}\")\n",
    "\n",
    "# # Print results for language modeling task\n",
    "# print(\"\\nResults for Auxiliary Task 1 (Language Modeling):\")\n",
    "# print(aux_results_table)\n",
    "\n",
    "# # Print results for SQuAD task\n",
    "# print(\"\\nResults for Auxiliary Task 2 (SQuAD Question Answering):\")\n",
    "# print(squad_results_table)\n",
    "\n",
    "# # Calculate degradation ratios for both auxiliary tasks\n",
    "# print(\"\\nDegradation Analysis:\")\n",
    "# summarization_drop_range = base_scores['rouge1'] - range_scores['rouge1']\n",
    "# summarization_drop_max = base_scores['rouge1'] - max_scores['rouge1']\n",
    "\n",
    "# # Language modeling degradation ratio\n",
    "# if summarization_drop_range > 0.001:\n",
    "#     lm_range_ratio = range_perplexity_increase / (summarization_drop_range * 100)\n",
    "# else:\n",
    "#     lm_range_ratio = float('inf')\n",
    "    \n",
    "# if summarization_drop_max > 0.001:\n",
    "#     lm_max_ratio = max_perplexity_increase / (summarization_drop_max * 100)\n",
    "# else:\n",
    "#     lm_max_ratio = float('inf')\n",
    "\n",
    "# # SQuAD degradation ratio (using F1 score)\n",
    "# if summarization_drop_range > 0.001 and f1_drop_range > 0.001:\n",
    "#     qa_range_ratio = f1_drop_range / summarization_drop_range\n",
    "# else:\n",
    "#     qa_range_ratio = float('inf')\n",
    "    \n",
    "# if summarization_drop_max > 0.001 and f1_drop_max > 0.001:\n",
    "#     qa_max_ratio = f1_drop_max / summarization_drop_max\n",
    "# else:\n",
    "#     qa_max_ratio = float('inf')\n",
    "\n",
    "# # Create table for language modeling degradation\n",
    "# lm_degradation_table = PrettyTable()\n",
    "# lm_degradation_table.field_names = [\n",
    "#     \"Masking Type\", \n",
    "#     \"ROUGE-1 Drop\", \n",
    "#     \"Perplexity Increase %\", \n",
    "#     \"Degradation Ratio (lower is better)\"\n",
    "# ]\n",
    "# lm_degradation_table.add_row([\n",
    "#     \"Range Masking\", \n",
    "#     f\"{summarization_drop_range:.4f}\", \n",
    "#     f\"{range_perplexity_increase:.2f}%\",\n",
    "#     f\"{lm_range_ratio:.4f}\"\n",
    "# ])\n",
    "# lm_degradation_table.add_row([\n",
    "#     \"MAX Masking\", \n",
    "#     f\"{summarization_drop_max:.4f}\", \n",
    "#     f\"{max_perplexity_increase:.2f}%\",\n",
    "#     f\"{lm_max_ratio:.4f}\"\n",
    "# ])\n",
    "\n",
    "# # Create table for SQuAD degradation\n",
    "# qa_degradation_table = PrettyTable()\n",
    "# qa_degradation_table.field_names = [\n",
    "#     \"Masking Type\", \n",
    "#     \"ROUGE-1 Drop (Main)\", \n",
    "#     \"F1 Drop (QA)\", \n",
    "#     \"Degradation Ratio (lower is better)\"\n",
    "# ]\n",
    "# qa_degradation_table.add_row([\n",
    "#     \"Range Masking\", \n",
    "#     f\"{summarization_drop_range:.4f}\", \n",
    "#     f\"{f1_drop_range:.4f}\",\n",
    "#     f\"{qa_range_ratio:.4f}\"\n",
    "# ])\n",
    "# qa_degradation_table.add_row([\n",
    "#     \"MAX Masking\", \n",
    "#     f\"{summarization_drop_max:.4f}\", \n",
    "#     f\"{f1_drop_max:.4f}\",\n",
    "#     f\"{qa_max_ratio:.4f}\"\n",
    "# ])\n",
    "\n",
    "# print(\"Language Modeling Degradation Analysis:\")\n",
    "# print(lm_degradation_table)\n",
    "\n",
    "# print(\"\\nSQuAD QA Degradation Analysis:\")\n",
    "# print(qa_degradation_table)\n",
    "\n",
    "# # Calculate combined specificity score (average of both auxiliary tasks)\n",
    "# lm_relative_specificity = lm_max_ratio / lm_range_ratio if lm_range_ratio > 0 else float('inf')\n",
    "# qa_relative_specificity = qa_max_ratio / qa_range_ratio if qa_range_ratio > 0 else float('inf')\n",
    "\n",
    "# # Average specificity (if both are valid)\n",
    "# if not (np.isinf(lm_relative_specificity) or np.isinf(qa_relative_specificity)):\n",
    "#     combined_specificity = (lm_relative_specificity + qa_relative_specificity) / 2\n",
    "#     specificity_description = f\"Range masking is {combined_specificity:.2f}x more specific than MAX masking on average\"\n",
    "# else:\n",
    "#     # Use the non-infinite one if available\n",
    "#     if not np.isinf(lm_relative_specificity):\n",
    "#         combined_specificity = lm_relative_specificity\n",
    "#         specificity_description = f\"Range masking is {combined_specificity:.2f}x more specific than MAX masking (based on LM task)\"\n",
    "#     elif not np.isinf(qa_relative_specificity):\n",
    "#         combined_specificity = qa_relative_specificity\n",
    "#         specificity_description = f\"Range masking is {combined_specificity:.2f}x more specific than MAX masking (based on QA task)\"\n",
    "#     else:\n",
    "#         combined_specificity = float('inf')\n",
    "#         specificity_description = \"Could not determine relative specificity (division by zero)\"\n",
    "\n",
    "# print(f\"\\nCombined Specificity Analysis:\")\n",
    "# print(specificity_description)\n",
    "\n",
    "# # Write results to file\n",
    "# with open(\"masking_comparison_results.txt\", \"w\") as f:\n",
    "#     f.write(\"Results for Main Task (Summarization):\\n\")\n",
    "#     f.write(str(main_results_table) + \"\\n\\n\")\n",
    "    \n",
    "#     f.write(\"Results for Auxiliary Task 1 (Language Modeling):\\n\")\n",
    "#     f.write(str(aux_results_table) + \"\\n\\n\")\n",
    "    \n",
    "#     f.write(\"Results for Auxiliary Task 2 (SQuAD Question Answering):\\n\")\n",
    "#     f.write(str(squad_results_table) + \"\\n\\n\")\n",
    "    \n",
    "#     f.write(\"Language Modeling Degradation Analysis:\\n\")\n",
    "#     f.write(str(lm_degradation_table) + \"\\n\\n\")\n",
    "    \n",
    "#     f.write(\"SQuAD QA Degradation Analysis:\\n\")\n",
    "#     f.write(str(qa_degradation_table) + \"\\n\\n\")\n",
    "    \n",
    "#     f.write(f\"Layer: {mask_layer}\\n\")\n",
    "#     f.write(f\"Masking Percentage: {percent}\\n\")\n",
    "#     f.write(f\"Range Masking Tau: {3.5}\\n\\n\")\n",
    "    \n",
    "#     f.write(\"Specificity Analysis:\\n\")\n",
    "#     f.write(f\"LM Specificity (MAX/Range): {lm_relative_specificity:.4f}\\n\")\n",
    "#     f.write(f\"QA Specificity (MAX/Range): {qa_relative_specificity:.4f}\\n\")\n",
    "#     f.write(f\"Combined Specificity: {combined_specificity:.4f}\\n\")\n",
    "#     f.write(f\"{specificity_description}\\n\")\n",
    "\n",
    "# print(\"Results saved to masking_comparison_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking with Range...\n",
      "Total Masked: 1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.2159: 100%|██████████| 50/50 [37:08<00:00, 44.56s/it]\n",
      "Perplexity: 8.0745: 100%|██████████| 200/200 [00:42<00:00,  4.72it/s]\n",
      "EM: 0.5200, F1: 0.6554: 100%|██████████| 50/50 [00:21<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Range masking (partial)\n",
    "print(\"Masking with Range...\")\n",
    "model = reset_llma(model)\n",
    "tao = 3\n",
    "model = mask_range_llma(model, mask_max_low_std, fc_vals, tao)\n",
    "t = int(mask_max_low_std.shape[0]-torch.count_nonzero(mask_max_low_std))\n",
    "print(f\"Total Masked: {t}\")\n",
    "\n",
    "# Evaluate Range masking on main task\n",
    "range_scores, *_ = evaluate_llma_summarization(model, dataset_sample, tokenizer, max_samples=100)\n",
    "main_results_table.add_row([\n",
    "    \"Range Masking\",\n",
    "    f\"{range_scores['rouge1']:.4f}\",\n",
    "    f\"{range_scores['rouge2']:.4f}\",\n",
    "    f\"{range_scores['rougeL']:.4f}\"\n",
    "])\n",
    "\n",
    "# Evaluate Range masking on language modeling task\n",
    "range_aux_scores = evaluate_llma_language_modeling(model, wiki_sample, tokenizer, max_samples=200)\n",
    "range_perplexity = range_aux_scores[\"perplexity\"]\n",
    "range_perplexity_increase = ((range_perplexity - base_perplexity) / base_perplexity) * 100\n",
    "aux_results_table.add_row([\n",
    "    \"Range Masking\", \n",
    "    f\"{range_perplexity:.4f}\", \n",
    "    f\"{range_perplexity_increase:.2f}%\"\n",
    "])\n",
    "\n",
    "# Evaluate Range masking on SQuAD task\n",
    "range_squad_scores = evaluate_llma_squad(model, squad_sample, tokenizer, max_samples=50)\n",
    "em_drop_range = base_squad_scores['exact_match'] - range_squad_scores['exact_match']\n",
    "f1_drop_range = base_squad_scores['f1'] - range_squad_scores['f1']\n",
    "squad_results_table.add_row([\n",
    "    \"Range Masking\",\n",
    "    f\"{range_squad_scores['exact_match']:.4f}\",\n",
    "    f\"{range_squad_scores['f1']:.4f}\",\n",
    "    f\"{em_drop_range:.4f}\",\n",
    "    f\"{f1_drop_range:.4f}\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Masking Type</th>\n",
       "            <th>ROUGE-1</th>\n",
       "            <th>ROUGE-2</th>\n",
       "            <th>ROUGE-L</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>Base (No Masking)</td>\n",
       "            <td>0.2741</td>\n",
       "            <td>0.1313</td>\n",
       "            <td>0.1966</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Range Masking</td>\n",
       "            <td>0.2159</td>\n",
       "            <td>0.0975</td>\n",
       "            <td>0.1595</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------------+---------+---------+---------+\n",
       "|    Masking Type   | ROUGE-1 | ROUGE-2 | ROUGE-L |\n",
       "+-------------------+---------+---------+---------+\n",
       "| Base (No Masking) |  0.2741 |  0.1313 |  0.1966 |\n",
       "|   Range Masking   |  0.2159 |  0.0975 |  0.1595 |\n",
       "+-------------------+---------+---------+---------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def parse_data(text):\n",
    "    # Find all tau sections\n",
    "    tau_sections = re.split(r'Tao:\\s+', text)[1:]  # Skip the first empty element\n",
    "    \n",
    "    # Initialize dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    for section in tau_sections:\n",
    "        # Extract tau value\n",
    "        tau_match = re.match(r'(\\d+\\.?\\d*)', section)\n",
    "        if not tau_match:\n",
    "            continue\n",
    "        \n",
    "        tau = float(tau_match.group(1))\n",
    "        \n",
    "        # Extract class data\n",
    "        class_data = []\n",
    "        for class_idx in range(4):\n",
    "            pattern = r'Class ' + str(class_idx) + r'\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+'\n",
    "            match = re.search(pattern, section)\n",
    "            \n",
    "            if match:\n",
    "                row_data = [float(match.group(i)) for i in range(1, 9)]\n",
    "                class_data.append(row_data)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if class_data:\n",
    "            results[tau] = np.array(class_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_averages_table(parsed_data):\n",
    "    # Column names\n",
    "    columns = [\n",
    "        'Base Accuracy', 'Base Confidence', 'Base Complement Acc', 'Base Compliment Conf',\n",
    "        'STD Accuracy', 'STD Confidence', 'STD compliment ACC', 'STD compliment Conf'\n",
    "    ]\n",
    "    \n",
    "    # Calculate averages for each tau value\n",
    "    results = []\n",
    "    for tau, data in sorted(parsed_data.items()):\n",
    "        row = [tau]\n",
    "        row.extend([np.mean(data[:, i]) for i in range(data.shape[1])])\n",
    "        results.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results, columns=['Tau'] + columns)\n",
    "    return df.round(4)\n",
    "\n",
    "# Read data from file\n",
    "with open('tao_abiliation.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Parse the data\n",
    "parsed_data = parse_data(data)\n",
    "\n",
    "# Create and display the averages table\n",
    "result_table = create_averages_table(parsed_data)\n",
    "print(result_table)\n",
    "\n",
    "# Save to CSV\n",
    "result_table.to_csv('tau_averages.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
