{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d1/grad/mha361/anaconda3/envs/memit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 3.990647792816162\n",
      "step: 10, loss: 0.37118732929229736\n",
      "step: 20, loss: 0.3362318277359009\n",
      "step: 30, loss: 0.09571561217308044\n",
      "step: 40, loss: 0.12030036002397537\n",
      "step: 50, loss: 0.13667671382427216\n",
      "step: 60, loss: 0.038196198642253876\n",
      "step: 70, loss: 0.07350394874811172\n",
      "step: 80, loss: 0.21376359462738037\n",
      "step: 90, loss: 0.14637543261051178\n",
      "step: 100, loss: 0.13455474376678467\n",
      "step: 110, loss: 0.10028056055307388\n",
      "step: 120, loss: 0.0601409375667572\n",
      "step: 130, loss: 0.09087196737527847\n",
      "step: 140, loss: 0.1182173565030098\n",
      "step: 150, loss: 0.08207911998033524\n",
      "step: 160, loss: 0.04764505475759506\n",
      "step: 170, loss: 0.024719884619116783\n",
      "step: 180, loss: 0.09355863928794861\n",
      "step: 190, loss: 0.03971570357680321\n",
      "step: 200, loss: 0.09468912333250046\n",
      "step: 210, loss: 0.05551377683877945\n",
      "step: 220, loss: 0.11327378451824188\n",
      "step: 230, loss: 0.11468115448951721\n",
      "step: 240, loss: 0.08720338344573975\n",
      "step: 250, loss: 0.09425006061792374\n",
      "step: 260, loss: 0.0637039765715599\n",
      "step: 270, loss: 0.07483372837305069\n",
      "step: 280, loss: 0.12980930507183075\n",
      "step: 290, loss: 0.10836219787597656\n",
      "step: 300, loss: 0.1544344127178192\n",
      "step: 310, loss: 0.11341768503189087\n",
      "step: 320, loss: 0.15220527350902557\n",
      "step: 330, loss: 0.07310640811920166\n",
      "step: 340, loss: 0.07519593834877014\n",
      "step: 350, loss: 0.0508994460105896\n",
      "step: 360, loss: 0.1391129046678543\n",
      "step: 370, loss: 0.08767443150281906\n",
      "step: 380, loss: 0.07783786952495575\n",
      "step: 390, loss: 0.13107314705848694\n",
      "step: 400, loss: 0.08598286658525467\n",
      "step: 410, loss: 0.1565433144569397\n",
      "step: 420, loss: 0.076337531208992\n",
      "step: 430, loss: 0.13361909985542297\n",
      "step: 440, loss: 0.040911607444286346\n",
      "step: 0, loss: 0.03385324403643608\n",
      "step: 10, loss: 0.015565859153866768\n",
      "step: 20, loss: 0.06301263719797134\n",
      "step: 30, loss: 0.03572291508316994\n",
      "step: 40, loss: 0.10252012312412262\n",
      "step: 50, loss: 0.025456935167312622\n",
      "step: 60, loss: 0.04284265637397766\n",
      "step: 70, loss: 0.06526970863342285\n",
      "step: 80, loss: 0.11121272295713425\n",
      "step: 90, loss: 0.08557172864675522\n",
      "step: 100, loss: 0.06382261216640472\n",
      "step: 110, loss: 0.06105560064315796\n",
      "step: 120, loss: 0.10654032975435257\n",
      "step: 130, loss: 0.11546523869037628\n",
      "step: 140, loss: 0.10477225482463837\n",
      "step: 150, loss: 0.03557237610220909\n",
      "step: 160, loss: 0.07554835081100464\n",
      "step: 170, loss: 0.03148049861192703\n",
      "step: 180, loss: 0.1183905377984047\n",
      "step: 190, loss: 0.042864665389060974\n",
      "step: 200, loss: 0.03455191105604172\n",
      "step: 210, loss: 0.037698786705732346\n",
      "step: 220, loss: 0.1106521263718605\n",
      "step: 230, loss: 0.08061537146568298\n",
      "step: 240, loss: 0.117466501891613\n",
      "step: 250, loss: 0.05061101168394089\n",
      "step: 260, loss: 0.03712976723909378\n",
      "step: 270, loss: 0.036919523030519485\n",
      "step: 280, loss: 0.020690836012363434\n",
      "step: 290, loss: 0.1001516580581665\n",
      "step: 300, loss: 0.07968013733625412\n",
      "step: 310, loss: 0.05178719386458397\n",
      "step: 320, loss: 0.03034099191427231\n",
      "step: 330, loss: 0.1536630094051361\n",
      "step: 340, loss: 0.07242723554372787\n",
      "step: 350, loss: 0.05637557804584503\n",
      "step: 360, loss: 0.1073068305850029\n",
      "step: 370, loss: 0.15311548113822937\n",
      "step: 380, loss: 0.10260505229234695\n",
      "step: 390, loss: 0.08874401450157166\n",
      "step: 400, loss: 0.147237166762352\n",
      "step: 410, loss: 0.06490642577409744\n",
      "step: 420, loss: 0.04833662509918213\n",
      "step: 430, loss: 0.05462557077407837\n",
      "step: 440, loss: 0.07597516477108002\n",
      "step: 0, loss: 0.11321542412042618\n",
      "step: 10, loss: 0.03503039479255676\n",
      "step: 20, loss: 0.02788444235920906\n",
      "step: 30, loss: 0.06926146894693375\n",
      "step: 40, loss: 0.049946609884500504\n",
      "step: 50, loss: 0.08321662992238998\n",
      "step: 60, loss: 0.05955062434077263\n",
      "step: 70, loss: 0.029698191210627556\n",
      "step: 80, loss: 0.10974638909101486\n",
      "step: 90, loss: 0.02923920936882496\n",
      "step: 100, loss: 0.0683043971657753\n",
      "step: 110, loss: 0.10273410379886627\n",
      "step: 120, loss: 0.02897130697965622\n",
      "step: 130, loss: 0.11696498841047287\n",
      "step: 140, loss: 0.05586034059524536\n",
      "step: 150, loss: 0.028133615851402283\n",
      "step: 160, loss: 0.024797294288873672\n",
      "step: 170, loss: 0.044792115688323975\n",
      "step: 180, loss: 0.1499277502298355\n",
      "step: 190, loss: 0.10704918205738068\n",
      "step: 200, loss: 0.07315603643655777\n",
      "step: 210, loss: 0.04240040481090546\n",
      "step: 220, loss: 0.07459548115730286\n",
      "step: 230, loss: 0.04310793802142143\n",
      "step: 240, loss: 0.025718173012137413\n",
      "step: 250, loss: 0.020660754293203354\n",
      "step: 260, loss: 0.10287054628133774\n",
      "step: 270, loss: 0.10714941471815109\n",
      "step: 280, loss: 0.03877098858356476\n",
      "step: 290, loss: 0.055724792182445526\n",
      "step: 300, loss: 0.06230615824460983\n",
      "step: 310, loss: 0.061846569180488586\n",
      "step: 320, loss: 0.08452876657247543\n",
      "step: 330, loss: 0.05080748721957207\n",
      "step: 340, loss: 0.09270616620779037\n",
      "step: 350, loss: 0.08166538923978806\n",
      "step: 360, loss: 0.05697937682271004\n",
      "step: 370, loss: 0.043635498732328415\n",
      "step: 380, loss: 0.04087095707654953\n",
      "step: 390, loss: 0.07376763969659805\n",
      "step: 400, loss: 0.061982523649930954\n",
      "step: 410, loss: 0.061358317732810974\n",
      "step: 420, loss: 0.016186783090233803\n",
      "step: 430, loss: 0.03899798542261124\n",
      "step: 440, loss: 0.00798218883574009\n",
      "step: 0, loss: 0.05397997051477432\n",
      "step: 10, loss: 0.054323915392160416\n",
      "step: 20, loss: 0.042192790657281876\n",
      "step: 30, loss: 0.006837547291070223\n",
      "step: 40, loss: 0.03384236991405487\n",
      "step: 50, loss: 0.02286209911108017\n",
      "step: 60, loss: 0.014704115688800812\n",
      "step: 70, loss: 0.07367595285177231\n",
      "step: 80, loss: 0.047874800860881805\n",
      "step: 90, loss: 0.10066154599189758\n",
      "step: 100, loss: 0.02329779416322708\n",
      "step: 110, loss: 0.015209788456559181\n",
      "step: 120, loss: 0.02391977235674858\n",
      "step: 130, loss: 0.04513701796531677\n",
      "step: 140, loss: 0.03380057215690613\n",
      "step: 150, loss: 0.0579296313226223\n",
      "step: 160, loss: 0.031017109751701355\n",
      "step: 170, loss: 0.04504317790269852\n",
      "step: 180, loss: 0.06794418394565582\n",
      "step: 190, loss: 0.04719974100589752\n",
      "step: 200, loss: 0.039309531450271606\n",
      "step: 210, loss: 0.06079183518886566\n",
      "step: 220, loss: 0.008044497109949589\n",
      "step: 230, loss: 0.05791231617331505\n",
      "step: 240, loss: 0.013466753996908665\n",
      "step: 250, loss: 0.047017812728881836\n",
      "step: 260, loss: 0.011539213359355927\n",
      "step: 270, loss: 0.07712533324956894\n",
      "step: 280, loss: 0.030999932438135147\n",
      "step: 290, loss: 0.08295945078134537\n",
      "step: 300, loss: 0.015065743587911129\n",
      "step: 310, loss: 0.08572567254304886\n",
      "step: 320, loss: 0.033986371010541916\n",
      "step: 330, loss: 0.015355822630226612\n",
      "step: 340, loss: 0.12314125895500183\n",
      "step: 350, loss: 0.009895559400320053\n",
      "step: 360, loss: 0.03871766850352287\n",
      "step: 370, loss: 0.060151077806949615\n",
      "step: 380, loss: 0.16863656044006348\n",
      "step: 390, loss: 0.046050120145082474\n",
      "step: 400, loss: 0.038792937994003296\n",
      "step: 410, loss: 0.01638779602944851\n",
      "step: 420, loss: 0.09491989761590958\n",
      "step: 430, loss: 0.07692885398864746\n",
      "step: 440, loss: 0.004199783317744732\n",
      "step: 0, loss: 0.010667424649000168\n",
      "step: 10, loss: 0.05055611580610275\n",
      "step: 20, loss: 0.04289801046252251\n",
      "step: 30, loss: 0.05114906281232834\n",
      "step: 40, loss: 0.07982663810253143\n",
      "step: 50, loss: 0.013394573703408241\n",
      "step: 60, loss: 0.013529913499951363\n",
      "step: 70, loss: 0.031839389353990555\n",
      "step: 80, loss: 0.03302401304244995\n",
      "step: 90, loss: 0.05456213280558586\n",
      "step: 100, loss: 0.00847667083144188\n",
      "step: 110, loss: 0.0312754325568676\n",
      "step: 120, loss: 0.03813386335968971\n",
      "step: 130, loss: 0.08023472130298615\n",
      "step: 140, loss: 0.11113326251506805\n",
      "step: 150, loss: 0.07420124858617783\n",
      "step: 160, loss: 0.04297960549592972\n",
      "step: 170, loss: 0.09694168716669083\n",
      "step: 180, loss: 0.024012429639697075\n",
      "step: 190, loss: 0.015797551721334457\n",
      "step: 200, loss: 0.007881749421358109\n",
      "step: 210, loss: 0.04427962750196457\n",
      "step: 220, loss: 0.04906251281499863\n",
      "step: 230, loss: 0.12325277179479599\n",
      "step: 240, loss: 0.028382066637277603\n",
      "step: 250, loss: 0.021916674450039864\n",
      "step: 260, loss: 0.07953765988349915\n",
      "step: 270, loss: 0.01644616201519966\n",
      "step: 280, loss: 0.09878308326005936\n",
      "step: 290, loss: 0.03593681380152702\n",
      "step: 300, loss: 0.024608317762613297\n",
      "step: 310, loss: 0.038527194410562515\n",
      "step: 320, loss: 0.023038608953356743\n",
      "step: 330, loss: 0.026915324851870537\n",
      "step: 340, loss: 0.05163087323307991\n",
      "step: 350, loss: 0.04436801001429558\n",
      "step: 360, loss: 0.06853518635034561\n",
      "step: 370, loss: 0.04911944642663002\n",
      "step: 380, loss: 0.06578917801380157\n",
      "step: 390, loss: 0.09903744608163834\n",
      "step: 400, loss: 0.08950947970151901\n",
      "step: 410, loss: 0.040481701493263245\n",
      "step: 420, loss: 0.03785557299852371\n",
      "step: 430, loss: 0.04423528537154198\n",
      "step: 440, loss: 0.004089535679668188\n",
      "step: 0, loss: 0.007212631870061159\n",
      "step: 10, loss: 0.06911258399486542\n",
      "step: 20, loss: 0.009941848926246166\n",
      "step: 30, loss: 0.02159048244357109\n",
      "step: 40, loss: 0.0556434765458107\n",
      "step: 50, loss: 0.05075301229953766\n",
      "step: 60, loss: 0.040702398866415024\n",
      "step: 70, loss: 0.003977523185312748\n",
      "step: 80, loss: 0.026269342750310898\n",
      "step: 90, loss: 0.06964711099863052\n",
      "step: 100, loss: 0.032008420675992966\n",
      "step: 110, loss: 0.057902947068214417\n",
      "step: 120, loss: 0.012133359909057617\n",
      "step: 130, loss: 0.05329170078039169\n",
      "step: 140, loss: 0.05208103358745575\n",
      "step: 150, loss: 0.09676911681890488\n",
      "step: 160, loss: 0.05064275488257408\n",
      "step: 170, loss: 0.009706468321383\n",
      "step: 180, loss: 0.07637470960617065\n",
      "step: 190, loss: 0.03851328417658806\n",
      "step: 200, loss: 0.0031557464972138405\n",
      "step: 210, loss: 0.022694343701004982\n",
      "step: 220, loss: 0.06358905881643295\n",
      "step: 230, loss: 0.03717838227748871\n",
      "step: 240, loss: 0.046712636947631836\n",
      "step: 250, loss: 0.16849416494369507\n",
      "step: 260, loss: 0.043018877506256104\n",
      "step: 270, loss: 0.04297808185219765\n",
      "step: 280, loss: 0.01685783453285694\n",
      "step: 290, loss: 0.01644177921116352\n",
      "step: 300, loss: 0.043541599065065384\n",
      "step: 310, loss: 0.09806153923273087\n",
      "step: 320, loss: 0.028382929041981697\n",
      "step: 330, loss: 0.00990305095911026\n",
      "step: 340, loss: 0.05146224796772003\n",
      "step: 350, loss: 0.12002731114625931\n",
      "step: 360, loss: 0.10798696428537369\n",
      "step: 370, loss: 0.03799387067556381\n",
      "step: 380, loss: 0.019167128950357437\n",
      "step: 390, loss: 0.046489082276821136\n",
      "step: 400, loss: 0.024171164259314537\n",
      "step: 410, loss: 0.012110499665141106\n",
      "step: 420, loss: 0.08133730292320251\n",
      "step: 430, loss: 0.05796501040458679\n",
      "step: 440, loss: 0.002802620641887188\n",
      "step: 0, loss: 0.025115758180618286\n",
      "step: 10, loss: 0.00442331749945879\n",
      "step: 20, loss: 0.11664236336946487\n",
      "step: 30, loss: 0.011622853577136993\n",
      "step: 40, loss: 0.029150962829589844\n",
      "step: 50, loss: 0.07069123536348343\n",
      "step: 60, loss: 0.04173526167869568\n",
      "step: 70, loss: 0.029776545241475105\n",
      "step: 80, loss: 0.015680231153964996\n",
      "step: 90, loss: 0.048978447914123535\n",
      "step: 100, loss: 0.11679960787296295\n",
      "step: 110, loss: 0.0998934954404831\n",
      "step: 120, loss: 0.056237656623125076\n",
      "step: 130, loss: 0.11036589741706848\n",
      "step: 140, loss: 0.13849008083343506\n",
      "step: 150, loss: 0.1253424435853958\n",
      "step: 160, loss: 0.02689489722251892\n",
      "step: 170, loss: 0.06517429649829865\n",
      "step: 180, loss: 0.06053945794701576\n",
      "step: 190, loss: 0.028294207528233528\n",
      "step: 200, loss: 0.027294404804706573\n",
      "step: 210, loss: 0.07299178838729858\n",
      "step: 220, loss: 0.10064638406038284\n",
      "step: 230, loss: 0.059362251311540604\n",
      "step: 240, loss: 0.030641969293355942\n",
      "step: 250, loss: 0.09373476356267929\n",
      "step: 260, loss: 0.044730931520462036\n",
      "step: 270, loss: 0.060403112322092056\n",
      "step: 280, loss: 0.054951928555965424\n",
      "step: 290, loss: 0.039718929678201675\n",
      "step: 300, loss: 0.04449131712317467\n",
      "step: 310, loss: 0.04936545342206955\n",
      "step: 320, loss: 0.04778311401605606\n",
      "step: 330, loss: 0.04897584393620491\n",
      "step: 340, loss: 0.01813577674329281\n",
      "step: 350, loss: 0.016774436458945274\n",
      "step: 360, loss: 0.13607941567897797\n",
      "step: 370, loss: 0.10494069755077362\n",
      "step: 380, loss: 0.03109832853078842\n",
      "step: 390, loss: 0.05630401521921158\n",
      "step: 400, loss: 0.055680375546216965\n",
      "step: 410, loss: 0.031657908111810684\n",
      "step: 420, loss: 0.029567860066890717\n",
      "step: 430, loss: 0.015002140775322914\n",
      "step: 440, loss: 0.004838189575821161\n",
      "step: 0, loss: 0.00513447355479002\n",
      "step: 10, loss: 0.04048541933298111\n",
      "step: 20, loss: 0.05733231455087662\n",
      "step: 30, loss: 0.00295666279271245\n",
      "step: 40, loss: 0.0028466868679970503\n",
      "step: 50, loss: 0.0020129031036049128\n",
      "step: 60, loss: 0.055046215653419495\n",
      "step: 70, loss: 0.04499077796936035\n",
      "step: 80, loss: 0.029003601521253586\n",
      "step: 90, loss: 0.05694189667701721\n",
      "step: 100, loss: 0.07646017521619797\n",
      "step: 110, loss: 0.03095131739974022\n",
      "step: 120, loss: 0.07280845940113068\n",
      "step: 130, loss: 0.025441374629735947\n",
      "step: 140, loss: 0.0856073796749115\n",
      "step: 150, loss: 0.13203829526901245\n",
      "step: 160, loss: 0.007486771792173386\n",
      "step: 170, loss: 0.05338449776172638\n",
      "step: 180, loss: 0.0240241177380085\n",
      "step: 190, loss: 0.04150131717324257\n",
      "step: 200, loss: 0.009422951377928257\n",
      "step: 210, loss: 0.03669391945004463\n",
      "step: 220, loss: 0.04198785498738289\n",
      "step: 230, loss: 0.004779220558702946\n",
      "step: 240, loss: 0.012317456305027008\n",
      "step: 250, loss: 0.037949733436107635\n",
      "step: 260, loss: 0.04756305739283562\n",
      "step: 270, loss: 0.046050913631916046\n",
      "step: 280, loss: 0.0153425894677639\n",
      "step: 290, loss: 0.047239065170288086\n",
      "step: 300, loss: 0.010132212191820145\n",
      "step: 310, loss: 0.03357622027397156\n",
      "step: 320, loss: 0.04441651329398155\n",
      "step: 330, loss: 0.03347686305642128\n",
      "step: 340, loss: 0.019034121185541153\n",
      "step: 350, loss: 0.02493259496986866\n",
      "step: 360, loss: 0.03777533024549484\n",
      "step: 370, loss: 0.014025912620127201\n",
      "step: 380, loss: 0.07854970544576645\n",
      "step: 390, loss: 0.01751113310456276\n",
      "step: 400, loss: 0.07093701511621475\n",
      "step: 410, loss: 0.03613315895199776\n",
      "step: 420, loss: 0.004420532379299402\n",
      "step: 430, loss: 0.031007597222924232\n",
      "step: 440, loss: 0.0033601520117372274\n",
      "step: 0, loss: 0.06761655956506729\n",
      "step: 10, loss: 0.012144211679697037\n",
      "step: 20, loss: 0.03017357736825943\n",
      "step: 30, loss: 0.03311131149530411\n",
      "step: 40, loss: 0.010792777873575687\n",
      "step: 50, loss: 0.029446110129356384\n",
      "step: 60, loss: 0.053219858556985855\n",
      "step: 70, loss: 0.013697052374482155\n",
      "step: 80, loss: 0.006560016889125109\n",
      "step: 90, loss: 0.042428020387887955\n",
      "step: 100, loss: 0.009510928764939308\n",
      "step: 110, loss: 0.04568374156951904\n",
      "step: 120, loss: 0.042657867074012756\n",
      "step: 130, loss: 0.021379705518484116\n",
      "step: 140, loss: 0.006394915748387575\n",
      "step: 150, loss: 0.009388015605509281\n",
      "step: 160, loss: 0.027337314561009407\n",
      "step: 170, loss: 0.03831255063414574\n",
      "step: 180, loss: 0.05071631819009781\n",
      "step: 190, loss: 0.008735868148505688\n",
      "step: 200, loss: 0.035494569689035416\n",
      "step: 210, loss: 0.023150084540247917\n",
      "step: 220, loss: 0.05197928845882416\n",
      "step: 230, loss: 0.01605493389070034\n",
      "step: 240, loss: 0.01357224676758051\n",
      "step: 250, loss: 0.016897976398468018\n",
      "step: 260, loss: 0.018525434657931328\n",
      "step: 270, loss: 0.03348855674266815\n",
      "step: 280, loss: 0.04298378527164459\n",
      "step: 290, loss: 0.04102005809545517\n",
      "step: 300, loss: 0.04095736891031265\n",
      "step: 310, loss: 0.011485832743346691\n",
      "step: 320, loss: 0.02335263416171074\n",
      "step: 330, loss: 0.03266937658190727\n",
      "step: 340, loss: 0.03430810943245888\n",
      "step: 350, loss: 0.08234068006277084\n",
      "step: 360, loss: 0.009033720940351486\n",
      "step: 370, loss: 0.16118887066841125\n",
      "step: 380, loss: 0.036727529019117355\n",
      "step: 390, loss: 0.04925044625997543\n",
      "step: 400, loss: 0.07625633478164673\n",
      "step: 410, loss: 0.03199348971247673\n",
      "step: 420, loss: 0.13962328433990479\n",
      "step: 430, loss: 0.10131509602069855\n",
      "step: 440, loss: 0.010516569018363953\n",
      "step: 0, loss: 0.039460357278585434\n",
      "step: 10, loss: 0.030672840774059296\n",
      "step: 20, loss: 0.008095107041299343\n",
      "step: 30, loss: 0.004230631049722433\n",
      "step: 40, loss: 0.02707865834236145\n",
      "step: 50, loss: 0.025302905589342117\n",
      "step: 60, loss: 0.03437339514493942\n",
      "step: 70, loss: 0.019489893689751625\n",
      "step: 80, loss: 0.02693190798163414\n",
      "step: 90, loss: 0.040770769119262695\n",
      "step: 100, loss: 0.022075777873396873\n",
      "step: 110, loss: 0.02038615196943283\n",
      "step: 120, loss: 0.023238064721226692\n",
      "step: 130, loss: 0.015787379816174507\n",
      "step: 140, loss: 0.05093023553490639\n",
      "step: 150, loss: 0.057025983929634094\n",
      "step: 160, loss: 0.009579598903656006\n",
      "step: 170, loss: 0.03578168898820877\n",
      "step: 180, loss: 0.02228362485766411\n",
      "step: 190, loss: 0.04455215111374855\n",
      "step: 200, loss: 0.022981714457273483\n",
      "step: 210, loss: 0.05337056145071983\n",
      "step: 220, loss: 0.009453458711504936\n",
      "step: 230, loss: 0.025211794301867485\n",
      "step: 240, loss: 0.022624149918556213\n",
      "step: 250, loss: 0.030186353251338005\n",
      "step: 260, loss: 0.021732445806264877\n",
      "step: 270, loss: 0.028294438496232033\n",
      "step: 280, loss: 0.010968967340886593\n",
      "step: 290, loss: 0.0026822646614164114\n",
      "step: 300, loss: 0.006638443097472191\n",
      "step: 310, loss: 0.00481826439499855\n",
      "step: 320, loss: 0.07146278768777847\n",
      "step: 330, loss: 0.007482395973056555\n",
      "step: 340, loss: 0.005583155434578657\n",
      "step: 350, loss: 0.04912930354475975\n",
      "step: 360, loss: 0.032221198081970215\n",
      "step: 370, loss: 0.02184753119945526\n",
      "step: 380, loss: 0.009057139977812767\n",
      "step: 390, loss: 0.04874726012349129\n",
      "step: 400, loss: 0.006002416368573904\n",
      "step: 410, loss: 0.03428107500076294\n",
      "step: 420, loss: 0.025667285546660423\n",
      "step: 430, loss: 0.03341158851981163\n",
      "step: 440, loss: 0.004410878289490938\n",
      "<pad>: N/A (0 occurrences)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from utilities import train, eval, pad, get_model_bert\n",
    "from POS_dataset import PosDataset\n",
    "from prettytable import PrettyTable\n",
    "import nltk\n",
    "tagged_sents = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "tags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))\n",
    "\n",
    "\",\".join(tags)\n",
    "\n",
    "tags = [\"<pad>\"] + tags\n",
    "\n",
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}\n",
    "\n",
    "# Let's split the data into train and test (or eval)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(tagged_sents, test_size=.1)\n",
    "len(train_data), len(test_data)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class MaskLayer(nn.Module):\n",
    "    def __init__(self, lower_bound, upper_bound, replacement_values):\n",
    "        super(MaskLayer, self).__init__()\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.replacement_values = replacement_values\n",
    "\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        lower_bound = self.lower_bound.to(dtype=x.dtype, device=x.device).view(1, 1, -1)\n",
    "        upper_bound = self.upper_bound.to(dtype=x.dtype, device=x.device).view(1, 1, -1)\n",
    "        replacement_values = self.replacement_values.to(dtype=x.dtype, device=x.device).view(1, 1, -1)\n",
    "\n",
    " \n",
    "\n",
    "        mask = (x >= lower_bound) & (x <= upper_bound)\n",
    "        x = torch.where(mask, replacement_values, x)\n",
    "        return x\n",
    "    \n",
    "    def set_perms(self,lower_bound, upper_bound, replacement_values):\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.replacement_values = replacement_values\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        self.bert = self.model.bert\n",
    "        self.masking_layer = torch.ones(768).to(\"cuda\")\n",
    "        self.mask_layer = MaskLayer(torch.tensor(float('inf')), torch.tensor(float('-inf')), torch.tensor(0.0))\n",
    "\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "        '''\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            self.bert.train()\n",
    "            encoded_layers = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded_layers = self.bert(x)\n",
    "                enc = encoded_layers[-1]\n",
    "        # enc = nn.ReLU(enc)\n",
    "        # enc = enc * self.masking_layer\n",
    "        enc = self.mask_layer(enc)\n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        confidence = logits.softmax(-1).max(-1).values\n",
    "        return enc, logits, y, y_hat, confidence\n",
    "    \n",
    "    \n",
    "model = Net(vocab_size=len(tag2idx))\n",
    "model.to(device)\n",
    "\n",
    "train_dataset = PosDataset(train_data, tokenizer, tag2idx)\n",
    "eval_dataset = PosDataset(test_data, tokenizer, tag2idx)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=True,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "test_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "activation_iter = data.DataLoader(dataset=train_dataset+eval_dataset,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "for i in range(10):\n",
    "    train(model, train_iter, optimizer, criterion)\n",
    "\n",
    "from utilities import  eval\n",
    "\n",
    "model.masking_layer = torch.ones(768).to(\"cuda\")\n",
    "activation_iter = data.DataLoader(dataset=train_dataset+eval_dataset,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "enc_dict = eval(model, activation_iter, idx2tag, tag2idx,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = PrettyTable()\n",
    "\n",
    "results_table.field_names = results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"Base Complement Acc\", \"Base Compliment Conf\", \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"]\n",
    "\n",
    "class_labels = []\n",
    "base_accuracies = []\n",
    "base_confidences = []\n",
    "base_comp_acc = []\n",
    "base_comp_conf = []\n",
    "max_accuracies = []\n",
    "max_confidences = []\n",
    "max_comp_acc = []\n",
    "max_comp_conf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> ----------------\n",
      "Original:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0, 0)\n",
      "Compliment: (0, 0)\n",
      "Compliment: (0, 0)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0, 0)\n",
      "Compliment: (0, 0)\n",
      "-----------------------------\n",
      "NNP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9944, 0.9925)\n",
      "Compliment: (0.9922, 0.9941)\n",
      "Compliment: (0.9922, 0.9941)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.7383, 0.0859)\n",
      "Compliment: (0.9918, 0.8706)\n",
      "-----------------------------\n",
      "RB ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.983, 0.9868)\n",
      "Compliment: (0.9926, 0.9941)\n",
      "Compliment: (0.9926, 0.9941)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.7874, 0.0779)\n",
      "Compliment: (0.9928, 0.8632)\n",
      "-----------------------------\n",
      "LS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9231, 0.9587)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.6923, 0.0707)\n",
      "Compliment: (0.9922, 0.8781)\n",
      "-----------------------------\n",
      "`` ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 1.0)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.085)\n",
      "Compliment: (0.9924, 0.8758)\n",
      "-----------------------------\n",
      "NN ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9801, 0.9866)\n",
      "Compliment: (0.9942, 0.995)\n",
      "Compliment: (0.9942, 0.995)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8536, 0.1063)\n",
      "Compliment: (0.9948, 0.8688)\n",
      "-----------------------------\n",
      "UH ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.983)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.6667, 0.0446)\n",
      "Compliment: (0.9925, 0.8648)\n",
      "-----------------------------\n",
      "PDT ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9866)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8519, 0.0484)\n",
      "Compliment: (0.9929, 0.8816)\n",
      "-----------------------------\n",
      "VBN ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9958, 0.9979)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9391, 0.0902)\n",
      "Compliment: (0.9931, 0.864)\n",
      "-----------------------------\n",
      "-NONE- ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 1.0)\n",
      "Compliment: (0.9918, 0.9935)\n",
      "Compliment: (0.9918, 0.9935)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.1187)\n",
      "Compliment: (0.9912, 0.8683)\n",
      "-----------------------------\n",
      "TO ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9995, 0.9999)\n",
      "Compliment: (0.9922, 0.9938)\n",
      "Compliment: (0.9922, 0.9938)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9986, 0.0979)\n",
      "Compliment: (0.9924, 0.8757)\n",
      "-----------------------------\n",
      "PRP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9977, 0.999)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9814, 0.0794)\n",
      "Compliment: (0.9921, 0.8853)\n",
      "-----------------------------\n",
      "RBS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9143, 0.9797)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.7429, 0.0901)\n",
      "Compliment: (0.9922, 0.8728)\n",
      "-----------------------------\n",
      "VB ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9914, 0.9944)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8978, 0.0862)\n",
      "Compliment: (0.9914, 0.8624)\n",
      "-----------------------------\n",
      "WP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9997)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.0616)\n",
      "Compliment: (0.9922, 0.8735)\n",
      "-----------------------------\n",
      "SYM ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0, 0.8962)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:152: RuntimeWarning: divide by zero encountered in divide\n",
      "  std_vals_normalized = (std_vals - min_vals) / (max_vals - min_vals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0, 0.0398)\n",
      "Compliment: (0.9926, 0.8695)\n",
      "-----------------------------\n",
      "NNS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9937, 0.9935)\n",
      "Compliment: (0.9923, 0.994)\n",
      "Compliment: (0.9923, 0.994)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9033, 0.0766)\n",
      "Compliment: (0.9934, 0.8738)\n",
      "-----------------------------\n",
      "WDT ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9888, 0.9987)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9775, 0.0684)\n",
      "Compliment: (0.9925, 0.8704)\n",
      "-----------------------------\n",
      "POS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9988, 0.9998)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9587, 0.0653)\n",
      "Compliment: (0.9926, 0.874)\n",
      "-----------------------------\n",
      "EX ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9984)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8523, 0.0588)\n",
      "Compliment: (0.9919, 0.8716)\n",
      "-----------------------------\n",
      ". ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 1.0)\n",
      "Compliment: (0.9921, 0.9937)\n",
      "Compliment: (0.9921, 0.9937)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.0885)\n",
      "Compliment: (0.9923, 0.8531)\n",
      "-----------------------------\n",
      "# ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9952)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.75, 0.0433)\n",
      "Compliment: (0.9921, 0.8792)\n",
      "-----------------------------\n",
      "CD ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9972, 0.9989)\n",
      "Compliment: (0.9922, 0.9938)\n",
      "Compliment: (0.9922, 0.9938)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9913, 0.0862)\n",
      "Compliment: (0.9922, 0.8667)\n",
      "-----------------------------\n",
      "DT ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9962, 0.9983)\n",
      "Compliment: (0.992, 0.9936)\n",
      "Compliment: (0.992, 0.9936)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9728, 0.1062)\n",
      "Compliment: (0.9924, 0.8673)\n",
      "-----------------------------\n",
      "VBP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9894, 0.9847)\n",
      "Compliment: (0.9924, 0.9941)\n",
      "Compliment: (0.9924, 0.9941)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.869, 0.0655)\n",
      "Compliment: (0.9924, 0.8718)\n",
      "-----------------------------\n",
      "NNPS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9672, 0.9172)\n",
      "Compliment: (0.9924, 0.9941)\n",
      "Compliment: (0.9924, 0.9941)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.5861, 0.065)\n",
      "Compliment: (0.9928, 0.8661)\n",
      "-----------------------------\n",
      "WP$ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9987)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.0658)\n",
      "Compliment: (0.9924, 0.8838)\n",
      "-----------------------------\n",
      "-LRB- ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9917, 0.9985)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9833, 0.0568)\n",
      "Compliment: (0.9922, 0.8658)\n",
      "-----------------------------\n",
      "$ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9998)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.0563)\n",
      "Compliment: (0.9928, 0.8783)\n",
      "-----------------------------\n",
      ", ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9998, 1.0)\n",
      "Compliment: (0.992, 0.9936)\n",
      "Compliment: (0.992, 0.9936)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9998, 0.1306)\n",
      "Compliment: (0.9924, 0.8659)\n",
      "-----------------------------\n",
      "JJ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9907, 0.9939)\n",
      "Compliment: (0.9925, 0.9939)\n",
      "Compliment: (0.9925, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8315, 0.093)\n",
      "Compliment: (0.9938, 0.8752)\n",
      "-----------------------------\n",
      "CC ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9943, 0.9973)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9581, 0.076)\n",
      "Compliment: (0.9925, 0.8754)\n",
      "-----------------------------\n",
      "PRP$ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9995)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9961, 0.0761)\n",
      "Compliment: (0.9926, 0.8743)\n",
      "-----------------------------\n",
      "JJR ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9396, 0.9495)\n",
      "Compliment: (0.9926, 0.9941)\n",
      "Compliment: (0.9926, 0.9941)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.5433, 0.0897)\n",
      "Compliment: (0.9926, 0.8707)\n",
      "-----------------------------\n",
      "VBG ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9918, 0.9944)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8548, 0.062)\n",
      "Compliment: (0.993, 0.8714)\n",
      "-----------------------------\n",
      "FW ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.5, 0.6992)\n",
      "Compliment: (0.9924, 0.994)\n",
      "Compliment: (0.9924, 0.994)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.5, 0.083)\n",
      "Compliment: (0.9908, 0.8324)\n",
      "-----------------------------\n",
      "RP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9398, 0.9359)\n",
      "Compliment: (0.9925, 0.9941)\n",
      "Compliment: (0.9925, 0.9941)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.6806, 0.0705)\n",
      "Compliment: (0.992, 0.8638)\n",
      "-----------------------------\n",
      "-RRB- ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9983)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Compliment: (0.9924, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9762, 0.0719)\n",
      "Compliment: (0.9928, 0.8682)\n",
      "-----------------------------\n",
      ": ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.999)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9698, 0.0778)\n",
      "Compliment: (0.9917, 0.8604)\n",
      "-----------------------------\n",
      "MD ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9998)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9989, 0.0892)\n",
      "Compliment: (0.9925, 0.883)\n",
      "-----------------------------\n",
      "IN ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9961, 0.9961)\n",
      "Compliment: (0.992, 0.9937)\n",
      "Compliment: (0.992, 0.9937)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9379, 0.0944)\n",
      "Compliment: (0.9919, 0.8639)\n",
      "-----------------------------\n",
      "WRB ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9994)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9438, 0.0747)\n",
      "Compliment: (0.9926, 0.8816)\n",
      "-----------------------------\n",
      "RBR ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9265, 0.9172)\n",
      "Compliment: (0.9925, 0.994)\n",
      "Compliment: (0.9925, 0.994)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.6103, 0.0637)\n",
      "Compliment: (0.9921, 0.8684)\n",
      "-----------------------------\n",
      "'' ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9999)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Compliment: (0.9923, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.0831)\n",
      "Compliment: (0.9925, 0.8782)\n",
      "-----------------------------\n",
      "VBD ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9727, 0.979)\n",
      "Compliment: (0.993, 0.9944)\n",
      "Compliment: (0.993, 0.9944)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8179, 0.0904)\n",
      "Compliment: (0.9933, 0.8837)\n",
      "-----------------------------\n",
      "+--------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+\n",
      "| Class  | Base Accuracy | Base Confidence | Base Complement Acc | Base Compliment Conf | MAX Accuracy | MAX Confidence | Max compliment acc | Max compliment conf |\n",
      "+--------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+\n",
      "| <pad>  |       0       |        0        |          0          |          0           |      0       |       0        |         0          |          0          |\n",
      "|  NNP   |     0.9944    |      0.9925     |        0.9922       |        0.9941        |    0.7383    |     0.0859     |       0.9918       |        0.8706       |\n",
      "|   RB   |     0.983     |      0.9868     |        0.9926       |        0.9941        |    0.7874    |     0.0779     |       0.9928       |        0.8632       |\n",
      "|   LS   |     0.9231    |      0.9587     |        0.9924       |        0.9939        |    0.6923    |     0.0707     |       0.9922       |        0.8781       |\n",
      "|   ``   |      1.0      |       1.0       |        0.9923       |        0.9939        |     1.0      |     0.085      |       0.9924       |        0.8758       |\n",
      "|   NN   |     0.9801    |      0.9866     |        0.9942       |        0.995         |    0.8536    |     0.1063     |       0.9948       |        0.8688       |\n",
      "|   UH   |      1.0      |      0.983      |        0.9924       |        0.9939        |    0.6667    |     0.0446     |       0.9925       |        0.8648       |\n",
      "|  PDT   |      1.0      |      0.9866     |        0.9924       |        0.9939        |    0.8519    |     0.0484     |       0.9929       |        0.8816       |\n",
      "|  VBN   |     0.9958    |      0.9979     |        0.9923       |        0.9939        |    0.9391    |     0.0902     |       0.9931       |        0.864        |\n",
      "| -NONE- |      1.0      |       1.0       |        0.9918       |        0.9935        |     1.0      |     0.1187     |       0.9912       |        0.8683       |\n",
      "|   TO   |     0.9995    |      0.9999     |        0.9922       |        0.9938        |    0.9986    |     0.0979     |       0.9924       |        0.8757       |\n",
      "|  PRP   |     0.9977    |      0.999      |        0.9923       |        0.9939        |    0.9814    |     0.0794     |       0.9921       |        0.8853       |\n",
      "|  RBS   |     0.9143    |      0.9797     |        0.9924       |        0.9939        |    0.7429    |     0.0901     |       0.9922       |        0.8728       |\n",
      "|   VB   |     0.9914    |      0.9944     |        0.9924       |        0.9939        |    0.8978    |     0.0862     |       0.9914       |        0.8624       |\n",
      "|   WP   |      1.0      |      0.9997     |        0.9923       |        0.9939        |     1.0      |     0.0616     |       0.9922       |        0.8735       |\n",
      "|  SYM   |      0.0      |      0.8962     |        0.9924       |        0.9939        |     0.0      |     0.0398     |       0.9926       |        0.8695       |\n",
      "|  NNS   |     0.9937    |      0.9935     |        0.9923       |        0.994         |    0.9033    |     0.0766     |       0.9934       |        0.8738       |\n",
      "|  WDT   |     0.9888    |      0.9987     |        0.9924       |        0.9939        |    0.9775    |     0.0684     |       0.9925       |        0.8704       |\n",
      "|  POS   |     0.9988    |      0.9998     |        0.9923       |        0.9939        |    0.9587    |     0.0653     |       0.9926       |        0.874        |\n",
      "|   EX   |      1.0      |      0.9984     |        0.9924       |        0.9939        |    0.8523    |     0.0588     |       0.9919       |        0.8716       |\n",
      "|   .    |      1.0      |       1.0       |        0.9921       |        0.9937        |     1.0      |     0.0885     |       0.9923       |        0.8531       |\n",
      "|   #    |      1.0      |      0.9952     |        0.9924       |        0.9939        |     0.75     |     0.0433     |       0.9921       |        0.8792       |\n",
      "|   CD   |     0.9972    |      0.9989     |        0.9922       |        0.9938        |    0.9913    |     0.0862     |       0.9922       |        0.8667       |\n",
      "|   DT   |     0.9962    |      0.9983     |        0.992        |        0.9936        |    0.9728    |     0.1062     |       0.9924       |        0.8673       |\n",
      "|  VBP   |     0.9894    |      0.9847     |        0.9924       |        0.9941        |    0.869     |     0.0655     |       0.9924       |        0.8718       |\n",
      "|  NNPS  |     0.9672    |      0.9172     |        0.9924       |        0.9941        |    0.5861    |     0.065      |       0.9928       |        0.8661       |\n",
      "|  WP$   |      1.0      |      0.9987     |        0.9924       |        0.9939        |     1.0      |     0.0658     |       0.9924       |        0.8838       |\n",
      "| -LRB-  |     0.9917    |      0.9985     |        0.9924       |        0.9939        |    0.9833    |     0.0568     |       0.9922       |        0.8658       |\n",
      "|   $    |      1.0      |      0.9998     |        0.9923       |        0.9939        |     1.0      |     0.0563     |       0.9928       |        0.8783       |\n",
      "|   ,    |     0.9998    |       1.0       |        0.992        |        0.9936        |    0.9998    |     0.1306     |       0.9924       |        0.8659       |\n",
      "|   JJ   |     0.9907    |      0.9939     |        0.9925       |        0.9939        |    0.8315    |     0.093      |       0.9938       |        0.8752       |\n",
      "|   CC   |     0.9943    |      0.9973     |        0.9923       |        0.9939        |    0.9581    |     0.076      |       0.9925       |        0.8754       |\n",
      "|  PRP$  |      1.0      |      0.9995     |        0.9923       |        0.9939        |    0.9961    |     0.0761     |       0.9926       |        0.8743       |\n",
      "|  JJR   |     0.9396    |      0.9495     |        0.9926       |        0.9941        |    0.5433    |     0.0897     |       0.9926       |        0.8707       |\n",
      "|  VBG   |     0.9918    |      0.9944     |        0.9924       |        0.9939        |    0.8548    |     0.062      |       0.993        |        0.8714       |\n",
      "|   FW   |      0.5      |      0.6992     |        0.9924       |        0.994         |     0.5      |     0.083      |       0.9908       |        0.8324       |\n",
      "|   RP   |     0.9398    |      0.9359     |        0.9925       |        0.9941        |    0.6806    |     0.0705     |       0.992        |        0.8638       |\n",
      "| -RRB-  |      1.0      |      0.9983     |        0.9924       |        0.9939        |    0.9762    |     0.0719     |       0.9928       |        0.8682       |\n",
      "|   :    |      1.0      |      0.999      |        0.9923       |        0.9939        |    0.9698    |     0.0778     |       0.9917       |        0.8604       |\n",
      "|   MD   |      1.0      |      0.9998     |        0.9923       |        0.9939        |    0.9989    |     0.0892     |       0.9925       |        0.883        |\n",
      "|   IN   |     0.9961    |      0.9961     |        0.992        |        0.9937        |    0.9379    |     0.0944     |       0.9919       |        0.8639       |\n",
      "|  WRB   |      1.0      |      0.9994     |        0.9923       |        0.9939        |    0.9438    |     0.0747     |       0.9926       |        0.8816       |\n",
      "|  RBR   |     0.9265    |      0.9172     |        0.9925       |        0.994         |    0.6103    |     0.0637     |       0.9921       |        0.8684       |\n",
      "|   ''   |      1.0      |      0.9999     |        0.9923       |        0.9939        |     1.0      |     0.0831     |       0.9925       |        0.8782       |\n",
      "|  VBD   |     0.9727    |      0.979      |        0.993        |        0.9944        |    0.8179    |     0.0904     |       0.9933       |        0.8837       |\n",
      "+--------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from utilities import compute_masks, eval\n",
    "for tok in range(45):\n",
    "    print(idx2tag[tok],\"----------------\")\n",
    "    model.masking_layer = torch.ones(768).to(\"cuda\")\n",
    "    activation_iter = data.DataLoader(dataset=train_dataset+eval_dataset,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=pad)\n",
    "    print(\"Original:\")\n",
    "    enc_dict = eval(model, activation_iter, idx2tag, tag2idx, tok)\n",
    "    class_labels.append(idx2tag[tok])\n",
    "    base_accuracies.append(enc_dict[1][0])\n",
    "    base_confidences.append(enc_dict[1][1])\n",
    "    base_comp_acc.append(enc_dict[2][0])\n",
    "    base_comp_conf.append(enc_dict[2][1])\n",
    "    print(\"Tok:\", enc_dict[1])\n",
    "    print('Compliment:', enc_dict[2])\n",
    "\n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max = compute_masks(enc_dict[0][tok],0.5)\n",
    "    # print(\"STD:\")print(\"Tok:\", enc_dict[1])\n",
    "    print('Compliment:', enc_dict[2])\n",
    "    # model.masking_layer = mask_std.to(\"cuda\")\n",
    "\n",
    "    # enc_dict = eval(model, activation_iter, idx2tag, tag2idx, tok)\n",
    "    print(\"Max:\")\n",
    "    model.masking_layer = mask_max.to(\"cuda\")\n",
    "\n",
    "    enc_dict = eval(model, activation_iter, idx2tag, tag2idx, tok)\n",
    "    max_accuracies.append(enc_dict[1][0])\n",
    "    max_confidences.append(enc_dict[1][1])\n",
    "    max_comp_acc.append(enc_dict[2][0])\n",
    "    max_comp_conf.append(enc_dict[2][1])\n",
    "    print(\"Tok:\", enc_dict[1])\n",
    "    print('Compliment:', enc_dict[2])\n",
    "    print(\"-----------------------------\")\n",
    "    \n",
    "    results_table.add_row([\n",
    "                class_labels[tok],\n",
    "                base_accuracies[tok],\n",
    "                base_confidences[tok],\n",
    "                base_comp_acc[tok],\n",
    "                base_comp_conf[tok],\n",
    "                max_accuracies[tok],\n",
    "                max_confidences[tok],\n",
    "                max_comp_acc[tok],\n",
    "                max_comp_conf[tok],\n",
    "            ])\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m enc_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx2tag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m33\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:340\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, iterator, idx2tag, tag2idx, tok)\u001b[0m\n\u001b[1;32m    337\u001b[0m tag_confidence_sum \u001b[38;5;241m=\u001b[39m {tag: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tag2idx}\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterator):\n\u001b[1;32m    341\u001b[0m         words, x, is_heads, tags, y, seqlens \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    342\u001b[0m         enc, _, _, y_hat, confidence \u001b[38;5;241m=\u001b[39m model(x, y)  \u001b[38;5;66;03m# y_hat: (N, T)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc_dict = eval(model, activation_iter, idx2tag, tag2idx,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_masks(fc_vals, percent):\n",
    "    # Convert input to numpy array\n",
    "    fc_vals_array = np.array(fc_vals)\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_vals = np.mean(np.abs(fc_vals_array), axis=0)\n",
    "    std_vals = np.std(fc_vals_array, axis=0)\n",
    "    min_vals = np.min(fc_vals_array, axis=0)\n",
    "    max_vals = np.max(fc_vals_array, axis=0)\n",
    "    \n",
    "    # Normalize standard deviation\n",
    "    std_vals_normalized = (std_vals - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    mean_vals_tensor = torch.from_numpy(mean_vals)\n",
    "    std_vals_tensor = torch.from_numpy(std_vals_normalized)\n",
    "    \n",
    "    # Compute masks\n",
    "    mask_max = compute_max_mask(mean_vals_tensor, percent)\n",
    "    mask_std = compute_std_mask(std_vals_tensor, percent)\n",
    "    mask_max_low_std = compute_max_low_std_mask(mean_vals_tensor, std_vals_tensor, percent)\n",
    "    mask_intersection = torch.logical_or(mask_std, mask_max).float()\n",
    "    \n",
    "    return mask_max, mask_std, mask_intersection, mask_max_low_std\n",
    "\n",
    "def compute_max_mask(values, percent):\n",
    "    sorted_indices = torch.argsort(values, descending=True)\n",
    "    mask_count = int(percent * len(values))\n",
    "    mask = torch.ones_like(values)\n",
    "    mask[sorted_indices[:mask_count]] = 0.0\n",
    "    return mask\n",
    "\n",
    "def compute_std_mask(values, percent):\n",
    "    sorted_indices = torch.argsort(values, descending=False)\n",
    "    mask_count = int(percent * len(values))\n",
    "    mask = torch.ones_like(values)\n",
    "    mask[sorted_indices[:mask_count]] = 0.0\n",
    "    return mask\n",
    "\n",
    "def compute_max_low_std_mask(mean_vals, std_vals, percent):\n",
    "    # Get indices of bottom 50% std values\n",
    "    bottom_50_percent_std_count = int(0.99 * len(std_vals))\n",
    "    bottom_50_percent_std_indices = torch.argsort(std_vals)[:bottom_50_percent_std_count]\n",
    "    \n",
    "    # Create a mask for bottom 50% std values\n",
    "    bottom_50_percent_std_mask = torch.zeros_like(std_vals, dtype=torch.bool)\n",
    "    bottom_50_percent_std_mask[bottom_50_percent_std_indices] = True\n",
    "    \n",
    "    # Filter mean values\n",
    "    mean_vals_filtered = mean_vals.clone()\n",
    "    mean_vals_filtered[~bottom_50_percent_std_mask] = float('-inf')\n",
    "    \n",
    "    # Compute mask\n",
    "    return compute_max_mask(mean_vals_filtered, percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 0\n",
      "Layer 38 1\n",
      "Layer 44 2\n",
      "Layer 20 3\n",
      "Layer 10 4\n",
      "Layer 17 5\n",
      "Layer 24 6\n",
      "Layer 15 7\n",
      "Layer 32 8\n",
      "Layer 8 9\n",
      "Layer 25 10\n",
      "Layer 9 11\n",
      "Layer 41 12\n",
      "Layer 30 13\n",
      "Layer 39 14\n",
      "Layer 7 15\n",
      "Layer 2 16\n",
      "Layer 22 17\n",
      "Layer 31 18\n",
      "Layer 12 19\n",
      "Layer 26 20\n",
      "Layer 27 21\n",
      "Layer 11 22\n",
      "Layer 46 23\n",
      "Layer 23 24\n",
      "Layer 40 25\n",
      "Layer 4 26\n",
      "Layer 19 27\n",
      "Layer 45 28\n",
      "Layer 43 29\n",
      "Layer 42 30\n",
      "Layer 14 31\n",
      "Layer 33 32\n",
      "Layer 37 33\n",
      "Layer 34 34\n",
      "Layer 36 35\n",
      "Layer 21 36\n",
      "Layer 16 37\n",
      "Layer 29 38\n",
      "Layer 5 39\n",
      "Layer 28 40\n",
      "Layer 35 41\n",
      "Layer 13 42\n",
      "Layer 1 43\n",
      "Layer 6 44\n",
      "Layer 3 45\n",
      "Layer 18 46\n"
     ]
    }
   ],
   "source": [
    "for i, fc in enumerate(enc_dict):\n",
    "    print(f\"Layer {fc}\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, fc1 in enumerate(enc_dict):\n",
    "    tag = idx2tag[fc1]\n",
    "    fc1 = enc_dict[fc1]\n",
    "    \n",
    "    fc1 = np.array(fc1)\n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std = compute_masks(fc1, 0.15)\n",
    "    \n",
    "    m = np.mean(np.abs(fc1), axis=0)\n",
    "    s = np.std(fc1, axis=0)\n",
    "    min_val = np.min(fc1, axis=0)\n",
    "    max_val = np.max(fc1, axis=0)\n",
    "    \n",
    "    # Normalize std and mean\n",
    "    s_norm = (s - min_val) / (max_val - min_val)\n",
    "    m_norm = m#(m - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create indices for different masks\n",
    "    indices_max = np.where(mask_max == 0)[0]\n",
    "    indices_std = np.where(mask_std == 0)[0]\n",
    "    indices_intersection = np.intersect1d(indices_max, indices_std)\n",
    "    indices_max_minus_std = np.setdiff1d(indices_max, indices_std)\n",
    "    indices_std_minus_max = np.setdiff1d(indices_std, indices_max)\n",
    "    \n",
    "    # Count the indices in each set\n",
    "    count_all = len(m_norm)\n",
    "    count_max = len(indices_max)\n",
    "    count_std = len(indices_std)\n",
    "    count_intersection = len(indices_intersection)\n",
    "    count_max_minus_std = len(indices_max_minus_std)\n",
    "    count_std_minus_max = len(indices_std_minus_max)\n",
    "    \n",
    "    out = Output()\n",
    "    with out:\n",
    "        # Create subplots with counts in titles\n",
    "        fig = make_subplots(rows=2, cols=3, \n",
    "                            subplot_titles=(f\"All Activations (Count: {count_all})\",\n",
    "                                            f\"Max Mask (Count: {count_max})\", \n",
    "                                            f\"Std Mask (Count: {count_std})\", \n",
    "                                            f\"Intersection (Count: {count_intersection})\",\n",
    "                                            f\"Max - Std (Count: {count_max_minus_std})\", \n",
    "                                            f\"Std - Max (Count: {count_std_minus_max})\"))\n",
    "        \n",
    "        # Helper function to add traces\n",
    "        def add_traces(indices, row, col):\n",
    "            indices_list = list(indices)  # Convert range or numpy array to list\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices_list,\n",
    "                    y=m_norm[indices_list],\n",
    "                    mode='markers',\n",
    "                    name='Mean',\n",
    "                    marker=dict(size=3, color='blue'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices_list,\n",
    "                    y=s_norm[indices_list],\n",
    "                    mode='markers',\n",
    "                    name='Std Dev',\n",
    "                    marker=dict(size=3, color='red'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            for j in indices_list:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[j, j],\n",
    "                        y=[m_norm[j], s_norm[j]],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='gray', width=0.5),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        \n",
    "        # Add traces for all activations\n",
    "        add_traces(range(len(m_norm)), 1, 1)\n",
    "        \n",
    "        # Add traces for other plots\n",
    "        add_traces(indices_max, 1, 2)\n",
    "        add_traces(indices_std, 1, 3)\n",
    "        add_traces(indices_intersection, 2, 1)\n",
    "        add_traces(indices_max_minus_std, 2, 2)\n",
    "        add_traces(indices_std_minus_max, 2, 3)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Mean and Standard Deviation of Activations - Class {i+1}'+ tag,\n",
    "            height=1200,\n",
    "            width=1800,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update x and y axis labels for all subplots\n",
    "        for row in range(1, 3):\n",
    "            for col in range(1, 4):\n",
    "                fig.update_xaxes(title_text=\"Activation Index\", row=row, col=col)\n",
    "                fig.update_yaxes(title_text=\"Normalized Value\", row=row, col=col)\n",
    "        \n",
    "        display(fig)\n",
    "    \n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "from utilities import compute_masks\n",
    "from IPython.display import display\n",
    "\n",
    "def create_index_tracking_plot(indices_per_class, title):\n",
    "    num_classes = len(indices_per_class)\n",
    "    all_indices = sorted(set.union(*[set(indices) for indices in indices_per_class]))\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Create a color scale\n",
    "    color_scale = px.colors.diverging.RdYlGn_r  # Red to Yellow to Green color scale\n",
    "\n",
    "    # Add edges for indices present in multiple classes\n",
    "    for idx in all_indices:\n",
    "        classes_with_idx = [i for i, indices in enumerate(indices_per_class) if idx in indices]\n",
    "        if len(classes_with_idx) > 1:\n",
    "            x = [idx] * len(classes_with_idx)\n",
    "            y = classes_with_idx\n",
    "            color_index = (len(classes_with_idx) - 1) / (num_classes - 1)  # Normalize to [0, 1]\n",
    "            edge_color = px.colors.sample_colorscale(color_scale, [color_index])[0]\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode='lines',\n",
    "                line=dict(color=edge_color, width=2),\n",
    "                hoverinfo='text',\n",
    "                hovertext=f'Index: {idx}<br>Present in {len(classes_with_idx)} classes',\n",
    "                showlegend=False\n",
    "            ))\n",
    "    \n",
    "    # Add scatter plots for each class\n",
    "    for class_idx, indices in enumerate(indices_per_class):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=indices,\n",
    "            y=[class_idx] * len(indices),\n",
    "            mode='markers',\n",
    "            name=f'Class {class_idx + 1}',\n",
    "            marker=dict(size=4, symbol='circle', color='black'),\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f'Index: {idx}<br>Class: {class_idx + 1}' for idx in indices]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='Activation Index',\n",
    "        yaxis_title='Class',\n",
    "        yaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=list(range(num_classes)),\n",
    "            ticktext=[f'Class {i+1}' for i in range(num_classes)]\n",
    "        ),\n",
    "        hovermode='closest',\n",
    "        width=1500,\n",
    "        height=800,\n",
    "        plot_bgcolor='white',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    \n",
    "    # Add color bar\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            colorscale=color_scale,\n",
    "            showscale=True,\n",
    "            cmin=1,\n",
    "            cmax=num_classes,\n",
    "            colorbar=dict(\n",
    "                title='Number of Classes',\n",
    "                tickvals=list(range(1, num_classes+1)),\n",
    "                ticktext=list(range(1, num_classes+1))\n",
    "            )\n",
    "        ),\n",
    "        hoverinfo='none',\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Collect indices for each class\n",
    "max_indices_per_class = []\n",
    "std_indices_per_class = []\n",
    "\n",
    "for fc1 in all_fc_vals:\n",
    "    mask_max, mask_std = compute_masks(fc1, 0.15)\n",
    "    max_indices_per_class.append(np.where(mask_max == 0)[0])\n",
    "    std_indices_per_class.append(np.where(mask_std == 0)[0])\n",
    "\n",
    "# Create and display visualizations\n",
    "output_widgets = []\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "    fig_max = create_index_tracking_plot(max_indices_per_class, 'Max Mask Indices Across Classes')\n",
    "    display(fig_max)\n",
    "output_widgets.append(out)\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "    fig_std = create_index_tracking_plot(std_indices_per_class, 'Std Mask Indices Across Classes')\n",
    "    display(fig_std)\n",
    "output_widgets.append(out)\n",
    "\n",
    "# Display all visualizations\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
