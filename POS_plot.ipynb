{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d1/grad/mha361/anaconda3/envs/memit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.012983798980713\n",
      "step: 10, loss: 0.3446679413318634\n",
      "step: 20, loss: 0.28085753321647644\n",
      "step: 30, loss: 0.12452409416437149\n",
      "step: 40, loss: 0.11375977098941803\n",
      "step: 50, loss: 0.12363813072443008\n",
      "step: 60, loss: 0.0389459989964962\n",
      "step: 70, loss: 0.08982899785041809\n",
      "step: 80, loss: 0.16142012178897858\n",
      "step: 90, loss: 0.25490763783454895\n",
      "step: 100, loss: 0.12713389098644257\n",
      "step: 110, loss: 0.11952819675207138\n",
      "step: 120, loss: 0.07232357561588287\n",
      "step: 130, loss: 0.09034412354230881\n",
      "step: 140, loss: 0.06357061862945557\n",
      "step: 150, loss: 0.05770187824964523\n",
      "step: 160, loss: 0.04031563177704811\n",
      "step: 170, loss: 0.0304564218968153\n",
      "step: 180, loss: 0.0903044268488884\n",
      "step: 190, loss: 0.05765240266919136\n",
      "step: 200, loss: 0.11337007582187653\n",
      "step: 210, loss: 0.05553313344717026\n",
      "step: 220, loss: 0.15457281470298767\n",
      "step: 230, loss: 0.13043850660324097\n",
      "step: 240, loss: 0.07155825197696686\n",
      "step: 250, loss: 0.06299138069152832\n",
      "step: 260, loss: 0.06146363541483879\n",
      "step: 270, loss: 0.08767524361610413\n",
      "step: 280, loss: 0.0659414529800415\n",
      "step: 290, loss: 0.06431438773870468\n",
      "step: 300, loss: 0.16332080960273743\n",
      "step: 310, loss: 0.10949110239744186\n",
      "step: 320, loss: 0.14823183417320251\n",
      "step: 330, loss: 0.08775263279676437\n",
      "step: 340, loss: 0.041348379105329514\n",
      "step: 350, loss: 0.043348658829927444\n",
      "step: 360, loss: 0.08066048473119736\n",
      "step: 370, loss: 0.09882175922393799\n",
      "step: 380, loss: 0.056578926742076874\n",
      "step: 390, loss: 0.11573680490255356\n",
      "step: 400, loss: 0.13915172219276428\n",
      "step: 410, loss: 0.08141694962978363\n",
      "step: 420, loss: 0.15173974633216858\n",
      "step: 430, loss: 0.22390219569206238\n",
      "step: 440, loss: 0.07425839453935623\n",
      "step: 0, loss: 0.030312109738588333\n",
      "step: 10, loss: 0.030063612386584282\n",
      "step: 20, loss: 0.0719703957438469\n",
      "step: 30, loss: 0.05952499434351921\n",
      "step: 40, loss: 0.15599775314331055\n",
      "step: 50, loss: 0.02800208143889904\n",
      "step: 60, loss: 0.05725758895277977\n",
      "step: 70, loss: 0.08546363562345505\n",
      "step: 80, loss: 0.13683496415615082\n",
      "step: 90, loss: 0.11839372664690018\n",
      "step: 100, loss: 0.029971105977892876\n",
      "step: 110, loss: 0.0657137930393219\n",
      "step: 120, loss: 0.04245559871196747\n",
      "step: 130, loss: 0.14332149922847748\n",
      "step: 140, loss: 0.12965461611747742\n",
      "step: 150, loss: 0.08296185731887817\n",
      "step: 160, loss: 0.0660480186343193\n",
      "step: 170, loss: 0.05608343705534935\n",
      "step: 180, loss: 0.08614634722471237\n",
      "step: 190, loss: 0.023624805733561516\n",
      "step: 200, loss: 0.09866144508123398\n",
      "step: 210, loss: 0.11273543536663055\n",
      "step: 220, loss: 0.09042652696371078\n",
      "step: 230, loss: 0.05392903834581375\n",
      "step: 240, loss: 0.08578074723482132\n",
      "step: 250, loss: 0.0550566203892231\n",
      "step: 260, loss: 0.027223920449614525\n",
      "step: 270, loss: 0.04365214705467224\n",
      "step: 280, loss: 0.0629417821764946\n",
      "step: 290, loss: 0.12285961210727692\n",
      "step: 300, loss: 0.14162179827690125\n",
      "step: 310, loss: 0.04579972103238106\n",
      "step: 320, loss: 0.01705881580710411\n",
      "step: 330, loss: 0.044622328132390976\n",
      "step: 340, loss: 0.08734126389026642\n",
      "step: 350, loss: 0.06106473505496979\n",
      "step: 360, loss: 0.0745084211230278\n",
      "step: 370, loss: 0.14973308145999908\n",
      "step: 380, loss: 0.0741538554430008\n",
      "step: 390, loss: 0.06299329549074173\n",
      "step: 400, loss: 0.0821438729763031\n",
      "step: 410, loss: 0.08012598752975464\n",
      "step: 420, loss: 0.12240170687437057\n",
      "step: 430, loss: 0.09040115773677826\n",
      "step: 440, loss: 0.030744489282369614\n",
      "step: 0, loss: 0.07467658817768097\n",
      "step: 10, loss: 0.06599672138690948\n",
      "step: 20, loss: 0.030211806297302246\n",
      "step: 30, loss: 0.02274482697248459\n",
      "step: 40, loss: 0.07733001559972763\n",
      "step: 50, loss: 0.033738888800144196\n",
      "step: 60, loss: 0.06707464158535004\n",
      "step: 70, loss: 0.03442884981632233\n",
      "step: 80, loss: 0.10576058179140091\n",
      "step: 90, loss: 0.026052653789520264\n",
      "step: 100, loss: 0.08360922336578369\n",
      "step: 110, loss: 0.07640109956264496\n",
      "step: 120, loss: 0.058018263429403305\n",
      "step: 130, loss: 0.09924125671386719\n",
      "step: 140, loss: 0.03185577690601349\n",
      "step: 150, loss: 0.05394044145941734\n",
      "step: 160, loss: 0.04012783244252205\n",
      "step: 170, loss: 0.02888902835547924\n",
      "step: 180, loss: 0.051499225199222565\n",
      "step: 190, loss: 0.04986364394426346\n",
      "step: 200, loss: 0.016552641987800598\n",
      "step: 210, loss: 0.010934869758784771\n",
      "step: 220, loss: 0.024435918778181076\n",
      "step: 230, loss: 0.026930445805191994\n",
      "step: 240, loss: 0.058472443372011185\n",
      "step: 250, loss: 0.024912536144256592\n",
      "step: 260, loss: 0.0934138372540474\n",
      "step: 270, loss: 0.066737599670887\n",
      "step: 280, loss: 0.03591957315802574\n",
      "step: 290, loss: 0.15648329257965088\n",
      "step: 300, loss: 0.14831960201263428\n",
      "step: 310, loss: 0.007903450168669224\n",
      "step: 320, loss: 0.07243990153074265\n",
      "step: 330, loss: 0.023013437166810036\n",
      "step: 340, loss: 0.050498414784669876\n",
      "step: 350, loss: 0.06349004060029984\n",
      "step: 360, loss: 0.06314832717180252\n",
      "step: 370, loss: 0.07842544466257095\n",
      "step: 380, loss: 0.014464297331869602\n",
      "step: 390, loss: 0.046708811074495316\n",
      "step: 400, loss: 0.045959532260894775\n",
      "step: 410, loss: 0.04678671061992645\n",
      "step: 420, loss: 0.026916055008769035\n",
      "step: 430, loss: 0.07069861888885498\n",
      "step: 440, loss: 0.052671242505311966\n",
      "step: 0, loss: 0.036232296377420425\n",
      "step: 10, loss: 0.021116293966770172\n",
      "step: 20, loss: 0.026483235880732536\n",
      "step: 30, loss: 0.030097290873527527\n",
      "step: 40, loss: 0.018959766253829002\n",
      "step: 50, loss: 0.0335409939289093\n",
      "step: 60, loss: 0.046252891421318054\n",
      "step: 70, loss: 0.09645026922225952\n",
      "step: 80, loss: 0.05881587788462639\n",
      "step: 90, loss: 0.07736536860466003\n",
      "step: 100, loss: 0.08151863515377045\n",
      "step: 110, loss: 0.04069089516997337\n",
      "step: 120, loss: 0.055093832314014435\n",
      "step: 130, loss: 0.075208380818367\n",
      "step: 140, loss: 0.06467641144990921\n",
      "step: 150, loss: 0.013452524319291115\n",
      "step: 160, loss: 0.0928039476275444\n",
      "step: 170, loss: 0.12899811565876007\n",
      "step: 180, loss: 0.022679295390844345\n",
      "step: 190, loss: 0.053450096398591995\n",
      "step: 200, loss: 0.13494756817817688\n",
      "step: 210, loss: 0.07598774880170822\n",
      "step: 220, loss: 0.03479701280593872\n",
      "step: 230, loss: 0.013475164771080017\n",
      "step: 240, loss: 0.03693803399801254\n",
      "step: 250, loss: 0.09252773970365524\n",
      "step: 260, loss: 0.02790922112762928\n",
      "step: 270, loss: 0.059043146669864655\n",
      "step: 280, loss: 0.028112400323152542\n",
      "step: 290, loss: 0.06910835206508636\n",
      "step: 300, loss: 0.06477782130241394\n",
      "step: 310, loss: 0.07873669266700745\n",
      "step: 320, loss: 0.017014596611261368\n",
      "step: 330, loss: 0.07242317497730255\n",
      "step: 340, loss: 0.047422558069229126\n",
      "step: 350, loss: 0.03952749818563461\n",
      "step: 360, loss: 0.04703373461961746\n",
      "step: 370, loss: 0.03621998801827431\n",
      "step: 380, loss: 0.22031711041927338\n",
      "step: 390, loss: 0.016673486679792404\n",
      "step: 400, loss: 0.01138247549533844\n",
      "step: 410, loss: 0.04099060595035553\n",
      "step: 420, loss: 0.07866403460502625\n",
      "step: 430, loss: 0.041762273758649826\n",
      "step: 440, loss: 0.05954502150416374\n",
      "step: 0, loss: 0.06697487831115723\n",
      "step: 10, loss: 0.014467467553913593\n",
      "step: 20, loss: 0.0441945344209671\n",
      "step: 30, loss: 0.04140416905283928\n",
      "step: 40, loss: 0.02143341489136219\n",
      "step: 50, loss: 0.009226079098880291\n",
      "step: 60, loss: 0.01342849899083376\n",
      "step: 70, loss: 0.030362095683813095\n",
      "step: 80, loss: 0.01869230717420578\n",
      "step: 90, loss: 0.06349816173315048\n",
      "step: 100, loss: 0.03687041997909546\n",
      "step: 110, loss: 0.007848971523344517\n",
      "step: 120, loss: 0.05371136590838432\n",
      "step: 130, loss: 0.058537762612104416\n",
      "step: 140, loss: 0.0962718203663826\n",
      "step: 150, loss: 0.04786296561360359\n",
      "step: 160, loss: 0.007747233845293522\n",
      "step: 170, loss: 0.10387904942035675\n",
      "step: 180, loss: 0.028034687042236328\n",
      "step: 190, loss: 0.004938135389238596\n",
      "step: 200, loss: 0.006067055743187666\n",
      "step: 210, loss: 0.03935878351330757\n",
      "step: 220, loss: 0.009157619439065456\n",
      "step: 230, loss: 0.07733992487192154\n",
      "step: 240, loss: 0.10795971751213074\n",
      "step: 250, loss: 0.08084627985954285\n",
      "step: 260, loss: 0.07158318907022476\n",
      "step: 270, loss: 0.13829782605171204\n",
      "step: 280, loss: 0.04788055270910263\n",
      "step: 290, loss: 0.05383281037211418\n",
      "step: 300, loss: 0.02302188239991665\n",
      "step: 310, loss: 0.06392225623130798\n",
      "step: 320, loss: 0.056802354753017426\n",
      "step: 330, loss: 0.051169246435165405\n",
      "step: 340, loss: 0.06410808861255646\n",
      "step: 350, loss: 0.0433829091489315\n",
      "step: 360, loss: 0.06395061314105988\n",
      "step: 370, loss: 0.06634706258773804\n",
      "step: 380, loss: 0.02273538149893284\n",
      "step: 390, loss: 0.09050747007131577\n",
      "step: 400, loss: 0.027655355632305145\n",
      "step: 410, loss: 0.03356429561972618\n",
      "step: 420, loss: 0.10877791047096252\n",
      "step: 430, loss: 0.057661376893520355\n",
      "step: 440, loss: 0.027398528531193733\n",
      "step: 0, loss: 0.0053192563354969025\n",
      "step: 10, loss: 0.0748455598950386\n",
      "step: 20, loss: 0.013201781548559666\n",
      "step: 30, loss: 0.01216062344610691\n",
      "step: 40, loss: 0.08277486264705658\n",
      "step: 50, loss: 0.06359907239675522\n",
      "step: 60, loss: 0.04260686784982681\n",
      "step: 70, loss: 0.012763322331011295\n",
      "step: 80, loss: 0.03145937994122505\n",
      "step: 90, loss: 0.06375458091497421\n",
      "step: 100, loss: 0.029153987765312195\n",
      "step: 110, loss: 0.04380768910050392\n",
      "step: 120, loss: 0.008873384445905685\n",
      "step: 130, loss: 0.045569904148578644\n",
      "step: 140, loss: 0.0707566887140274\n",
      "step: 150, loss: 0.04063240811228752\n",
      "step: 160, loss: 0.04059002920985222\n",
      "step: 170, loss: 0.023619892075657845\n",
      "step: 180, loss: 0.15884289145469666\n",
      "step: 190, loss: 0.0761859193444252\n",
      "step: 200, loss: 0.041124939918518066\n",
      "step: 210, loss: 0.05045802518725395\n",
      "step: 220, loss: 0.0025341548025608063\n",
      "step: 230, loss: 0.030969534069299698\n",
      "step: 240, loss: 0.0947716012597084\n",
      "step: 250, loss: 0.0688730850815773\n",
      "step: 260, loss: 0.10540656000375748\n",
      "step: 270, loss: 0.11982052028179169\n",
      "step: 280, loss: 0.0527079738676548\n",
      "step: 290, loss: 0.027635175734758377\n",
      "step: 300, loss: 0.04890526831150055\n",
      "step: 310, loss: 0.07683999836444855\n",
      "step: 320, loss: 0.009916985407471657\n",
      "step: 330, loss: 0.014825726859271526\n",
      "step: 340, loss: 0.06198319420218468\n",
      "step: 350, loss: 0.08671919256448746\n",
      "step: 360, loss: 0.045112576335668564\n",
      "step: 370, loss: 0.020404407754540443\n",
      "step: 380, loss: 0.04679500684142113\n",
      "step: 390, loss: 0.018023356795310974\n",
      "step: 400, loss: 0.04594969376921654\n",
      "step: 410, loss: 0.031465768814086914\n",
      "step: 420, loss: 0.03281892091035843\n",
      "step: 430, loss: 0.016146386042237282\n",
      "step: 440, loss: 0.0014416464837267995\n",
      "step: 0, loss: 0.019365835934877396\n",
      "step: 10, loss: 0.043010447174310684\n",
      "step: 20, loss: 0.05609492212533951\n",
      "step: 30, loss: 0.023194732144474983\n",
      "step: 40, loss: 0.04380427673459053\n",
      "step: 50, loss: 0.02482197992503643\n",
      "step: 60, loss: 0.06552933156490326\n",
      "step: 70, loss: 0.06903282552957535\n",
      "step: 80, loss: 0.031333714723587036\n",
      "step: 90, loss: 0.004385283216834068\n",
      "step: 100, loss: 0.06434290111064911\n",
      "step: 110, loss: 0.045440591871738434\n",
      "step: 120, loss: 0.006229888182133436\n",
      "step: 130, loss: 0.04746876657009125\n",
      "step: 140, loss: 0.04007246345281601\n",
      "step: 150, loss: 0.004129163455218077\n",
      "step: 160, loss: 0.07390723377466202\n",
      "step: 170, loss: 0.055049698799848557\n",
      "step: 180, loss: 0.019738495349884033\n",
      "step: 190, loss: 0.0056366631761193275\n",
      "step: 200, loss: 0.011908582411706448\n",
      "step: 210, loss: 0.0501507930457592\n",
      "step: 220, loss: 0.016423964872956276\n",
      "step: 230, loss: 0.06141672655940056\n",
      "step: 240, loss: 0.0656091719865799\n",
      "step: 250, loss: 0.09880062937736511\n",
      "step: 260, loss: 0.013290883973240852\n",
      "step: 270, loss: 0.058081455528736115\n",
      "step: 280, loss: 0.013863423839211464\n",
      "step: 290, loss: 0.03501741215586662\n",
      "step: 300, loss: 0.02598723955452442\n",
      "step: 310, loss: 0.04440869390964508\n",
      "step: 320, loss: 0.035632967948913574\n",
      "step: 330, loss: 0.009978328831493855\n",
      "step: 340, loss: 0.019050216302275658\n",
      "step: 350, loss: 0.03455072268843651\n",
      "step: 360, loss: 0.05557330325245857\n",
      "step: 370, loss: 0.07764564454555511\n",
      "step: 380, loss: 0.007992308586835861\n",
      "step: 390, loss: 0.12785354256629944\n",
      "step: 400, loss: 0.10178421437740326\n",
      "step: 410, loss: 0.017691709101200104\n",
      "step: 420, loss: 0.040008220821619034\n",
      "step: 430, loss: 0.03621872141957283\n",
      "step: 440, loss: 0.024683848023414612\n",
      "step: 0, loss: 0.017532141879200935\n",
      "step: 10, loss: 0.06964952498674393\n",
      "step: 20, loss: 0.008444396778941154\n",
      "step: 30, loss: 0.0065129525028169155\n",
      "step: 40, loss: 0.002044023247435689\n",
      "step: 50, loss: 0.007964229211211205\n",
      "step: 60, loss: 0.006762537639588118\n",
      "step: 70, loss: 0.0019022601190954447\n",
      "step: 80, loss: 0.02848842926323414\n",
      "step: 90, loss: 0.04010314121842384\n",
      "step: 100, loss: 0.012212919071316719\n",
      "step: 110, loss: 0.029377199709415436\n",
      "step: 120, loss: 0.02776242233812809\n",
      "step: 130, loss: 0.007447977550327778\n",
      "step: 140, loss: 0.021009234711527824\n",
      "step: 150, loss: 0.07264012843370438\n",
      "step: 160, loss: 0.03821300342679024\n",
      "step: 170, loss: 0.013403144665062428\n",
      "step: 180, loss: 0.015507718548178673\n",
      "step: 190, loss: 0.035143669694662094\n",
      "step: 200, loss: 0.010641993954777718\n",
      "step: 210, loss: 0.027435990050435066\n",
      "step: 220, loss: 0.04956182837486267\n",
      "step: 230, loss: 0.025518188253045082\n",
      "step: 240, loss: 0.031216906383633614\n",
      "step: 250, loss: 0.037766024470329285\n",
      "step: 260, loss: 0.06356237083673477\n",
      "step: 270, loss: 0.015025925822556019\n",
      "step: 280, loss: 0.011969123035669327\n",
      "step: 290, loss: 0.048310067504644394\n",
      "step: 300, loss: 0.02840334363281727\n",
      "step: 310, loss: 0.02537013217806816\n",
      "step: 320, loss: 0.05917998030781746\n",
      "step: 330, loss: 0.013486841693520546\n",
      "step: 340, loss: 0.061989374458789825\n",
      "step: 350, loss: 0.02019278146326542\n",
      "step: 360, loss: 0.03848351165652275\n",
      "step: 370, loss: 0.04529443010687828\n",
      "step: 380, loss: 0.10309341549873352\n",
      "step: 390, loss: 0.0556185208261013\n",
      "step: 400, loss: 0.11265180259943008\n",
      "step: 410, loss: 0.1355554461479187\n",
      "step: 420, loss: 0.09845523536205292\n",
      "step: 430, loss: 0.0480920746922493\n",
      "step: 440, loss: 0.04897360876202583\n",
      "step: 0, loss: 0.1082722395658493\n",
      "step: 10, loss: 0.1056554764509201\n",
      "step: 20, loss: 0.02459896169602871\n",
      "step: 30, loss: 0.049926675856113434\n",
      "step: 40, loss: 0.03524649515748024\n",
      "step: 50, loss: 0.028033852577209473\n",
      "step: 60, loss: 0.06607569009065628\n",
      "step: 70, loss: 0.11986065655946732\n",
      "step: 80, loss: 0.00999746099114418\n",
      "step: 90, loss: 0.03469228744506836\n",
      "step: 100, loss: 0.09422173351049423\n",
      "step: 110, loss: 0.04794315993785858\n",
      "step: 120, loss: 0.09326985478401184\n",
      "step: 130, loss: 0.04075339436531067\n",
      "step: 140, loss: 0.03179731220006943\n",
      "step: 150, loss: 0.05235971137881279\n",
      "step: 160, loss: 0.07199546694755554\n",
      "step: 170, loss: 0.0386715903878212\n",
      "step: 180, loss: 0.018876472488045692\n",
      "step: 190, loss: 0.0170375257730484\n",
      "step: 200, loss: 0.011183256283402443\n",
      "step: 210, loss: 0.11351531744003296\n",
      "step: 220, loss: 0.07803080976009369\n",
      "step: 230, loss: 0.02603914774954319\n",
      "step: 240, loss: 0.030577797442674637\n",
      "step: 250, loss: 0.008435030467808247\n",
      "step: 260, loss: 0.01868594065308571\n",
      "step: 270, loss: 0.029621291905641556\n",
      "step: 280, loss: 0.022290237247943878\n",
      "step: 290, loss: 0.05912957713007927\n",
      "step: 300, loss: 0.010931969620287418\n",
      "step: 310, loss: 0.002951218979433179\n",
      "step: 320, loss: 0.03270580992102623\n",
      "step: 330, loss: 0.009503936395049095\n",
      "step: 340, loss: 0.01027415320277214\n",
      "step: 350, loss: 0.0037782026920467615\n",
      "step: 360, loss: 0.016040323302149773\n",
      "step: 370, loss: 0.03288835659623146\n",
      "step: 380, loss: 0.03770887851715088\n",
      "step: 390, loss: 0.012129459530115128\n",
      "step: 400, loss: 0.07156824320554733\n",
      "step: 410, loss: 0.013284591026604176\n",
      "step: 420, loss: 0.028605923056602478\n",
      "step: 430, loss: 0.09369925409555435\n",
      "step: 440, loss: 0.003476747078821063\n",
      "step: 0, loss: 0.020160937681794167\n",
      "step: 10, loss: 0.025994542986154556\n",
      "step: 20, loss: 0.05859486758708954\n",
      "step: 30, loss: 0.02730877883732319\n",
      "step: 40, loss: 0.03841150924563408\n",
      "step: 50, loss: 0.01952572725713253\n",
      "step: 60, loss: 0.016710493713617325\n",
      "step: 70, loss: 0.0020178840495646\n",
      "step: 80, loss: 0.006671318784356117\n",
      "step: 90, loss: 0.016174044460058212\n",
      "step: 100, loss: 0.027248119935393333\n",
      "step: 110, loss: 0.06720463931560516\n",
      "step: 120, loss: 0.027627287432551384\n",
      "step: 130, loss: 0.042174313217401505\n",
      "step: 140, loss: 0.01188385859131813\n",
      "step: 150, loss: 0.007067212834954262\n",
      "step: 160, loss: 0.02221713960170746\n",
      "step: 170, loss: 0.018549079075455666\n",
      "step: 180, loss: 0.04450337588787079\n",
      "step: 190, loss: 0.03736228868365288\n",
      "step: 200, loss: 0.010754173621535301\n",
      "step: 210, loss: 0.048743706196546555\n",
      "step: 220, loss: 0.007363826967775822\n",
      "step: 230, loss: 0.03302197903394699\n",
      "step: 240, loss: 0.022790469229221344\n",
      "step: 250, loss: 0.014097251929342747\n",
      "step: 260, loss: 0.03768296167254448\n",
      "step: 270, loss: 0.03096628189086914\n",
      "step: 280, loss: 0.022250905632972717\n",
      "step: 290, loss: 0.023033706471323967\n",
      "step: 300, loss: 0.01243104413151741\n",
      "step: 310, loss: 0.012167029082775116\n",
      "step: 320, loss: 0.026561500504612923\n",
      "step: 330, loss: 0.05028502270579338\n",
      "step: 340, loss: 0.026300685480237007\n",
      "step: 350, loss: 0.05852648615837097\n",
      "step: 360, loss: 0.04507974162697792\n",
      "step: 370, loss: 0.0575261153280735\n",
      "step: 380, loss: 0.049031373113393784\n",
      "step: 390, loss: 0.017449598759412766\n",
      "step: 400, loss: 0.011008739471435547\n",
      "step: 410, loss: 0.040162600576877594\n",
      "step: 420, loss: 0.021063368767499924\n",
      "step: 430, loss: 0.039982009679079056\n",
      "step: 440, loss: 0.004164660815149546\n",
      "<pad>: N/A (0 occurrences)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from utilities import train, eval, pad, get_model_bert\n",
    "from POS_dataset import PosDataset\n",
    "from prettytable import PrettyTable\n",
    "import nltk\n",
    "tagged_sents = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "tags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))\n",
    "\n",
    "\",\".join(tags)\n",
    "\n",
    "tags = [\"<pad>\"] + tags\n",
    "\n",
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}\n",
    "\n",
    "# Let's split the data into train and test (or eval)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(tagged_sents, test_size=.1)\n",
    "len(train_data), len(test_data)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class MaskLayer(nn.Module):\n",
    "    def __init__(self, lower_bound, upper_bound, replacement_values):\n",
    "        super(MaskLayer, self).__init__()\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.replacement_values = replacement_values\n",
    "\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        lower_bound = self.lower_bound.to(dtype=x.dtype, device=x.device).view(1, 1, -1)\n",
    "        upper_bound = self.upper_bound.to(dtype=x.dtype, device=x.device).view(1, 1, -1)\n",
    "        replacement_values = self.replacement_values.to(dtype=x.dtype, device=x.device).view(1, 1, -1)\n",
    "\n",
    " \n",
    "\n",
    "        mask = (x >= lower_bound) & (x <= upper_bound)\n",
    "        x = torch.where(mask, replacement_values, x)\n",
    "        return x\n",
    "    \n",
    "    def set_perms(self,lower_bound, upper_bound, replacement_values):\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.replacement_values = replacement_values\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        self.bert = self.model.bert\n",
    "        self.masking_layer = torch.ones(768).to(\"cuda\")\n",
    "        self.mask_layer = MaskLayer(torch.tensor(float('inf')), torch.tensor(float('-inf')), torch.tensor(0.0))\n",
    "\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "        '''\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            self.bert.train()\n",
    "            encoded_layers = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded_layers = self.bert(x)\n",
    "                enc = encoded_layers[-1]\n",
    "        # enc = nn.ReLU(enc)\n",
    "        # enc = enc * self.masking_layer\n",
    "        enc = self.mask_layer(enc)\n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        confidence = logits.softmax(-1).max(-1).values\n",
    "        return enc, logits, y, y_hat, confidence\n",
    "    \n",
    "    \n",
    "model = Net(vocab_size=len(tag2idx))\n",
    "model.to(device)\n",
    "\n",
    "train_dataset = PosDataset(train_data, tokenizer, tag2idx)\n",
    "eval_dataset = PosDataset(test_data, tokenizer, tag2idx)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=True,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "test_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "activation_iter = data.DataLoader(dataset=train_dataset+eval_dataset,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "for i in range(10):\n",
    "    train(model, train_iter, optimizer, criterion)\n",
    "\n",
    "from utilities import  eval\n",
    "\n",
    "model.masking_layer = torch.ones(768).to(\"cuda\")\n",
    "activation_iter = data.DataLoader(dataset=train_dataset+eval_dataset,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "enc_dict = eval(model, activation_iter, idx2tag, tag2idx,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_range_bert(model, mask, fc_vals):\n",
    "    mean = torch.tensor(np.mean(fc_vals, axis=0))\n",
    "    std = torch.tensor(np.std(fc_vals, axis=0))\n",
    "    mask = mask.to(torch.bool)\n",
    "    a = 2.5\n",
    "    lower_bound = torch.full_like(mean, torch.inf)\n",
    "    lower_bound[~mask] = mean[~mask] - a*std[~mask]\n",
    "    upper_bound = torch.full_like(mean, -torch.inf)\n",
    "    upper_bound[~mask] = mean[~mask] + a*std[~mask]\n",
    "    \n",
    "    model.mask_layer.lower_bound = lower_bound.to(device)\n",
    "    model.mask_layer.upper_bound = upper_bound.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = PrettyTable()\n",
    "\n",
    "results_table.field_names = results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"Base Complement Acc\", \"Base Compliment Conf\", \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"]\n",
    "\n",
    "class_labels = []\n",
    "base_accuracies = []\n",
    "base_confidences = []\n",
    "base_comp_acc = []\n",
    "base_comp_conf = []\n",
    "max_accuracies = []\n",
    "max_confidences = []\n",
    "max_comp_acc = []\n",
    "max_comp_conf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0, 0)\n",
      "Compliment: (0, 0)\n",
      "Compliment: (0, 0)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0, 0)\n",
      "Compliment: (0, 0)\n",
      "-----------------------------\n",
      "WRB ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9994)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.2079, 0.0354)\n",
      "Compliment: (0.9935, 0.9908)\n",
      "-----------------------------\n",
      "VBN ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9859, 0.9937)\n",
      "Compliment: (0.9936, 0.9942)\n",
      "Compliment: (0.9936, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1012, 0.0393)\n",
      "Compliment: (0.9937, 0.9777)\n",
      "-----------------------------\n",
      "LS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9912)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0769, 0.0231)\n",
      "Compliment: (0.9934, 0.9899)\n",
      "-----------------------------\n",
      "NNS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9929, 0.9941)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0771, 0.0334)\n",
      "Compliment: (0.9934, 0.9852)\n",
      "-----------------------------\n",
      "JJ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9866, 0.9891)\n",
      "Compliment: (0.9939, 0.9945)\n",
      "Compliment: (0.9939, 0.9945)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1047, 0.039)\n",
      "Compliment: (0.9946, 0.9611)\n",
      "-----------------------------\n",
      "VBZ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9944, 0.9971)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0936, 0.0339)\n",
      "Compliment: (0.9933, 0.9887)\n",
      "-----------------------------\n",
      "NNPS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9098, 0.8977)\n",
      "Compliment: (0.9936, 0.9945)\n",
      "Compliment: (0.9936, 0.9945)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0328, 0.0455)\n",
      "Compliment: (0.9934, 0.9818)\n",
      "-----------------------------\n",
      "CC ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9956, 0.9966)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0525, 0.0303)\n",
      "Compliment: (0.9935, 0.9837)\n",
      "-----------------------------\n",
      "VBD ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9918, 0.9955)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1268, 0.0367)\n",
      "Compliment: (0.9932, 0.9846)\n",
      "-----------------------------\n",
      "WDT ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9955, 0.9958)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0697, 0.0365)\n",
      "Compliment: (0.993, 0.9882)\n",
      "-----------------------------\n",
      "SYM ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0, 0.8156)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0, 0.022)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "-----------------------------\n",
      "CD ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9986, 0.9992)\n",
      "Compliment: (0.9932, 0.994)\n",
      "Compliment: (0.9932, 0.994)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1641, 0.0348)\n",
      "Compliment: (0.9932, 0.9878)\n",
      "-----------------------------\n",
      "EX ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9659, 0.9843)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0227, 0.0545)\n",
      "Compliment: (0.994, 0.9847)\n",
      "-----------------------------\n",
      "DT ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9973, 0.9978)\n",
      "Compliment: (0.9931, 0.9939)\n",
      "Compliment: (0.9931, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1143, 0.032)\n",
      "Compliment: (0.9933, 0.985)\n",
      "-----------------------------\n",
      "RB ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9869, 0.9915)\n",
      "Compliment: (0.9936, 0.9943)\n",
      "Compliment: (0.9936, 0.9943)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1371, 0.0442)\n",
      "Compliment: (0.9945, 0.9858)\n",
      "-----------------------------\n",
      "NN ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9861, 0.9848)\n",
      "Compliment: (0.9945, 0.9956)\n",
      "Compliment: (0.9945, 0.9956)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0853, 0.0378)\n",
      "Compliment: (0.9947, 0.9569)\n",
      "-----------------------------\n",
      "VBG ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.989, 0.9939)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0452, 0.0371)\n",
      "Compliment: (0.9936, 0.9852)\n",
      "-----------------------------\n",
      "PRP$ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9994)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1749, 0.0382)\n",
      "Compliment: (0.993, 0.9914)\n",
      "-----------------------------\n",
      "-NONE- ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9988)\n",
      "Compliment: (0.993, 0.9939)\n",
      "Compliment: (0.993, 0.9939)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.2495, 0.0351)\n",
      "Compliment: (0.993, 0.9914)\n",
      "-----------------------------\n",
      "VB ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9914, 0.9936)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1014, 0.038)\n",
      "Compliment: (0.9934, 0.9853)\n",
      "-----------------------------\n",
      "RP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9722, 0.9864)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0185, 0.0444)\n",
      "Compliment: (0.9936, 0.982)\n",
      "-----------------------------\n",
      "WP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9997)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.2199, 0.035)\n",
      "Compliment: (0.9935, 0.9912)\n",
      "-----------------------------\n",
      "JJR ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8215, 0.9187)\n",
      "Compliment: (0.9941, 0.9945)\n",
      "Compliment: (0.9941, 0.9945)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0341, 0.0299)\n",
      "Compliment: (0.9938, 0.9724)\n",
      "-----------------------------\n",
      ": ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9982, 0.9965)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1297, 0.0322)\n",
      "Compliment: (0.9936, 0.9883)\n",
      "-----------------------------\n",
      "WP$ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9993)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.2143, 0.0239)\n",
      "Compliment: (0.9934, 0.9922)\n",
      "-----------------------------\n",
      "RBR ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9706, 0.9784)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0368, 0.0422)\n",
      "Compliment: (0.9941, 0.9835)\n",
      "-----------------------------\n",
      "RBS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9654)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1429, 0.033)\n",
      "Compliment: (0.9937, 0.9909)\n",
      "-----------------------------\n",
      "TO ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9998)\n",
      "Compliment: (0.9933, 0.9941)\n",
      "Compliment: (0.9933, 0.9941)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.279, 0.0332)\n",
      "Compliment: (0.9933, 0.9926)\n",
      "-----------------------------\n",
      "$ ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9999)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.4254, 0.0324)\n",
      "Compliment: (0.9934, 0.9918)\n",
      "-----------------------------\n",
      "VBP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9932, 0.9885)\n",
      "Compliment: (0.9934, 0.9943)\n",
      "Compliment: (0.9934, 0.9943)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.056, 0.0351)\n",
      "Compliment: (0.993, 0.987)\n",
      "-----------------------------\n",
      "`` ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9999)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.3624, 0.0416)\n",
      "Compliment: (0.9934, 0.9914)\n",
      "-----------------------------\n",
      "PRP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9994, 0.9993)\n",
      "Compliment: (0.9933, 0.9941)\n",
      "Compliment: (0.9933, 0.9941)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1678, 0.0349)\n",
      "Compliment: (0.9934, 0.9913)\n",
      "-----------------------------\n",
      "-RRB- ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9993)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.2937, 0.0274)\n",
      "Compliment: (0.9933, 0.9898)\n",
      "-----------------------------\n",
      ". ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9999)\n",
      "Compliment: (0.9932, 0.994)\n",
      "Compliment: (0.9932, 0.994)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.2142, 0.0483)\n",
      "Compliment: (0.993, 0.9911)\n",
      "-----------------------------\n",
      "IN ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9967, 0.9941)\n",
      "Compliment: (0.9931, 0.9942)\n",
      "Compliment: (0.9931, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1374, 0.0383)\n",
      "Compliment: (0.9934, 0.9845)\n",
      "-----------------------------\n",
      "MD ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9999)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.233, 0.039)\n",
      "Compliment: (0.993, 0.9924)\n",
      "-----------------------------\n",
      "JJS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.989, 0.9887)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.8462, 0.0424)\n",
      "Compliment: (0.9935, 0.9902)\n",
      "-----------------------------\n",
      "PDT ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9926)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1852, 0.0247)\n",
      "Compliment: (0.9934, 0.9887)\n",
      "-----------------------------\n",
      "'' ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9986, 0.9992)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.3055, 0.0325)\n",
      "Compliment: (0.9934, 0.9905)\n",
      "-----------------------------\n",
      "UH ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.6667, 0.9837)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0, 0.022)\n",
      "Compliment: (0.9935, 0.9636)\n",
      "-----------------------------\n",
      "# ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (1.0, 0.9934)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.3125, 0.0241)\n",
      "Compliment: (0.9931, 0.9897)\n",
      "-----------------------------\n",
      "FW ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.5, 0.9084)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Compliment: (0.9935, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.0, 0.022)\n",
      "Compliment: (0.9812, 0.4273)\n",
      "-----------------------------\n",
      "NNP ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9979, 0.9984)\n",
      "Compliment: (0.993, 0.9938)\n",
      "Compliment: (0.993, 0.9938)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.119, 0.0386)\n",
      "Compliment: (0.9933, 0.9895)\n",
      "-----------------------------\n",
      "POS ----------------\n",
      "Original:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.9988, 0.9995)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Compliment: (0.9934, 0.9942)\n",
      "Max:\n",
      "<pad>: N/A (0 occurrences)\n",
      "Tok: (0.1978, 0.039)\n",
      "Compliment: (0.9931, 0.9911)\n",
      "-----------------------------\n",
      "+--------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+\n",
      "| Class  | Base Accuracy | Base Confidence | Base Complement Acc | Base Compliment Conf | MAX Accuracy | MAX Confidence | Max compliment acc | Max compliment conf |\n",
      "+--------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+\n",
      "| <pad>  |       0       |        0        |          0          |          0           |      0       |       0        |         0          |          0          |\n",
      "|  WRB   |      1.0      |      0.9994     |        0.9934       |        0.9942        |     1.0      |     0.9994     |       0.9934       |        0.9942       |\n",
      "|  VBN   |     0.9859    |      0.9937     |        0.9936       |        0.9942        |    0.9859    |     0.9937     |       0.9936       |        0.9942       |\n",
      "|   LS   |      1.0      |      0.9912     |        0.9934       |        0.9942        |     1.0      |     0.9912     |       0.9934       |        0.9942       |\n",
      "|  NNS   |     0.9929    |      0.9941     |        0.9935       |        0.9942        |    0.9929    |     0.9941     |       0.9935       |        0.9942       |\n",
      "|   JJ   |     0.9866    |      0.9891     |        0.9939       |        0.9945        |    0.9866    |     0.9891     |       0.9939       |        0.9945       |\n",
      "|  VBZ   |     0.9944    |      0.9971     |        0.9934       |        0.9942        |    0.9944    |     0.9971     |       0.9934       |        0.9942       |\n",
      "| <pad>  |       0       |        0        |          0          |          0           |      0       |       0        |         0          |          0          |\n",
      "|  WRB   |      1.0      |      0.9994     |        0.9934       |        0.9942        |     1.0      |     0.9994     |       0.9934       |        0.9942       |\n",
      "|  VBN   |     0.9859    |      0.9937     |        0.9936       |        0.9942        |    0.9859    |     0.9937     |       0.9936       |        0.9942       |\n",
      "|   LS   |      1.0      |      0.9912     |        0.9934       |        0.9942        |     1.0      |     0.9912     |       0.9934       |        0.9942       |\n",
      "|  NNS   |     0.9929    |      0.9941     |        0.9935       |        0.9942        |    0.9929    |     0.9941     |       0.9935       |        0.9942       |\n",
      "|   JJ   |     0.9866    |      0.9891     |        0.9939       |        0.9945        |    0.9866    |     0.9891     |       0.9939       |        0.9945       |\n",
      "|  VBZ   |     0.9944    |      0.9971     |        0.9934       |        0.9942        |    0.9944    |     0.9971     |       0.9934       |        0.9942       |\n",
      "| <pad>  |       0       |        0        |          0          |          0           |      0       |       0        |         0          |          0          |\n",
      "|  WRB   |      1.0      |      0.9994     |        0.9934       |        0.9942        |     1.0      |     0.9994     |       0.9934       |        0.9942       |\n",
      "|  VBN   |     0.9859    |      0.9937     |        0.9936       |        0.9942        |    0.9859    |     0.9937     |       0.9936       |        0.9942       |\n",
      "|   LS   |      1.0      |      0.9912     |        0.9934       |        0.9942        |     1.0      |     0.9912     |       0.9934       |        0.9942       |\n",
      "|  NNS   |     0.9929    |      0.9941     |        0.9935       |        0.9942        |    0.9929    |     0.9941     |       0.9935       |        0.9942       |\n",
      "|   JJ   |     0.9866    |      0.9891     |        0.9939       |        0.9945        |    0.9866    |     0.9891     |       0.9939       |        0.9945       |\n",
      "|  VBZ   |     0.9944    |      0.9971     |        0.9934       |        0.9942        |    0.9944    |     0.9971     |       0.9934       |        0.9942       |\n",
      "| <pad>  |       0       |        0        |          0          |          0           |      0       |       0        |         0          |          0          |\n",
      "|  WRB   |      1.0      |      0.9993     |        0.9932       |        0.9932        |    0.3202    |     0.0344     |       0.9935       |        0.9902       |\n",
      "|  VBN   |     0.9855    |      0.9891     |        0.9924       |        0.9885        |    0.1635    |     0.0647     |       0.9938       |        0.9795       |\n",
      "|   LS   |      1.0      |      0.9588     |        0.9762       |        0.9601        |     0.0      |     0.102      |       0.9933       |        0.9906       |\n",
      "|  NNS   |     0.9929    |      0.9891     |        0.9932       |        0.9905        |    0.9534    |     0.0565     |       0.9932       |        0.9848       |\n",
      "|   JJ   |     0.9868    |      0.9734     |        0.991        |        0.9263        |    0.7506    |     0.1073     |       0.9947       |        0.9737       |\n",
      "|  VBZ   |     0.9944    |      0.9874     |        0.9802       |        0.9221        |    0.6014    |     0.1561     |       0.9928       |        0.9894       |\n",
      "|  NNPS  |     0.9057    |      0.8518     |        0.9847       |        0.9721        |      0       |       0        |         0          |          0          |\n",
      "| <pad>  |       0       |        0        |          0          |          0           |    0.2079    |     0.0354     |       0.9935       |        0.9908       |\n",
      "|  WRB   |      1.0      |      0.9994     |        0.9934       |        0.9942        |    0.1012    |     0.0393     |       0.9937       |        0.9777       |\n",
      "|  VBN   |     0.9859    |      0.9937     |        0.9936       |        0.9942        |    0.0769    |     0.0231     |       0.9934       |        0.9899       |\n",
      "|   LS   |      1.0      |      0.9912     |        0.9934       |        0.9942        |    0.0771    |     0.0334     |       0.9934       |        0.9852       |\n",
      "|  NNS   |     0.9929    |      0.9941     |        0.9935       |        0.9942        |    0.1047    |     0.039      |       0.9946       |        0.9611       |\n",
      "|   JJ   |     0.9866    |      0.9891     |        0.9939       |        0.9945        |    0.0936    |     0.0339     |       0.9933       |        0.9887       |\n",
      "|  VBZ   |     0.9944    |      0.9971     |        0.9934       |        0.9942        |    0.0328    |     0.0455     |       0.9934       |        0.9818       |\n",
      "|  NNPS  |     0.9098    |      0.8977     |        0.9936       |        0.9945        |    0.0525    |     0.0303     |       0.9935       |        0.9837       |\n",
      "|   CC   |     0.9956    |      0.9966     |        0.9934       |        0.9942        |    0.1268    |     0.0367     |       0.9932       |        0.9846       |\n",
      "|  VBD   |     0.9918    |      0.9955     |        0.9935       |        0.9942        |    0.0697    |     0.0365     |       0.993        |        0.9882       |\n",
      "|  WDT   |     0.9955    |      0.9958     |        0.9934       |        0.9942        |     0.0      |     0.022      |       0.9934       |        0.9942       |\n",
      "|  SYM   |      0.0      |      0.8156     |        0.9934       |        0.9942        |    0.1641    |     0.0348     |       0.9932       |        0.9878       |\n",
      "|   CD   |     0.9986    |      0.9992     |        0.9932       |        0.994         |    0.0227    |     0.0545     |       0.994        |        0.9847       |\n",
      "|   EX   |     0.9659    |      0.9843     |        0.9935       |        0.9942        |    0.1143    |     0.032      |       0.9933       |        0.985        |\n",
      "|   DT   |     0.9973    |      0.9978     |        0.9931       |        0.9939        |    0.1371    |     0.0442     |       0.9945       |        0.9858       |\n",
      "|   RB   |     0.9869    |      0.9915     |        0.9936       |        0.9943        |    0.0853    |     0.0378     |       0.9947       |        0.9569       |\n",
      "|   NN   |     0.9861    |      0.9848     |        0.9945       |        0.9956        |    0.0452    |     0.0371     |       0.9936       |        0.9852       |\n",
      "|  VBG   |     0.989     |      0.9939     |        0.9935       |        0.9942        |    0.1749    |     0.0382     |       0.993        |        0.9914       |\n",
      "|  PRP$  |      1.0      |      0.9994     |        0.9934       |        0.9942        |    0.2495    |     0.0351     |       0.993        |        0.9914       |\n",
      "| -NONE- |      1.0      |      0.9988     |        0.993        |        0.9939        |    0.1014    |     0.038      |       0.9934       |        0.9853       |\n",
      "|   VB   |     0.9914    |      0.9936     |        0.9935       |        0.9942        |    0.0185    |     0.0444     |       0.9936       |        0.982        |\n",
      "|   RP   |     0.9722    |      0.9864     |        0.9935       |        0.9942        |    0.2199    |     0.035      |       0.9935       |        0.9912       |\n",
      "|   WP   |      1.0      |      0.9997     |        0.9934       |        0.9942        |    0.0341    |     0.0299     |       0.9938       |        0.9724       |\n",
      "|  JJR   |     0.8215    |      0.9187     |        0.9941       |        0.9945        |    0.1297    |     0.0322     |       0.9936       |        0.9883       |\n",
      "|   :    |     0.9982    |      0.9965     |        0.9934       |        0.9942        |    0.2143    |     0.0239     |       0.9934       |        0.9922       |\n",
      "|  WP$   |      1.0      |      0.9993     |        0.9934       |        0.9942        |    0.0368    |     0.0422     |       0.9941       |        0.9835       |\n",
      "|  RBR   |     0.9706    |      0.9784     |        0.9935       |        0.9942        |    0.1429    |     0.033      |       0.9937       |        0.9909       |\n",
      "|  RBS   |      1.0      |      0.9654     |        0.9934       |        0.9942        |    0.279     |     0.0332     |       0.9933       |        0.9926       |\n",
      "|   TO   |      1.0      |      0.9998     |        0.9933       |        0.9941        |    0.4254    |     0.0324     |       0.9934       |        0.9918       |\n",
      "|   $    |      1.0      |      0.9999     |        0.9934       |        0.9942        |    0.056     |     0.0351     |       0.993        |        0.987        |\n",
      "| <pad>  |       0       |        0        |          0          |          0           |      0       |       0        |         0          |          0          |\n",
      "|  WRB   |      1.0      |      0.9994     |        0.9934       |        0.9942        |     1.0      |     0.9994     |       0.9934       |        0.9942       |\n",
      "|  VBN   |     0.9859    |      0.9937     |        0.9936       |        0.9942        |    0.9859    |     0.9937     |       0.9936       |        0.9942       |\n",
      "|   LS   |      1.0      |      0.9912     |        0.9934       |        0.9942        |     1.0      |     0.9912     |       0.9934       |        0.9942       |\n",
      "|  NNS   |     0.9929    |      0.9941     |        0.9935       |        0.9942        |    0.9929    |     0.9941     |       0.9935       |        0.9942       |\n",
      "|   JJ   |     0.9866    |      0.9891     |        0.9939       |        0.9945        |    0.9866    |     0.9891     |       0.9939       |        0.9945       |\n",
      "|  VBZ   |     0.9944    |      0.9971     |        0.9934       |        0.9942        |    0.9944    |     0.9971     |       0.9934       |        0.9942       |\n",
      "| <pad>  |       0       |        0        |          0          |          0           |      0       |       0        |         0          |          0          |\n",
      "|  WRB   |      1.0      |      0.9993     |        0.9932       |        0.9932        |    0.3202    |     0.0344     |       0.9935       |        0.9902       |\n",
      "|  VBN   |     0.9855    |      0.9891     |        0.9924       |        0.9885        |    0.1635    |     0.0647     |       0.9938       |        0.9795       |\n",
      "|   LS   |      1.0      |      0.9588     |        0.9762       |        0.9601        |     0.0      |     0.102      |       0.9933       |        0.9906       |\n",
      "|  NNS   |     0.9929    |      0.9891     |        0.9932       |        0.9905        |    0.9534    |     0.0565     |       0.9932       |        0.9848       |\n",
      "|   JJ   |     0.9868    |      0.9734     |        0.991        |        0.9263        |    0.7506    |     0.1073     |       0.9947       |        0.9737       |\n",
      "|  VBZ   |     0.9944    |      0.9874     |        0.9802       |        0.9221        |    0.6014    |     0.1561     |       0.9928       |        0.9894       |\n",
      "|  NNPS  |     0.9057    |      0.8518     |        0.9847       |        0.9721        |      0       |       0        |         0          |          0          |\n",
      "| <pad>  |       0       |        0        |          0          |          0           |    0.2079    |     0.0354     |       0.9935       |        0.9908       |\n",
      "|  WRB   |      1.0      |      0.9994     |        0.9934       |        0.9942        |    0.1012    |     0.0393     |       0.9937       |        0.9777       |\n",
      "|  VBN   |     0.9859    |      0.9937     |        0.9936       |        0.9942        |    0.0769    |     0.0231     |       0.9934       |        0.9899       |\n",
      "|   LS   |      1.0      |      0.9912     |        0.9934       |        0.9942        |    0.0771    |     0.0334     |       0.9934       |        0.9852       |\n",
      "|  NNS   |     0.9929    |      0.9941     |        0.9935       |        0.9942        |    0.1047    |     0.039      |       0.9946       |        0.9611       |\n",
      "|   JJ   |     0.9866    |      0.9891     |        0.9939       |        0.9945        |    0.0936    |     0.0339     |       0.9933       |        0.9887       |\n",
      "|  VBZ   |     0.9944    |      0.9971     |        0.9934       |        0.9942        |    0.0328    |     0.0455     |       0.9934       |        0.9818       |\n",
      "|  NNPS  |     0.9098    |      0.8977     |        0.9936       |        0.9945        |    0.0525    |     0.0303     |       0.9935       |        0.9837       |\n",
      "|   CC   |     0.9956    |      0.9966     |        0.9934       |        0.9942        |    0.1268    |     0.0367     |       0.9932       |        0.9846       |\n",
      "|  VBD   |     0.9918    |      0.9955     |        0.9935       |        0.9942        |    0.0697    |     0.0365     |       0.993        |        0.9882       |\n",
      "|  WDT   |     0.9955    |      0.9958     |        0.9934       |        0.9942        |     0.0      |     0.022      |       0.9934       |        0.9942       |\n",
      "|  SYM   |      0.0      |      0.8156     |        0.9934       |        0.9942        |    0.1641    |     0.0348     |       0.9932       |        0.9878       |\n",
      "|   CD   |     0.9986    |      0.9992     |        0.9932       |        0.994         |    0.0227    |     0.0545     |       0.994        |        0.9847       |\n",
      "|   EX   |     0.9659    |      0.9843     |        0.9935       |        0.9942        |    0.1143    |     0.032      |       0.9933       |        0.985        |\n",
      "|   DT   |     0.9973    |      0.9978     |        0.9931       |        0.9939        |    0.1371    |     0.0442     |       0.9945       |        0.9858       |\n",
      "|   RB   |     0.9869    |      0.9915     |        0.9936       |        0.9943        |    0.0853    |     0.0378     |       0.9947       |        0.9569       |\n",
      "|   NN   |     0.9861    |      0.9848     |        0.9945       |        0.9956        |    0.0452    |     0.0371     |       0.9936       |        0.9852       |\n",
      "|  VBG   |     0.989     |      0.9939     |        0.9935       |        0.9942        |    0.1749    |     0.0382     |       0.993        |        0.9914       |\n",
      "|  PRP$  |      1.0      |      0.9994     |        0.9934       |        0.9942        |    0.2495    |     0.0351     |       0.993        |        0.9914       |\n",
      "| -NONE- |      1.0      |      0.9988     |        0.993        |        0.9939        |    0.1014    |     0.038      |       0.9934       |        0.9853       |\n",
      "|   VB   |     0.9914    |      0.9936     |        0.9935       |        0.9942        |    0.0185    |     0.0444     |       0.9936       |        0.982        |\n",
      "|   RP   |     0.9722    |      0.9864     |        0.9935       |        0.9942        |    0.2199    |     0.035      |       0.9935       |        0.9912       |\n",
      "|   WP   |      1.0      |      0.9997     |        0.9934       |        0.9942        |    0.0341    |     0.0299     |       0.9938       |        0.9724       |\n",
      "|  JJR   |     0.8215    |      0.9187     |        0.9941       |        0.9945        |    0.1297    |     0.0322     |       0.9936       |        0.9883       |\n",
      "|   :    |     0.9982    |      0.9965     |        0.9934       |        0.9942        |    0.2143    |     0.0239     |       0.9934       |        0.9922       |\n",
      "|  WP$   |      1.0      |      0.9993     |        0.9934       |        0.9942        |    0.0368    |     0.0422     |       0.9941       |        0.9835       |\n",
      "|  RBR   |     0.9706    |      0.9784     |        0.9935       |        0.9942        |    0.1429    |     0.033      |       0.9937       |        0.9909       |\n",
      "|  RBS   |      1.0      |      0.9654     |        0.9934       |        0.9942        |    0.279     |     0.0332     |       0.9933       |        0.9926       |\n",
      "|   TO   |      1.0      |      0.9998     |        0.9933       |        0.9941        |    0.4254    |     0.0324     |       0.9934       |        0.9918       |\n",
      "|   $    |      1.0      |      0.9999     |        0.9934       |        0.9942        |    0.056     |     0.0351     |       0.993        |        0.987        |\n",
      "+--------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from utilities import compute_masks, eval\n",
    "for tok in range(45):\n",
    "    print(idx2tag[tok],\"----------------\")\n",
    "    # model.masking_layer = torch.ones(768).to(\"cuda\")\n",
    "    model.mask_layer = MaskLayer(torch.tensor(float('inf')), torch.tensor(float('-inf')), torch.tensor(0.0)).to(\"cuda\")\n",
    "    activation_iter = data.DataLoader(dataset=train_dataset+eval_dataset,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=pad)\n",
    "    print(\"Original:\")\n",
    "    enc_dict = eval(model, activation_iter, idx2tag, tag2idx, tok)\n",
    "    class_labels.append(idx2tag[tok])\n",
    "    base_accuracies.append(enc_dict[1][0])\n",
    "    base_confidences.append(enc_dict[1][1])\n",
    "    base_comp_acc.append(enc_dict[2][0])\n",
    "    base_comp_conf.append(enc_dict[2][1])\n",
    "    print(\"Tok:\", enc_dict[1])\n",
    "    print('Compliment:', enc_dict[2])\n",
    "\n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max = compute_masks(enc_dict[0][tok],1)\n",
    "    # print(\"STD:\")print(\"Tok:\", enc_dict[1])\n",
    "    print('Compliment:', enc_dict[2])\n",
    "    # model.masking_layer = mask_std.to(\"cuda\")\n",
    "\n",
    "    # enc_dict = eval(model, activation_iter, idx2tag, tag2idx, tok)\n",
    "    print(\"Max:\")\n",
    "    # model.masking_layer = mask_max.to(\"cuda\")\n",
    "    model = mask_range_bert(model, mask_max, enc_dict[0][tok])\n",
    "\n",
    "    enc_dict = eval(model, activation_iter, idx2tag, tag2idx, tok)\n",
    "    max_accuracies.append(enc_dict[1][0])\n",
    "    max_confidences.append(enc_dict[1][1])\n",
    "    max_comp_acc.append(enc_dict[2][0])\n",
    "    max_comp_conf.append(enc_dict[2][1])\n",
    "    print(\"Tok:\", enc_dict[1])\n",
    "    print('Compliment:', enc_dict[2])\n",
    "    print(\"-----------------------------\")\n",
    "    \n",
    "    results_table.add_row([\n",
    "                class_labels[tok],\n",
    "                base_accuracies[tok],\n",
    "                base_confidences[tok],\n",
    "                base_comp_acc[tok],\n",
    "                base_comp_conf[tok],\n",
    "                max_accuracies[tok],\n",
    "                max_confidences[tok],\n",
    "                max_comp_acc[tok],\n",
    "                max_comp_conf[tok],\n",
    "            ])\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_masks(fc_vals, percent):\n",
    "    # Convert input to numpy array\n",
    "    fc_vals_array = np.array(fc_vals)\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_vals = np.mean(np.abs(fc_vals_array), axis=0)\n",
    "    std_vals = np.std(fc_vals_array, axis=0)\n",
    "    min_vals = np.min(fc_vals_array, axis=0)\n",
    "    max_vals = np.max(fc_vals_array, axis=0)\n",
    "    \n",
    "    # Normalize standard deviation\n",
    "    std_vals_normalized = (std_vals - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    mean_vals_tensor = torch.from_numpy(mean_vals)\n",
    "    std_vals_tensor = torch.from_numpy(std_vals_normalized)\n",
    "    \n",
    "    # Compute masks\n",
    "    mask_max = compute_max_mask(mean_vals_tensor, percent)\n",
    "    mask_std = compute_std_mask(std_vals_tensor, percent)\n",
    "    mask_max_low_std = compute_max_low_std_mask(mean_vals_tensor, std_vals_tensor, percent)\n",
    "    mask_intersection = torch.logical_or(mask_std, mask_max).float()\n",
    "    \n",
    "    return mask_max, mask_std, mask_intersection, mask_max_low_std\n",
    "\n",
    "def compute_max_mask(values, percent):\n",
    "    sorted_indices = torch.argsort(values, descending=True)\n",
    "    mask_count = int(percent * len(values))\n",
    "    mask = torch.ones_like(values)\n",
    "    mask[sorted_indices[:mask_count]] = 0.0\n",
    "    return mask\n",
    "\n",
    "def compute_std_mask(values, percent):\n",
    "    sorted_indices = torch.argsort(values, descending=False)\n",
    "    mask_count = int(percent * len(values))\n",
    "    mask = torch.ones_like(values)\n",
    "    mask[sorted_indices[:mask_count]] = 0.0\n",
    "    return mask\n",
    "\n",
    "def compute_max_low_std_mask(mean_vals, std_vals, percent):\n",
    "    # Get indices of bottom 50% std values\n",
    "    bottom_50_percent_std_count = int(0.99 * len(std_vals))\n",
    "    bottom_50_percent_std_indices = torch.argsort(std_vals)[:bottom_50_percent_std_count]\n",
    "    \n",
    "    # Create a mask for bottom 50% std values\n",
    "    bottom_50_percent_std_mask = torch.zeros_like(std_vals, dtype=torch.bool)\n",
    "    bottom_50_percent_std_mask[bottom_50_percent_std_indices] = True\n",
    "    \n",
    "    # Filter mean values\n",
    "    mean_vals_filtered = mean_vals.clone()\n",
    "    mean_vals_filtered[~bottom_50_percent_std_mask] = float('-inf')\n",
    "    \n",
    "    # Compute mask\n",
    "    return compute_max_mask(mean_vals_filtered, percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 0\n",
      "Layer 38 1\n",
      "Layer 44 2\n",
      "Layer 20 3\n",
      "Layer 10 4\n",
      "Layer 17 5\n",
      "Layer 24 6\n",
      "Layer 15 7\n",
      "Layer 32 8\n",
      "Layer 8 9\n",
      "Layer 25 10\n",
      "Layer 9 11\n",
      "Layer 41 12\n",
      "Layer 30 13\n",
      "Layer 39 14\n",
      "Layer 7 15\n",
      "Layer 2 16\n",
      "Layer 22 17\n",
      "Layer 31 18\n",
      "Layer 12 19\n",
      "Layer 26 20\n",
      "Layer 27 21\n",
      "Layer 11 22\n",
      "Layer 46 23\n",
      "Layer 23 24\n",
      "Layer 40 25\n",
      "Layer 4 26\n",
      "Layer 19 27\n",
      "Layer 45 28\n",
      "Layer 43 29\n",
      "Layer 42 30\n",
      "Layer 14 31\n",
      "Layer 33 32\n",
      "Layer 37 33\n",
      "Layer 34 34\n",
      "Layer 36 35\n",
      "Layer 21 36\n",
      "Layer 16 37\n",
      "Layer 29 38\n",
      "Layer 5 39\n",
      "Layer 28 40\n",
      "Layer 35 41\n",
      "Layer 13 42\n",
      "Layer 1 43\n",
      "Layer 6 44\n",
      "Layer 3 45\n",
      "Layer 18 46\n"
     ]
    }
   ],
   "source": [
    "for i, fc in enumerate(enc_dict):\n",
    "    print(f\"Layer {fc}\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, fc1 in enumerate(enc_dict):\n",
    "    tag = idx2tag[fc1]\n",
    "    fc1 = enc_dict[fc1]\n",
    "    \n",
    "    fc1 = np.array(fc1)\n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std = compute_masks(fc1, 0.15)\n",
    "    \n",
    "    m = np.mean(np.abs(fc1), axis=0)\n",
    "    s = np.std(fc1, axis=0)\n",
    "    min_val = np.min(fc1, axis=0)\n",
    "    max_val = np.max(fc1, axis=0)\n",
    "    \n",
    "    # Normalize std and mean\n",
    "    s_norm = (s - min_val) / (max_val - min_val)\n",
    "    m_norm = m#(m - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create indices for different masks\n",
    "    indices_max = np.where(mask_max == 0)[0]\n",
    "    indices_std = np.where(mask_std == 0)[0]\n",
    "    indices_intersection = np.intersect1d(indices_max, indices_std)\n",
    "    indices_max_minus_std = np.setdiff1d(indices_max, indices_std)\n",
    "    indices_std_minus_max = np.setdiff1d(indices_std, indices_max)\n",
    "    \n",
    "    # Count the indices in each set\n",
    "    count_all = len(m_norm)\n",
    "    count_max = len(indices_max)\n",
    "    count_std = len(indices_std)\n",
    "    count_intersection = len(indices_intersection)\n",
    "    count_max_minus_std = len(indices_max_minus_std)\n",
    "    count_std_minus_max = len(indices_std_minus_max)\n",
    "    \n",
    "    out = Output()\n",
    "    with out:\n",
    "        # Create subplots with counts in titles\n",
    "        fig = make_subplots(rows=2, cols=3, \n",
    "                            subplot_titles=(f\"All Activations (Count: {count_all})\",\n",
    "                                            f\"Max Mask (Count: {count_max})\", \n",
    "                                            f\"Std Mask (Count: {count_std})\", \n",
    "                                            f\"Intersection (Count: {count_intersection})\",\n",
    "                                            f\"Max - Std (Count: {count_max_minus_std})\", \n",
    "                                            f\"Std - Max (Count: {count_std_minus_max})\"))\n",
    "        \n",
    "        # Helper function to add traces\n",
    "        def add_traces(indices, row, col):\n",
    "            indices_list = list(indices)  # Convert range or numpy array to list\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices_list,\n",
    "                    y=m_norm[indices_list],\n",
    "                    mode='markers',\n",
    "                    name='Mean',\n",
    "                    marker=dict(size=3, color='blue'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices_list,\n",
    "                    y=s_norm[indices_list],\n",
    "                    mode='markers',\n",
    "                    name='Std Dev',\n",
    "                    marker=dict(size=3, color='red'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            for j in indices_list:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[j, j],\n",
    "                        y=[m_norm[j], s_norm[j]],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='gray', width=0.5),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        \n",
    "        # Add traces for all activations\n",
    "        add_traces(range(len(m_norm)), 1, 1)\n",
    "        \n",
    "        # Add traces for other plots\n",
    "        add_traces(indices_max, 1, 2)\n",
    "        add_traces(indices_std, 1, 3)\n",
    "        add_traces(indices_intersection, 2, 1)\n",
    "        add_traces(indices_max_minus_std, 2, 2)\n",
    "        add_traces(indices_std_minus_max, 2, 3)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Mean and Standard Deviation of Activations - Class {i+1}'+ tag,\n",
    "            height=1200,\n",
    "            width=1800,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update x and y axis labels for all subplots\n",
    "        for row in range(1, 3):\n",
    "            for col in range(1, 4):\n",
    "                fig.update_xaxes(title_text=\"Activation Index\", row=row, col=col)\n",
    "                fig.update_yaxes(title_text=\"Normalized Value\", row=row, col=col)\n",
    "        \n",
    "        display(fig)\n",
    "    \n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "from utilities import compute_masks\n",
    "from IPython.display import display\n",
    "\n",
    "def create_index_tracking_plot(indices_per_class, title):\n",
    "    num_classes = len(indices_per_class)\n",
    "    all_indices = sorted(set.union(*[set(indices) for indices in indices_per_class]))\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Create a color scale\n",
    "    color_scale = px.colors.diverging.RdYlGn_r  # Red to Yellow to Green color scale\n",
    "\n",
    "    # Add edges for indices present in multiple classes\n",
    "    for idx in all_indices:\n",
    "        classes_with_idx = [i for i, indices in enumerate(indices_per_class) if idx in indices]\n",
    "        if len(classes_with_idx) > 1:\n",
    "            x = [idx] * len(classes_with_idx)\n",
    "            y = classes_with_idx\n",
    "            color_index = (len(classes_with_idx) - 1) / (num_classes - 1)  # Normalize to [0, 1]\n",
    "            edge_color = px.colors.sample_colorscale(color_scale, [color_index])[0]\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode='lines',\n",
    "                line=dict(color=edge_color, width=2),\n",
    "                hoverinfo='text',\n",
    "                hovertext=f'Index: {idx}<br>Present in {len(classes_with_idx)} classes',\n",
    "                showlegend=False\n",
    "            ))\n",
    "    \n",
    "    # Add scatter plots for each class\n",
    "    for class_idx, indices in enumerate(indices_per_class):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=indices,\n",
    "            y=[class_idx] * len(indices),\n",
    "            mode='markers',\n",
    "            name=f'Class {class_idx + 1}',\n",
    "            marker=dict(size=4, symbol='circle', color='black'),\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f'Index: {idx}<br>Class: {class_idx + 1}' for idx in indices]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='Activation Index',\n",
    "        yaxis_title='Class',\n",
    "        yaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=list(range(num_classes)),\n",
    "            ticktext=[f'Class {i+1}' for i in range(num_classes)]\n",
    "        ),\n",
    "        hovermode='closest',\n",
    "        width=1500,\n",
    "        height=800,\n",
    "        plot_bgcolor='white',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    \n",
    "    # Add color bar\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            colorscale=color_scale,\n",
    "            showscale=True,\n",
    "            cmin=1,\n",
    "            cmax=num_classes,\n",
    "            colorbar=dict(\n",
    "                title='Number of Classes',\n",
    "                tickvals=list(range(1, num_classes+1)),\n",
    "                ticktext=list(range(1, num_classes+1))\n",
    "            )\n",
    "        ),\n",
    "        hoverinfo='none',\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Collect indices for each class\n",
    "max_indices_per_class = []\n",
    "std_indices_per_class = []\n",
    "\n",
    "for fc1 in all_fc_vals:\n",
    "    mask_max, mask_std = compute_masks(fc1, 0.15)\n",
    "    max_indices_per_class.append(np.where(mask_max == 0)[0])\n",
    "    std_indices_per_class.append(np.where(mask_std == 0)[0])\n",
    "\n",
    "# Create and display visualizations\n",
    "output_widgets = []\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "    fig_max = create_index_tracking_plot(max_indices_per_class, 'Max Mask Indices Across Classes')\n",
    "    display(fig_max)\n",
    "output_widgets.append(out)\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "    fig_std = create_index_tracking_plot(std_indices_per_class, 'Std Mask Indices Across Classes')\n",
    "    display(fig_std)\n",
    "output_widgets.append(out)\n",
    "\n",
    "# Display all visualizations\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
