{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from utilities import get_model_distilbert, record_activations\n",
    "\n",
    "mask_layer = 5\n",
    "text_tag = \"sentence\"\n",
    "compliment = True\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the dataset\n",
    "dataset_all = load_dataset(\"stanfordnlp/sst2\")\n",
    "# Select the train split\n",
    "dataset_all = dataset_all['train']\n",
    "model = get_model_distilbert(\"distilbert-base-uncased-finetuned-sst-2-english\", mask_layer)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "all_fc_vals = []\n",
    "batch_size = 32  # You can adjust this based on your GPU memory\n",
    "for j in range(0,2):\n",
    "    dataset = dataset_all.filter(lambda x: x['label'] in [j])\n",
    "    fc_vals = record_activations(dataset, model, tokenizer, text_tag='sentence', batch_size=256, mask_layer=mask_layer)\n",
    "    all_fc_vals.append(fc_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage:  0.2\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n",
      "'World' is already a single token (ID: 10603)\n",
      "'Sports' is already a single token (ID: 18153)\n",
      "'Business' is already a single token (ID: 24749)\n",
      "\n",
      "Added 1 new tokens to the tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d1/grad/mha361/anaconda3/envs/memit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1707745/1257197563.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16e97cedcd14edbaeda4ed2b0711d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a3a82909c24a8ebe3122f01fabb526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:591: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item['input_ids']).to(device)\n",
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:592: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item['attention_mask']).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9682 Confidence :  0.9625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db260de288bb4880b7d6b34f4966469f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m all_fc_vals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m--> 153\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     fc_vals \u001b[38;5;241m=\u001b[39m evaluate_gpt2_classification(lab, model, dataset, tokenizer)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy : \u001b[39m\u001b[38;5;124m'\u001b[39m, fc_vals[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence : \u001b[39m\u001b[38;5;124m'\u001b[39m, fc_vals[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:3771\u001b[0m, in \u001b[0;36mDataset.filter\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 3771\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_indices_from_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3779\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3793\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3798\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3799\u001b[0m new_dataset\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:3167\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3163\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3164\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3165\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3166\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3168\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3169\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:3558\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3554\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3555\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3556\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3558\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3566\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3567\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:3427\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3426\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3427\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3429\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3430\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3431\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:6538\u001b[0m, in \u001b[0;36mget_indices_from_mask_function\u001b[0;34m(function, batched, with_indices, with_rank, input_columns, indices_mapping, *args, **fn_kwargs)\u001b[0m\n\u001b[1;32m   6536\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys()))])\n\u001b[1;32m   6537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[0;32m-> 6538\u001b[0m     example \u001b[38;5;241m=\u001b[39m {key: batch[key][i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m   6539\u001b[0m     additional_args \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   6540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:6538\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   6536\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys()))])\n\u001b[1;32m   6537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[0;32m-> 6538\u001b[0m     example \u001b[38;5;241m=\u001b[39m {key: \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m   6539\u001b[0m     additional_args \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   6540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/formatting/formatting.py:279\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    277\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m--> 279\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format\u001b[38;5;241m.\u001b[39mremove(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/formatting/formatting.py:382\u001b[0m, in \u001b[0;36mLazyBatch.format\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/formatting/formatting.py:448\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 448\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/formatting/formatting.py:148\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "import random\n",
    "import numpy as np\n",
    "from utilities import evaluate_gpt2_classification as evaluate_gpt2_classification, mask_range_gpt,compute_masks, reset_gpt\n",
    "import torch  \n",
    "\n",
    "dataset_name = \"fancyzhx/dbpedia_14\"\n",
    "\n",
    "text_tag = \"text\"\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "\n",
    "\n",
    "tables = []\n",
    "layer = 11\n",
    "# for layer in range(0,12):\n",
    "per = 0.2\n",
    "print(\"Percentage: \", per)\n",
    "num_classes = 4\n",
    "\n",
    "# tao = 2.5\n",
    "\n",
    "lab = \"label\"\n",
    "# tao = torch.inf\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(dataset)\n",
    "# Set random seed\n",
    "seed_value = 42  # or any other integer\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():  # PyTorch-specific\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "special_tokens_dict = {}\n",
    "new_tokens = []\n",
    "label2text = dataset['train'].features[lab].names\n",
    "\n",
    "for label in label2text:\n",
    "    # Create special token format (with and without space)\n",
    "    special_token = f'{label}'\n",
    "    \n",
    "    # Check if the label is already a single token in the tokenizer\n",
    "    label_tokens = tokenizer.encode(label, add_special_tokens=False)\n",
    "    is_single_token = len(label_tokens) == 1\n",
    "    \n",
    "    if is_single_token:\n",
    "        print(f\"'{label}' is already a single token (ID: {label_tokens[0]})\")\n",
    "    \n",
    "    # Add both versions to new tokens list\n",
    "    new_tokens.extend([special_token])\n",
    "\n",
    "# Add the tokens to the tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"\\nAdded {num_added_tokens} new tokens to the tokenizer\")\n",
    "\n",
    "special_tokens = {\n",
    "    'pad_token': '<|pad|>',\n",
    "    'sep_token': '<|sep|>',\n",
    "    'eos_token': '<|eos|>'\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "def format_data(examples):\n",
    "    formatted_texts = []\n",
    "    for text, label in zip(examples[text_tag], examples[lab]):\n",
    "        # Convert label to string\n",
    "        \n",
    "        tok_text = tokenizer.encode(text, max_length=400, truncation=True)\n",
    "        text = tokenizer.decode(tok_text)\n",
    "        label_str = dataset['train'].features[lab].int2str(label)\n",
    "        formatted_text = f\"Classify emotion: {text}{tokenizer.sep_token}\"#{label_str}{tokenizer.eos_token}\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "    return {'formatted_text': formatted_texts}\n",
    "\n",
    "def tokenize_and_prepare(examples):\n",
    "\n",
    "    # Tokenize with batch processing\n",
    "    tokenized = tokenizer(\n",
    "        examples['formatted_text'],\n",
    "        padding='max_length',\n",
    "        max_length=408,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Clone input_ids to create labels\n",
    "    labels = tokenized['input_ids'].clone()\n",
    "    \n",
    "    # Find the position of sep_token\n",
    "    sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "    sep_positions = (labels == sep_token_id).nonzero(as_tuple=True)\n",
    "    \n",
    "    # Mask all tokens with -100 except for the token right after sep_token\n",
    "    labels[:] = -100  # Mask all initially\n",
    "    for batch_idx, sep_pos in zip(*sep_positions):\n",
    "        if sep_pos + 1 < labels.size(1):\n",
    "            labels[batch_idx, sep_pos + 1] = tokenized['input_ids'][batch_idx, sep_pos + 1]\n",
    "    \n",
    "    # Set padding tokens to -100\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "# Process the dataset\n",
    "formatted_dataset = dataset.map(format_data, batched=True)\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_and_prepare, \n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel as gt\n",
    "from models.gpt2 import GPT2LMHeadModel\n",
    "# Load pre-trained GPT-2 model\n",
    "model1 = gt.from_pretrained('gpt2')\n",
    "\n",
    "model1.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model1.config.m_layer = layer\n",
    "import os\n",
    "\n",
    "base_path = os.path.join(\"model_weights\", dataset_name)\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "weights_path = os.path.join(base_path, \"weights.pth\")\n",
    "\n",
    "model = GPT2LMHeadModel(model1.config)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "dataset_all = tokenized_dataset['train']\n",
    "\n",
    "all_fc_vals = []\n",
    "for j in range(0,num_classes):\n",
    "    dataset = dataset_all.filter(lambda x: x['label'] in [j])\n",
    "    fc_vals = evaluate_gpt2_classification(lab, model, dataset, tokenizer)\n",
    "    print('Accuracy : ', fc_vals[0], 'Confidence : ', fc_vals[1])\n",
    "    fc_vals = fc_vals[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utilities import compute_masks\n",
    "\n",
    "output_widgets = []\n",
    "j = 10\n",
    "max_all = []\n",
    "for i, v in enumerate(all_fc_vals):\n",
    "    v = np.array(v)\n",
    "    m = np.mean(np.abs(v), axis=0)\n",
    "    s = np.std(v, axis=0)\n",
    "    mini = np.min(v, axis=0)\n",
    "    maxi = np.max(v, axis=0)\n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max = compute_masks(v, 0.30)\n",
    "    max_all.append(mask_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print number of differences between the two classes\n",
    "print(torch.sum(max_all[0]!=max_all[1]))\n",
    "print(max_all[0]==max_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, v in enumerate(all_fc_vals):\n",
    "    v = np.array(v)\n",
    "    m = np.mean(np.abs(v), axis=0)\n",
    "    s = np.std(v, axis=0)\n",
    "    \n",
    "    \n",
    "    min_val = np.min(v, axis=0)\n",
    "    max_val = np.max(v, axis=0)    \n",
    "    \n",
    "    s = (s-min_val) / (max_val - min_val)\n",
    "    # m = (m-min_val) / (max_val - min_val)\n",
    "\n",
    "    # Create a new figure for each set of values\n",
    "    out = Output()\n",
    "    with out:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot the mean\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(m, 'bo', markersize=4)\n",
    "        plt.title(f'Mean of Activations - Set {i+1}')\n",
    "        plt.xlabel('Activation Index')\n",
    "        plt.ylabel('Mean Value')\n",
    "        plt.ylim(0, np.max(m))  # Ensure y-axis starts at 0\n",
    "\n",
    "        # Plot the standard deviation\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(s, 'ro', markersize=4)\n",
    "        plt.title(f'Standard Deviation of Activations - Set {i+1}')\n",
    "        plt.xlabel('Activation Index')\n",
    "        plt.ylabel('Standard Deviation')\n",
    "        plt.ylim(0, np.max(s))  # Ensure y-axis starts at 0\n",
    "\n",
    "        # Show the plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "VBox(output_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, v in enumerate(all_fc_vals):\n",
    "    v = np.array(v)\n",
    "    m = np.mean(np.abs(v), axis=0)\n",
    "    s = np.std(v, axis=0)\n",
    "    \n",
    "    min_val = np.min(v, axis=0)\n",
    "    max_val = np.max(v, axis=0)\n",
    "    s = (s-min_val) / (max_val - min_val)\n",
    "    # m = (m-min_val) / (max_val - min_val)\n",
    "    # Create a new figure for each set of values\n",
    "    out = Output()\n",
    "    with out:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Plot the mean with markers\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(768)),\n",
    "            y=m,\n",
    "            mode='markers',\n",
    "            name='Mean',\n",
    "            marker=dict(size=3, color='blue')\n",
    "        ))\n",
    "\n",
    "        # Plot the standard deviation with markers\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(768)),\n",
    "            y=s,\n",
    "            mode='markers',\n",
    "            name='Std Dev',\n",
    "            marker=dict(size=3, color='red')\n",
    "        ))\n",
    "\n",
    "        # Add lines connecting corresponding points\n",
    "        for j in range(768):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[j, j],\n",
    "                y=[m[j], s[j]],\n",
    "                mode='lines',\n",
    "                line=dict(color='gray', width=0.5),\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Mean and Standard Deviation of Activations - Set {i+1}',\n",
    "            xaxis_title='Activation Index',\n",
    "            yaxis_title='Value',\n",
    "            yaxis=dict(range=[0, max(np.max(m), np.max(s)) * 1.1]),\n",
    "            height=600,\n",
    "            width=1000,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "    \n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "# VBox(output_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "import importlib\n",
    "import utilities\n",
    "\n",
    "importlib.reload(utilities)\n",
    "from utilities import compute_masks\n",
    "from IPython.display import display\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, fc1 in enumerate(all_fc_vals):\n",
    "    fc1 = np.array(fc1)\n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_std_high_max = compute_masks(fc1, 0.15)\n",
    "    mask_std = mask_std_high_max\n",
    "    \n",
    "    m = np.mean(np.abs(fc1), axis=0)\n",
    "    s = np.std(fc1, axis=0)\n",
    "    min_val = np.min(fc1, axis=0)\n",
    "    max_val = np.max(fc1, axis=0)\n",
    "    \n",
    "    # Normalize std and mean\n",
    "    s_norm = (s - min_val) / (max_val - min_val)\n",
    "    m_norm = m#(m - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create indices for different masks\n",
    "    indices_max = np.where(mask_max == 0)[0]\n",
    "    indices_std = np.where(mask_std == 0)[0]\n",
    "    indices_intersection = np.intersect1d(indices_max, indices_std)\n",
    "    indices_max_minus_std = np.setdiff1d(indices_max, indices_std)\n",
    "    indices_std_minus_max = np.setdiff1d(indices_std, indices_max)\n",
    "    \n",
    "    # Count the indices in each set\n",
    "    count_max = len(indices_max)\n",
    "    count_std = len(indices_std)\n",
    "    count_intersection = len(indices_intersection)\n",
    "    count_max_minus_std = len(indices_max_minus_std)\n",
    "    count_std_minus_max = len(indices_std_minus_max)\n",
    "    \n",
    "    out = Output()\n",
    "    with out:\n",
    "        # Create subplots with counts in titles\n",
    "        fig = make_subplots(rows=2, cols=3, \n",
    "                            subplot_titles=(f\"Max Mask (Count: {count_max})\", \n",
    "                                            f\"Std Mask (Count: {count_std})\", \n",
    "                                            f\"Intersection (Count: {count_intersection})\",\n",
    "                                            f\"Max - Std (Count: {count_max_minus_std})\", \n",
    "                                            f\"Std - Max (Count: {count_std_minus_max})\"))\n",
    "        \n",
    "        # Helper function to add traces\n",
    "        def add_traces(indices, row, col):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices,\n",
    "                    y=m_norm[indices],\n",
    "                    mode='markers',\n",
    "                    name='Mean',\n",
    "                    marker=dict(size=3, color='blue'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices,\n",
    "                    y=s_norm[indices],\n",
    "                    mode='markers',\n",
    "                    name='Std Dev',\n",
    "                    marker=dict(size=3, color='red'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            for j in indices:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[j, j],\n",
    "                        y=[m_norm[j], s_norm[j]],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='gray', width=0.5),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        \n",
    "        # Add traces for all plots\n",
    "        add_traces(indices_max, 1, 1)\n",
    "        add_traces(indices_std, 1, 2)\n",
    "        add_traces(indices_intersection, 1, 3)\n",
    "        add_traces(indices_max_minus_std, 2, 1)\n",
    "        add_traces(indices_std_minus_max, 2, 2)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Mean and Standard Deviation of Activations - Class {i+1}',\n",
    "            height=1200,\n",
    "            width=1800,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update x and y axis labels for all subplots\n",
    "        for row in range(1, 3):\n",
    "            for col in range(1, 4):\n",
    "                if row == 2 and col == 3:\n",
    "                    continue  # Skip the empty subplot\n",
    "                fig.update_xaxes(title_text=\"Activation Index\", row=row, col=col)\n",
    "                fig.update_yaxes(title_text=\"Normalized Value\", row=row, col=col)\n",
    "        \n",
    "        display(fig)\n",
    "    \n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "from utilities import compute_masks\n",
    "from IPython.display import display\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, fc1 in enumerate(all_fc_vals):\n",
    "    fc1 = np.array(fc1)\n",
    "    mask_max, mask_std = compute_masks(fc1, 0.15)\n",
    "    \n",
    "    m = np.mean(np.abs(fc1), axis=0)\n",
    "    s = np.std(fc1, axis=0)\n",
    "    min_val = np.min(fc1, axis=0)\n",
    "    max_val = np.max(fc1, axis=0)\n",
    "    \n",
    "    # Normalize std and mean\n",
    "    s_norm = (s - min_val) / (max_val - min_val)\n",
    "    m_norm = m#(m - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create indices for different masks\n",
    "    indices_max = np.where(mask_max == 0)[0]\n",
    "    indices_std = np.where(mask_std == 0)[0]\n",
    "    indices_intersection = np.intersect1d(indices_max, indices_std)\n",
    "    indices_max_minus_std = np.setdiff1d(indices_max, indices_std)\n",
    "    indices_std_minus_max = np.setdiff1d(indices_std, indices_max)\n",
    "    \n",
    "    # Count the indices in each set\n",
    "    count_all = len(m_norm)\n",
    "    count_max = len(indices_max)\n",
    "    count_std = len(indices_std)\n",
    "    count_intersection = len(indices_intersection)\n",
    "    count_max_minus_std = len(indices_max_minus_std)\n",
    "    count_std_minus_max = len(indices_std_minus_max)\n",
    "    \n",
    "    out = Output()\n",
    "    with out:\n",
    "        # Create subplots with counts in titles\n",
    "        fig = make_subplots(rows=2, cols=3, \n",
    "                            subplot_titles=(f\"All Activations (Count: {count_all})\",\n",
    "                                            f\"Max Mask (Count: {count_max})\", \n",
    "                                            f\"Std Mask (Count: {count_std})\", \n",
    "                                            f\"Intersection (Count: {count_intersection})\",\n",
    "                                            f\"Max - Std (Count: {count_max_minus_std})\", \n",
    "                                            f\"Std - Max (Count: {count_std_minus_max})\"))\n",
    "        \n",
    "        # Helper function to add traces\n",
    "        def add_traces(indices, row, col):\n",
    "            indices_list = list(indices)  # Convert range or numpy array to list\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices_list,\n",
    "                    y=m_norm[indices_list],\n",
    "                    mode='markers',\n",
    "                    name='Mean',\n",
    "                    marker=dict(size=3, color='blue'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices_list,\n",
    "                    y=s_norm[indices_list],\n",
    "                    mode='markers',\n",
    "                    name='Std Dev',\n",
    "                    marker=dict(size=3, color='red'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            for j in indices_list:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[j, j],\n",
    "                        y=[m_norm[j], s_norm[j]],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='gray', width=0.5),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        \n",
    "        # Add traces for all activations\n",
    "        add_traces(range(len(m_norm)), 1, 1)\n",
    "        \n",
    "        # Add traces for other plots\n",
    "        add_traces(indices_max, 1, 2)\n",
    "        add_traces(indices_std, 1, 3)\n",
    "        add_traces(indices_intersection, 2, 1)\n",
    "        add_traces(indices_max_minus_std, 2, 2)\n",
    "        add_traces(indices_std_minus_max, 2, 3)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Mean and Standard Deviation of Activations - Class {i+1}',\n",
    "            height=1200,\n",
    "            width=1800,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update x and y axis labels for all subplots\n",
    "        for row in range(1, 3):\n",
    "            for col in range(1, 4):\n",
    "                fig.update_xaxes(title_text=\"Activation Index\", row=row, col=col)\n",
    "                fig.update_yaxes(title_text=\"Normalized Value\", row=row, col=col)\n",
    "        \n",
    "        display(fig)\n",
    "    \n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "from utilities import compute_masks\n",
    "from IPython.display import display\n",
    "\n",
    "def create_index_tracking_plot(indices_per_class, title):\n",
    "    num_classes = len(indices_per_class)\n",
    "    all_indices = sorted(set.union(*[set(indices) for indices in indices_per_class]))\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Create a color scale\n",
    "    color_scale = px.colors.diverging.RdYlGn_r  # Red to Yellow to Green color scale\n",
    "\n",
    "    # Add edges for indices present in multiple classes\n",
    "    for idx in all_indices:\n",
    "        classes_with_idx = [i for i, indices in enumerate(indices_per_class) if idx in indices]\n",
    "        if len(classes_with_idx) > 1:\n",
    "            x = [idx] * len(classes_with_idx)\n",
    "            y = classes_with_idx\n",
    "            color_index = (len(classes_with_idx) - 1) / (num_classes - 1)  # Normalize to [0, 1]\n",
    "            edge_color = px.colors.sample_colorscale(color_scale, [color_index])[0]\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode='lines',\n",
    "                line=dict(color=edge_color, width=2),\n",
    "                hoverinfo='text',\n",
    "                hovertext=f'Index: {idx}<br>Present in {len(classes_with_idx)} classes',\n",
    "                showlegend=False\n",
    "            ))\n",
    "    \n",
    "    # Add scatter plots for each class\n",
    "    for class_idx, indices in enumerate(indices_per_class):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=indices,\n",
    "            y=[class_idx] * len(indices),\n",
    "            mode='markers',\n",
    "            name=f'Class {class_idx + 1}',\n",
    "            marker=dict(size=4, symbol='circle', color='black'),\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f'Index: {idx}<br>Class: {class_idx + 1}' for idx in indices]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='Activation Index',\n",
    "        yaxis_title='Class',\n",
    "        yaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=list(range(num_classes)),\n",
    "            ticktext=[f'Class {i+1}' for i in range(num_classes)]\n",
    "        ),\n",
    "        hovermode='closest',\n",
    "        width=1500,\n",
    "        height=800,\n",
    "        plot_bgcolor='white',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    \n",
    "    # Add color bar\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            colorscale=color_scale,\n",
    "            showscale=True,\n",
    "            cmin=1,\n",
    "            cmax=num_classes,\n",
    "            colorbar=dict(\n",
    "                title='Number of Classes',\n",
    "                tickvals=list(range(1, num_classes+1)),\n",
    "                ticktext=list(range(1, num_classes+1))\n",
    "            )\n",
    "        ),\n",
    "        hoverinfo='none',\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Collect indices for each class\n",
    "max_indices_per_class = []\n",
    "std_indices_per_class = []\n",
    "\n",
    "for fc1 in all_fc_vals:\n",
    "    mask_max, mask_std = compute_masks(fc1, 0.15)\n",
    "    max_indices_per_class.append(np.where(mask_max == 0)[0])\n",
    "    std_indices_per_class.append(np.where(mask_std == 0)[0])\n",
    "\n",
    "# Create and display visualizations\n",
    "output_widgets = []\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "    fig_max = create_index_tracking_plot(max_indices_per_class, 'Max Mask Indices Across Classes')\n",
    "    display(fig_max)\n",
    "output_widgets.append(out)\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "    fig_std = create_index_tracking_plot(std_indices_per_class, 'Std Mask Indices Across Classes')\n",
    "    display(fig_std)\n",
    "output_widgets.append(out)\n",
    "\n",
    "# Display all visualizations\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "# from model_distill_bert import getmodel\n",
    "from utilities import compute_accuracy, compute_masks, mask_distillbert, get_model_distilbert, record_activations\n",
    "\n",
    "batch_size = 256\n",
    "mask_layer = 5\n",
    "text_tag = \"text\"\n",
    "compliment = True\n",
    "results_table = PrettyTable()\n",
    "if(compliment):\n",
    "   results_table.field_names = results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"Base Complement Acc\", \"Base Compliment Conf\", \"STD Accuracy\", \"STD Confidence\", \"STD compliment ACC\", \"STD compliment Conf\", \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\", \"Total Masked\", \"Intersedction\"]#, \"Same as Max\"]#\"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"\n",
    "# results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"STD Accuracy\", \"STD Confidence\", \"Same as Max\"]#, \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"]\n",
    "\n",
    "class_labels = []\n",
    "base_accuracies = []\n",
    "base_confidences = []\n",
    "base_comp_acc = []\n",
    "base_comp_conf = []\n",
    "std_masked_counts = []\n",
    "std_accuracies = []\n",
    "std_confidences = []\n",
    "std_comp_acc = []\n",
    "std_comp_conf = []\n",
    "max_masked_counts = []\n",
    "max_accuracies = []\n",
    "max_confidences = []\n",
    "max_comp_acc = []\n",
    "max_comp_conf = []\n",
    "diff_from_max = []\n",
    "total_masked = []\n",
    "\n",
    "dataset_list = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"2O24dpower2024/distilbert-base-uncased-finetuned-emotion\")\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset_all = load_dataset(\"dair-ai/emotion\")\n",
    "dataset_all = dataset_all['train']\n",
    "\n",
    "for j in range(0,7):\n",
    "    # model = get_model_distilbert(\"esuriddick/distilbert-base-uncased-finetuned-emotion\", mask_layer)\n",
    "    \n",
    "    model = get_model_distilbert(\"2O24dpower2024/distilbert-base-uncased-finetuned-emotion\", mask_layer)\n",
    "    dataset = dataset_all.filter(lambda x: x['label'] in [j])\n",
    "    dataset_complement = dataset_all.filter(lambda x: x['label'] not in [j])\n",
    "    \n",
    "    if(j==6):\n",
    "        dataset = dataset_all\n",
    "\n",
    "    class_labels.append(f\"Class {j}\")\n",
    "    acc = compute_accuracy(dataset, model, tokenizer, text_tag, batch_size=batch_size)\n",
    "    dataset_list.append(acc[2])\n",
    "    print(\"Class \",j, \"base accuracy: \", acc[0], acc[1])\n",
    "    base_accuracies.append(acc[0])\n",
    "    base_confidences.append(acc[1])\n",
    "    aug_dataset = acc[2]\n",
    "    if(compliment):\n",
    "        acc = compute_accuracy(dataset_complement, model, tokenizer, text_tag , batch_size=batch_size)\n",
    "        print(\"Class \",j, \"complement base accuracy: \", acc[0], acc[1])\n",
    "        base_comp_acc.append(acc[0])\n",
    "        base_comp_conf.append(acc[1])\n",
    "        aug_dataset.extend(acc[2])\n",
    "        \n",
    "\n",
    "    #record the activations of the first fully connected layer, CLS tokken\n",
    "    print(\"Recording activations...\")\n",
    "    # progress_bar = tqdm(total=len(dataset))\n",
    "    # model.to(device)\n",
    "    # model.eval()\n",
    "    # fc_vals = []\n",
    "    # with torch.no_grad():\n",
    "    #     for i in range(len(dataset)):\n",
    "    #         text = dataset[i]['sentence']\n",
    "    #         inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    #         outputs = model(**inputs)\n",
    "    #         fc_vals.append(outputs[1][mask_layer+1][:, 0].squeeze().cpu().numpy())\n",
    "    #         progress_bar.update(1)\n",
    "    #     progress_bar.close()\n",
    "\n",
    "    fc_vals = record_activations(dataset, model, tokenizer, text_tag=text_tag, mask_layer=mask_layer, batch_size=batch_size)\n",
    "\n",
    "        \n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max = compute_masks(fc_vals,0.50)\n",
    "    mask_std = mask_std_high_max\n",
    "    print(\"Masking STD...\")\n",
    "    model = mask_distillbert(model,mask_std)\n",
    "    t = int(mask_std.shape[0]-torch.count_nonzero(mask_std))\n",
    "    print(\"Total Masked :\", t)\n",
    "    total_masked.append(t)\n",
    "    diff_from_max.append(int((torch.logical_or(mask_std, mask_max) == 0).sum().item()))\n",
    "    acc = compute_accuracy(dataset, model, tokenizer, text_tag, batch_size=batch_size, in_aug_dataset=aug_dataset[:len(dataset)]) \n",
    "    dataset_list.append(acc[2])\n",
    "    print(\"accuracy after masking STD: \", acc[0], acc[1])\n",
    "    std_accuracies.append(acc[0])\n",
    "    std_confidences.append(acc[1])\n",
    "    if(compliment):\n",
    "        acc = compute_accuracy(dataset_complement, model, tokenizer, text_tag, batch_size=batch_size, in_aug_dataset=aug_dataset[len(dataset):])\n",
    "        print(\"accuracy after masking STD on complement: \", acc[0], acc[1])\n",
    "        std_comp_acc.append(acc[0])\n",
    "        std_comp_conf.append(acc[1])\n",
    "\n",
    "    print(\"Masking MAX...\")\n",
    "    model = mask_distillbert(model,mask_max)\n",
    "    t = int(mask_max.shape[0]-torch.count_nonzero(mask_max))\n",
    "    print(\"Total Masked :\", t)\n",
    "    # total_masked.append(t)\n",
    "    acc = compute_accuracy(dataset, model, tokenizer, text_tag, batch_size=batch_size, in_aug_dataset=aug_dataset[:len(dataset)])\n",
    "    dataset_list.append(acc[2])\n",
    "    print(\"accuracy after masking MAX: \", acc[0], acc[1])\n",
    "    max_accuracies.append(acc[0])\n",
    "    max_confidences.append(acc[1])\n",
    "    acc = compute_accuracy(dataset_complement, model, tokenizer, text_tag, batch_size=batch_size, in_aug_dataset=aug_dataset[len(dataset):])\n",
    "    print(\"accuracy after masking MAX on complement: \", acc[0], acc[1])\n",
    "    max_comp_acc.append(acc[0])\n",
    "    max_comp_conf.append(acc[1])\n",
    "    if(compliment):\n",
    "        results_table.add_row([\n",
    "            class_labels[j],\n",
    "            base_accuracies[j],\n",
    "            base_confidences[j],\n",
    "            base_comp_acc[j],\n",
    "            base_comp_conf[j],\n",
    "            std_accuracies[j],\n",
    "            std_confidences[j],\n",
    "            std_comp_acc[j],\n",
    "            std_comp_conf[j],\n",
    "            max_accuracies[j],\n",
    "            max_confidences[j],\n",
    "            max_comp_acc[j],\n",
    "            max_comp_conf[j],\n",
    "            total_masked[j],\n",
    "            diff_from_max[j]\n",
    "        ])\n",
    "    # results_table.add_row([\n",
    "    #     class_labels[j],\n",
    "    #     base_accuracies[j],\n",
    "    #     base_confidences[j],\n",
    "    #     std_accuracies[j],\n",
    "    #     std_confidences[j],\n",
    "    #     # max_accuracies[j],\n",
    "    #     # max_confidences[j],\n",
    "    #     diff_from_max[j]\n",
    "    # ])\n",
    "\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Assuming you already have your DataFrame\n",
    "# df = pd.DataFrame(dataset_list[1])\n",
    "\n",
    "def display_df(dataframe, rows_per_page=10):\n",
    "    # Convert the dataframe to HTML\n",
    "    html = dataframe.to_html(classes='table table-striped')\n",
    "    \n",
    "    # Add Bootstrap CSS\n",
    "    html = f\"\"\"\n",
    "    <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\">\n",
    "    <div class=\"container\">\n",
    "        {html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add pagination\n",
    "    total_rows = len(dataframe)\n",
    "    total_pages = (total_rows - 1) // rows_per_page + 1\n",
    "    \n",
    "    pagination_html = f\"\"\"\n",
    "    <nav>\n",
    "        <ul class=\"pagination justify-content-center\">\n",
    "            <li class=\"page-item\"><a class=\"page-link\" href=\"#\" id=\"prev-page\">Previous</a></li>\n",
    "            <li class=\"page-item\"><span class=\"page-link\" id=\"current-page\">1 / {total_pages}</span></li>\n",
    "            <li class=\"page-item\"><a class=\"page-link\" href=\"#\" id=\"next-page\">Next</a></li>\n",
    "        </ul>\n",
    "    </nav>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add JavaScript for pagination functionality\n",
    "    js = f\"\"\"\n",
    "    <script>\n",
    "        var currentPage = 1;\n",
    "        var rowsPerPage = {rows_per_page};\n",
    "        var totalPages = {total_pages};\n",
    "        \n",
    "        function showPage(page) {{\n",
    "            var rows = document.querySelectorAll('table.table tbody tr');\n",
    "            for (var i = 0; i < rows.length; i++) {{\n",
    "                if (i >= (page - 1) * rowsPerPage && i < page * rowsPerPage) {{\n",
    "                    rows[i].style.display = '';\n",
    "                }} else {{\n",
    "                    rows[i].style.display = 'none';\n",
    "                }}\n",
    "            }}\n",
    "            document.getElementById('current-page').textContent = page + ' / ' + totalPages;\n",
    "        }}\n",
    "        \n",
    "        document.getElementById('prev-page').addEventListener('click', function(e) {{\n",
    "            e.preventDefault();\n",
    "            if (currentPage > 1) {{\n",
    "                currentPage--;\n",
    "                showPage(currentPage);\n",
    "            }}\n",
    "        }});\n",
    "        \n",
    "        document.getElementById('next-page').addEventListener('click', function(e) {{\n",
    "            e.preventDefault();\n",
    "            if (currentPage < totalPages) {{\n",
    "                currentPage++;\n",
    "                showPage(currentPage);\n",
    "            }}\n",
    "        }});\n",
    "        \n",
    "        showPage(1);\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine all HTML and JavaScript\n",
    "    full_html = html + pagination_html + js\n",
    "    \n",
    "    # Display the result\n",
    "    display(HTML(full_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.iloc[::-1]\n",
    "display_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"esuriddick/distilbert-base-uncased-finetuned-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 & .969 & .956 & .913 & .903 & .678 & .653 & .845 & .839 & .983 & .914 & .847 & .793 \\\\\n",
      "Class 1 & .939 & .938 & .925 & .908 & .878 & .873 & .928 & .923 & .941 & .925 & .910 & .882 \\\\\n",
      "Class 2 & .902 & .872 & .932 & .923 & .790 & .786 & .762 & .758 & .703 & .685 & .902 & .827 \\\\\n",
      "Class 3 & .910 & .905 & .932 & .921 & .470 & .467 & .936 & .928 & .891 & .895 & .930 & .901 \\\\\n",
      "Class 4 & .869 & .854 & .938 & .927 & .747 & .704 & .902 & .896 & .851 & .851 & .925 & .887 \\\\\n",
      "Class 5 & .857 & .798 & .932 & .923 & .254 & .209 & .904 & .893 & .960 & .847 & .917 & .878 \\\\\n"
     ]
    }
   ],
   "source": [
    "data = '''Class 0\t0.9694\t0.9559\t0.913\t0.9032\t0.6781\t0.6527\t0.8445\t0.8386\t0.9828\t0.9139\t0.8465\t0.7929\t225\t225\n",
    "Class 1\t0.9385\t0.9382\t0.9248\t0.9085\t0.8778\t0.873\t0.9282\t0.9234\t0.9409\t0.9255\t0.9101\t0.8821\t222\t222\n",
    "Class 2\t0.9021\t0.8718\t0.9318\t0.9226\t0.7902\t0.786\t0.7619\t0.7582\t0.7028\t0.6849\t0.9025\t0.8274\t204\t204\n",
    "Class 3\t0.9097\t0.9055\t0.9325\t0.9206\t0.4702\t0.4671\t0.9358\t0.9282\t0.8912\t0.8946\t0.93\t0.9007\t227\t227\n",
    "Class 4\t0.869\t0.8537\t0.9378\t0.9274\t0.7471\t0.7041\t0.9017\t0.8963\t0.8506\t0.8511\t0.9248\t0.8871\t234\t234\n",
    "Class 5\t0.8571\t0.7979\t0.9321\t0.9229\t0.254\t0.2088\t0.9036\t0.8932\t0.9603\t0.8471\t0.9174\t0.8779\t239\t239'''\n",
    "\n",
    "def format_latex_row(line):\n",
    "    # Split the input line into components\n",
    "    parts = line.split()\n",
    "    \n",
    "    # Extract values, skipping the last two columns (384 and intersection)\n",
    "    values = parts[2:-2]\n",
    "    \n",
    "    # Format class name\n",
    "    class_name = f\"{parts[0]} {parts[1]}\"\n",
    "    \n",
    "    # Convert values to 3 decimal format\n",
    "    formatted_values = [f\"{float(val):.3f}\" for val in values]\n",
    "    \n",
    "    # Remove leading zeros\n",
    "    formatted_values = [val.replace('0.', '.') for val in formatted_values]\n",
    "    \n",
    "    # Combine into LaTeX format\n",
    "    return f\"{class_name} & \" + \" & \".join(formatted_values) + \" \\\\\\\\\"\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "for line in data.split('\\n'):\n",
    "    print(format_latex_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage:  0.3\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n",
      "'joy' is already a single token (ID: 2633)\n",
      "'love' is already a single token (ID: 23205)\n",
      "'anger' is already a single token (ID: 2564)\n",
      "\n",
      "Added 3 new tokens to the tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d1/grad/mha361/anaconda3/envs/memit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2673761/189444184.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording activations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132d6f7fb6524591ab252b56472d5470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:664: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item['input_ids']).to(device)\n",
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:665: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item['attention_mask']).to(device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f3a654ca9644dbb869b045090e3d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a764101c804d46bbad4fcf3bed7c255c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f15dd784859407ca09003e5974ce13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0150292fdee04a44829c5419f7fa182c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bac2aed1954c1b8642440f32895525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis for Percentage: 0.05\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |      38      |      12      |     0.3158     |\n",
      "|   1   |      38      |      9       |     0.2368     |\n",
      "|   2   |      38      |      8       |     0.2105     |\n",
      "|   3   |      38      |      9       |     0.2368     |\n",
      "|   4   |      38      |      10      |     0.2632     |\n",
      "|   5   |      38      |      14      |     0.3684     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.10\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |      76      |      18      |     0.2368     |\n",
      "|   1   |      76      |      19      |     0.2500     |\n",
      "|   2   |      76      |      23      |     0.3026     |\n",
      "|   3   |      76      |      15      |     0.1974     |\n",
      "|   4   |      76      |      22      |     0.2895     |\n",
      "|   5   |      76      |      24      |     0.3158     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.15\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     115      |      26      |     0.2261     |\n",
      "|   1   |     115      |      30      |     0.2609     |\n",
      "|   2   |     115      |      35      |     0.3043     |\n",
      "|   3   |     115      |      31      |     0.2696     |\n",
      "|   4   |     115      |      42      |     0.3652     |\n",
      "|   5   |     115      |      34      |     0.2957     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.20\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     153      |      44      |     0.2876     |\n",
      "|   1   |     153      |      44      |     0.2876     |\n",
      "|   2   |     153      |      50      |     0.3268     |\n",
      "|   3   |     153      |      51      |     0.3333     |\n",
      "|   4   |     153      |      58      |     0.3791     |\n",
      "|   5   |     153      |      52      |     0.3399     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.25\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     192      |      62      |     0.3229     |\n",
      "|   1   |     192      |      66      |     0.3438     |\n",
      "|   2   |     192      |      65      |     0.3385     |\n",
      "|   3   |     192      |      71      |     0.3698     |\n",
      "|   4   |     192      |      82      |     0.4271     |\n",
      "|   5   |     192      |      73      |     0.3802     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.30\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     230      |      82      |     0.3565     |\n",
      "|   1   |     230      |      87      |     0.3783     |\n",
      "|   2   |     230      |      84      |     0.3652     |\n",
      "|   3   |     230      |      92      |     0.4000     |\n",
      "|   4   |     230      |     106      |     0.4609     |\n",
      "|   5   |     230      |      93      |     0.4043     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.35\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     268      |     108      |     0.4030     |\n",
      "|   1   |     268      |     121      |     0.4515     |\n",
      "|   2   |     268      |     114      |     0.4254     |\n",
      "|   3   |     268      |     119      |     0.4440     |\n",
      "|   4   |     268      |     133      |     0.4963     |\n",
      "|   5   |     268      |     119      |     0.4440     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.40\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     307      |     146      |     0.4756     |\n",
      "|   1   |     307      |     140      |     0.4560     |\n",
      "|   2   |     307      |     137      |     0.4463     |\n",
      "|   3   |     307      |     149      |     0.4853     |\n",
      "|   4   |     307      |     165      |     0.5375     |\n",
      "|   5   |     307      |     158      |     0.5147     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.45\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     345      |     181      |     0.5246     |\n",
      "|   1   |     345      |     176      |     0.5101     |\n",
      "|   2   |     345      |     171      |     0.4957     |\n",
      "|   3   |     345      |     190      |     0.5507     |\n",
      "|   4   |     345      |     205      |     0.5942     |\n",
      "|   5   |     345      |     191      |     0.5536     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.50\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     384      |     225      |     0.5859     |\n",
      "|   1   |     384      |     222      |     0.5781     |\n",
      "|   2   |     384      |     204      |     0.5312     |\n",
      "|   3   |     384      |     227      |     0.5911     |\n",
      "|   4   |     384      |     234      |     0.6094     |\n",
      "|   5   |     384      |     239      |     0.6224     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.55\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     422      |     265      |     0.6280     |\n",
      "|   1   |     422      |     260      |     0.6161     |\n",
      "|   2   |     422      |     251      |     0.5948     |\n",
      "|   3   |     422      |     272      |     0.6445     |\n",
      "|   4   |     422      |     278      |     0.6588     |\n",
      "|   5   |     422      |     274      |     0.6493     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.60\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     460      |     311      |     0.6761     |\n",
      "|   1   |     460      |     300      |     0.6522     |\n",
      "|   2   |     460      |     304      |     0.6609     |\n",
      "|   3   |     460      |     324      |     0.7043     |\n",
      "|   4   |     460      |     311      |     0.6761     |\n",
      "|   5   |     460      |     323      |     0.7022     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.65\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     499      |     365      |     0.7315     |\n",
      "|   1   |     499      |     357      |     0.7154     |\n",
      "|   2   |     499      |     363      |     0.7275     |\n",
      "|   3   |     499      |     371      |     0.7435     |\n",
      "|   4   |     499      |     357      |     0.7154     |\n",
      "|   5   |     499      |     369      |     0.7395     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.70\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     537      |     415      |     0.7728     |\n",
      "|   1   |     537      |     412      |     0.7672     |\n",
      "|   2   |     537      |     414      |     0.7709     |\n",
      "|   3   |     537      |     417      |     0.7765     |\n",
      "|   4   |     537      |     408      |     0.7598     |\n",
      "|   5   |     537      |     419      |     0.7803     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.75\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     576      |     465      |     0.8073     |\n",
      "|   1   |     576      |     469      |     0.8142     |\n",
      "|   2   |     576      |     467      |     0.8108     |\n",
      "|   3   |     576      |     471      |     0.8177     |\n",
      "|   4   |     576      |     466      |     0.8090     |\n",
      "|   5   |     576      |     473      |     0.8212     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.80\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     614      |     527      |     0.8583     |\n",
      "|   1   |     614      |     528      |     0.8599     |\n",
      "|   2   |     614      |     529      |     0.8616     |\n",
      "|   3   |     614      |     522      |     0.8502     |\n",
      "|   4   |     614      |     520      |     0.8469     |\n",
      "|   5   |     614      |     525      |     0.8550     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.85\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     652      |     586      |     0.8988     |\n",
      "|   1   |     652      |     585      |     0.8972     |\n",
      "|   2   |     652      |     585      |     0.8972     |\n",
      "|   3   |     652      |     575      |     0.8819     |\n",
      "|   4   |     652      |     582      |     0.8926     |\n",
      "|   5   |     652      |     583      |     0.8942     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.90\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     691      |     644      |     0.9320     |\n",
      "|   1   |     691      |     648      |     0.9378     |\n",
      "|   2   |     691      |     642      |     0.9291     |\n",
      "|   3   |     691      |     633      |     0.9161     |\n",
      "|   4   |     691      |     647      |     0.9363     |\n",
      "|   5   |     691      |     642      |     0.9291     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 0.95\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     729      |     706      |     0.9684     |\n",
      "|   1   |     729      |     705      |     0.9671     |\n",
      "|   2   |     729      |     705      |     0.9671     |\n",
      "|   3   |     729      |     700      |     0.9602     |\n",
      "|   4   |     729      |     706      |     0.9684     |\n",
      "|   5   |     729      |     703      |     0.9643     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n",
      "\n",
      "Analysis for Percentage: 1.00\n",
      "+-------+--------------+--------------+----------------+\n",
      "| Class | Total Masked | Common Zeros | Common Zeros % |\n",
      "+-------+--------------+--------------+----------------+\n",
      "|   0   |     768      |     768      |     1.0000     |\n",
      "|   1   |     768      |     768      |     1.0000     |\n",
      "|   2   |     768      |     768      |     1.0000     |\n",
      "|   3   |     768      |     768      |     1.0000     |\n",
      "|   4   |     768      |     768      |     1.0000     |\n",
      "|   5   |     768      |     768      |     1.0000     |\n",
      "+-------+--------------+--------------+----------------+\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "import random\n",
    "import numpy as np\n",
    "from utilities import evaluate_gpt2_classification as evaluate_gpt2_classification, mask_range_gpt,compute_masks, reset_gpt, compute_mask_probe\n",
    "import torch  \n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_name = \"dair-ai/emotion\"\n",
    "\n",
    "text_tag = \"text\"\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "\n",
    "\n",
    "tables = []\n",
    "layer = 11\n",
    "# for i in tqdm(range(1, 21)):\n",
    "per = 0.3\n",
    "print(\"Percentage: \", per)\n",
    "num_classes = 6\n",
    "\n",
    "# tao = 2.5\n",
    "\n",
    "lab = \"label\"\n",
    "# tao = torch.inf\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(dataset)\n",
    "# Set random seed\n",
    "seed_value = 42  # or any other integer\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():  # PyTorch-specific\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "special_tokens_dict = {}\n",
    "new_tokens = []\n",
    "label2text = dataset['train'].features[lab].names\n",
    "\n",
    "for label in label2text:\n",
    "    # Create special token format (with and without space)\n",
    "    special_token = f'{label}'\n",
    "    \n",
    "    # Check if the label is already a single token in the tokenizer\n",
    "    label_tokens = tokenizer.encode(label, add_special_tokens=False)\n",
    "    is_single_token = len(label_tokens) == 1\n",
    "    \n",
    "    if is_single_token:\n",
    "        print(f\"'{label}' is already a single token (ID: {label_tokens[0]})\")\n",
    "    \n",
    "    # Add both versions to new tokens list\n",
    "    new_tokens.extend([special_token])\n",
    "\n",
    "# Add the tokens to the tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"\\nAdded {num_added_tokens} new tokens to the tokenizer\")\n",
    "\n",
    "special_tokens = {\n",
    "    'pad_token': '<|pad|>',\n",
    "    'sep_token': '<|sep|>',\n",
    "    'eos_token': '<|eos|>'\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "def format_data(examples):\n",
    "    formatted_texts = []\n",
    "    for text, label in zip(examples[text_tag], examples[lab]):\n",
    "        # Convert label to string\n",
    "        \n",
    "        tok_text = tokenizer.encode(text, max_length=400, truncation=True)\n",
    "        text = tokenizer.decode(tok_text)\n",
    "        label_str = dataset['train'].features[lab].int2str(label)\n",
    "        formatted_text = f\"Classify emotion: {text}{tokenizer.sep_token}\"#{label_str}{tokenizer.eos_token}\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "    return {'formatted_text': formatted_texts}\n",
    "\n",
    "def tokenize_and_prepare(examples):\n",
    "\n",
    "    # Tokenize with batch processing\n",
    "    tokenized = tokenizer(\n",
    "        examples['formatted_text'],\n",
    "        padding='max_length',\n",
    "        max_length=408,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Clone input_ids to create labels\n",
    "    labels = tokenized['input_ids'].clone()\n",
    "    \n",
    "    # Find the position of sep_token\n",
    "    sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "    sep_positions = (labels == sep_token_id).nonzero(as_tuple=True)\n",
    "    \n",
    "    # Mask all tokens with -100 except for the token right after sep_token\n",
    "    labels[:] = -100  # Mask all initially\n",
    "    for batch_idx, sep_pos in zip(*sep_positions):\n",
    "        if sep_pos + 1 < labels.size(1):\n",
    "            labels[batch_idx, sep_pos + 1] = tokenized['input_ids'][batch_idx, sep_pos + 1]\n",
    "    \n",
    "    # Set padding tokens to -100\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "# Process the dataset\n",
    "formatted_dataset = dataset.map(format_data, batched=True)\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_and_prepare, \n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel as gt\n",
    "from models.gpt2 import GPT2LMHeadModel\n",
    "# Load pre-trained GPT-2 model\n",
    "model1 = gt.from_pretrained('gpt2')\n",
    "\n",
    "model1.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model1.config.m_layer = layer\n",
    "import os\n",
    "\n",
    "base_path = os.path.join(\"model_weights\", dataset_name)\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "weights_path = os.path.join(base_path, \"weights.pth\")\n",
    "\n",
    "model = GPT2LMHeadModel(model1.config)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.tensor\")\n",
    "\n",
    "batch_size = 2048/2\n",
    "# mask_layer = 5\n",
    "compliment = True\n",
    "results_table = PrettyTable()\n",
    "if(compliment):\n",
    "    results_table.field_names = results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"Base Complement Acc\", \"Base Compliment Conf\", \"STD Accuracy\", \"STD Confidence\", \"STD compliment ACC\", \"STD compliment Conf\", \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\", \"Total Masked\", \"Intersection\"]#, \"Same as Max\"]#\"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"\n",
    "\n",
    "class_labels = []\n",
    "base_accuracies = []\n",
    "base_confidences = []\n",
    "base_comp_acc = []\n",
    "base_comp_conf = []\n",
    "std_masked_counts = []\n",
    "std_accuracies = []\n",
    "std_confidences = []\n",
    "std_comp_acc = []\n",
    "std_comp_conf = []\n",
    "max_masked_counts = []\n",
    "max_accuracies = []\n",
    "max_confidences = []\n",
    "max_comp_acc = []\n",
    "max_comp_conf = []\n",
    "diff_from_max = []\n",
    "total_masked = []\n",
    "\n",
    "#merge test and train set and then shuffle and make splits\n",
    "\n",
    "# First merge and shuffle\n",
    "tokenized_dataset = concatenate_datasets([tokenized_dataset['train'], tokenized_dataset['test']]).shuffle(seed=42)#.select(range(100))\n",
    "\n",
    "# Get the total length\n",
    "dataset_length = len(tokenized_dataset)\n",
    "\n",
    "\n",
    "# Calculate split index\n",
    "split_index = int(dataset_length * 0.2)  # 80% for training\n",
    "\n",
    "# Create the splits using dataset slicing\n",
    "tokenized_dataset1 = tokenized_dataset.select(range(split_index))  # training set\n",
    "recording_dataset = tokenized_dataset.select(range(split_index, dataset_length))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_fc_vals = []\n",
    "print(\"Recording activations...\")\n",
    "for j in range(0,num_classes):\n",
    "    dataset_recording = recording_dataset.filter(lambda x: x[lab] in [j])\n",
    "    fc_vals = evaluate_gpt2_classification(lab, model, dataset_recording, tokenizer)\n",
    "    fc_vals = fc_vals[2]\n",
    "    all_fc_vals.append(fc_vals)\n",
    "    \n",
    "# save all_fc_vals to file\n",
    "import pickle\n",
    "\n",
    "with open('all_fc_vals.pkl', 'wb') as f:\n",
    "    pickle.dump(all_fc_vals, f)\n",
    "    \n",
    "# Load all_fc_vals from file\n",
    "with open('all_fc_vals.pkl', 'rb') as f:\n",
    "    all_fc_vals = pickle.load(f)\n",
    "\n",
    "# from plot_correlation import create_correlation_plots\n",
    "\n",
    "# create_correlation_plots(all_fc_vals, num_classes)\n",
    "for i in range(1, 21):\n",
    "    per = 0.05 * i\n",
    "    print(f\"\\nAnalysis for Percentage: {per:.2f}\")\n",
    "    \n",
    "    # Create table for this percentage\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Class\", \"Total Masked\", \"Common Zeros\", \"Common Zeros %\"]\n",
    "    \n",
    "    for j in range(0, num_classes):\n",
    "        # Compute masks\n",
    "        mask_max, mask_std, mask_intersection, mask_max_low_std, \\\n",
    "        mask_max_high_std, mask_std_high_max, mask_max_random_off, \\\n",
    "        random_mask = compute_masks(all_fc_vals[j], per)\n",
    "        \n",
    "        # Calculate intersections and zeros\n",
    "        intersection_zeros = (~mask_std.bool()) & (~mask_max.bool())\n",
    "        num_common_zeros = intersection_zeros.sum().item()\n",
    "        total_masked = int(mask_std.shape[0] - torch.count_nonzero(mask_std))\n",
    "        percentage_common_zeros = num_common_zeros / total_masked\n",
    "        \n",
    "        # Add row to table\n",
    "        table.add_row([\n",
    "            j,\n",
    "            total_masked,\n",
    "            num_common_zeros,\n",
    "            f\"{percentage_common_zeros:.4f}\"\n",
    "        ])\n",
    "    \n",
    "    print(table)\n",
    "    print(\"-\" * 50)  # Separator between tables\n",
    "    \n",
    "    \n",
    "total_masked = []\n",
    "# print(\"Training Probe...\")\n",
    "\n",
    "# from linear_probe import train_probe\n",
    "\n",
    "# probe = train_probe(all_fc_vals,lambda_l1 = 1e-4, lambda_l2 = 1e-4)\n",
    "\n",
    "# print(\"Probe Trained\")\n",
    "\n",
    "# probe.to('cpu')\n",
    "\n",
    "# probe_weights = list(probe.parameters())[0]\n",
    "# tokenized_dataset1 = tokenized_dataset['test']#.shuffle().select(range(200))\n",
    "# recording_dataset = tokenized_dataset['train']#.shuffle().select(range(200))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca398da2a82541e7a6a170eb3e2dd7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0 base accuracy:  0.9694 0.9559\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42433a69d664ceda4c7bc48eb48b0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0 complement base accuracy:  0.913 0.9032\n",
      "Masking Probe...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(fc_vals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     61\u001b[0m mean[intersection_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m---> 63\u001b[0m mask_max_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39margsort(mean, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:num_common_zeros]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# mask_max_indices = mask_max_indices-intersection_indices\u001b[39;00m\n\u001b[1;32m     67\u001b[0m mask_max \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(mask_max)\n",
      "Cell \u001b[0;32mIn[29], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(fc_vals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     61\u001b[0m mean[intersection_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m---> 63\u001b[0m mask_max_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39margsort(mean, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:num_common_zeros]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# mask_max_indices = mask_max_indices-intersection_indices\u001b[39;00m\n\u001b[1;32m     67\u001b[0m mask_max \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(mask_max)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class_labels = []\n",
    "base_accuracies = []\n",
    "base_confidences = []\n",
    "base_comp_acc = []\n",
    "base_comp_conf = []\n",
    "std_masked_counts = []\n",
    "std_accuracies = []\n",
    "std_confidences = []\n",
    "std_comp_acc = []\n",
    "std_comp_conf = []\n",
    "max_masked_counts = []\n",
    "max_accuracies = []\n",
    "max_confidences = []\n",
    "max_comp_acc = []\n",
    "max_comp_conf = []\n",
    "diff_from_max = []\n",
    "total_masked = []\n",
    "for j in range(0,2):\n",
    "    \n",
    "    fc_vals = all_fc_vals[j]\n",
    "    model = reset_gpt(model)\n",
    "    dataset = tokenized_dataset1.filter(lambda x: x[lab] in [j])\n",
    "    dataset_recording = recording_dataset.filter(lambda x: x[lab] in [j])\n",
    "    dataset_complement = tokenized_dataset1.filter(lambda x: x[lab] not in [j])\n",
    "    \n",
    "\n",
    "    class_labels.append(f\"Class {j}\")\n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset, tokenizer)\n",
    "    print(\"Class \",j, \"base accuracy: \", acc[0], acc[1])\n",
    "    base_accuracies.append(acc[0])\n",
    "    base_confidences.append(acc[1])\n",
    "    if(compliment):\n",
    "        acc = evaluate_gpt2_classification(lab, model, dataset_complement, tokenizer)\n",
    "        print(\"Class \",j, \"complement base accuracy: \", acc[0], acc[1])\n",
    "        base_comp_acc.append(acc[0])\n",
    "        base_comp_conf.append(acc[1])\n",
    "    # print(\"Recording activations...\")\n",
    "    # fc_vals = evaluate_gpt2_classi\n",
    "        \n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max, mask_max_random_off, random_mask = compute_masks(fc_vals,0.3)\n",
    "    # mask_probe = compute_mask_probe(probe_weights[j], 0.3)\n",
    "    # mask_std = mask_max_low_std\n",
    "    print(\"Masking Probe...\")\n",
    "    # model = mask_distillbert(model,mask_std)\n",
    " \n",
    "    \n",
    "    # diff_from_max.append(int((torch.logical_or(mask_std, mask_max) == 0).sum().item()))\n",
    "    \n",
    "    intersection_zeros = (~mask_std.bool()) & (~mask_max.bool())\n",
    "    \n",
    "    mask_std[~intersection_zeros] = 1\n",
    "    \n",
    "    intersection_indices = torch.where(intersection_zeros)\n",
    "    \n",
    "    fc_vals2 = torch.tensor(fc_vals)\n",
    "    \n",
    "    num_common_zeros = intersection_zeros.sum().item()\n",
    "    \n",
    "    mean = torch.mean(fc_vals2, axis=0)\n",
    "    \n",
    "    mean[intersection_indices] = -torch.inf\n",
    "    \n",
    "    mask_max_indices = torch.argsort(mean, descending=True)[:num_common_zeros]\n",
    "    \n",
    "    # mask_max_indices = mask_max_indices-intersection_indices\n",
    "        \n",
    "    mask_max = torch.ones_like(mask_max)\n",
    "    \n",
    "    mask_max[mask_max_indices] = 0.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Number of non zeros: \", torch.count_nonzero(mask_max))\n",
    "    print(\"Number of common zeros: \", num_common_zeros)\n",
    "    \n",
    "    diff_from_max.append(num_common_zeros)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    tao = torch.inf#2.5\n",
    "    model = mask_range_gpt(model, mask_std, fc_vals, tao)        \n",
    "    t = int(mask_std.shape[0]-torch.count_nonzero(mask_std))\n",
    "    print(\"Total Masked :\", t)\n",
    "    total_masked.append(t)\n",
    "    \n",
    "    \n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset, tokenizer) \n",
    "    print(\"accuracy after masking STD: \", acc[0], acc[1])\n",
    "    std_accuracies.append(acc[0])\n",
    "    std_confidences.append(acc[1])\n",
    "    if(compliment):\n",
    "        acc = evaluate_gpt2_classification(lab, model, dataset_complement, tokenizer)\n",
    "        print(\"accuracy after masking STD on complement: \", acc[0], acc[1])\n",
    "        std_comp_acc.append(acc[0])\n",
    "        std_comp_conf.append(acc[1])\n",
    "    model = reset_gpt(model)\n",
    "    tao = torch.inf\n",
    "    print(\"Masking MAX...\")\n",
    "    # model = mask_distillbert(model,mask_max) \n",
    "    model = mask_range_gpt(model, mask_max, fc_vals, tao)\n",
    "    t = int(mask_max.shape[0]-torch.count_nonzero(mask_max))\n",
    "    print(\"Total Masked :\", t)\n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset, tokenizer)\n",
    "    print(\"accuracy after masking MAX: \", acc[0], acc[1])\n",
    "    max_accuracies.append(acc[0])\n",
    "    max_confidences.append(acc[1])\n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset_complement, tokenizer)\n",
    "    print(\"accuracy after masking MAX on complement: \", acc[0], acc[1])\n",
    "    max_comp_acc.append(acc[0])\n",
    "    max_comp_conf.append(acc[1])\n",
    "    if(compliment):\n",
    "        results_table.add_row([\n",
    "            class_labels[j],\n",
    "            base_accuracies[j],\n",
    "            base_confidences[j],\n",
    "            base_comp_acc[j],\n",
    "            base_comp_conf[j],\n",
    "            std_accuracies[j],\n",
    "            std_confidences[j],\n",
    "            std_comp_acc[j],\n",
    "            std_comp_conf[j],\n",
    "            max_accuracies[j],\n",
    "            max_confidences[j],\n",
    "            max_comp_acc[j],\n",
    "            max_comp_conf[j],\n",
    "            total_masked[j],\n",
    "            diff_from_max[j]\n",
    "        ])            \n",
    "# print(\"Layer \", mask_layer)\n",
    "print(results_table)\n",
    "tables.append(results_table)\n",
    "# print(\"Layer \", mask_layer)\n",
    "print(\"Average Base Accuracy: \",round(sum(base_accuracies)/len(base_accuracies), 4))\n",
    "print(\"Average Base Confidence: \", round(sum(base_confidences)/len(base_confidences), 4))\n",
    "print(\"Average MAX Accuracy: \", round(sum(max_accuracies)/len(max_accuracies), 4))\n",
    "print(\"Average MAX Confidence: \", round(sum(max_confidences)/len(max_confidences), 4))\n",
    "print(\"Average MAX Complement Accuracy: \", round(sum(max_comp_acc)/len(max_comp_acc), 4))\n",
    "print(\"Average MAX Complement Confidence: \", round(sum(max_comp_conf)/len(max_comp_conf), 4))\n",
    "\n",
    "# for table in tables:\n",
    "# print(table)\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[82, 87, 84]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8284, 0.9164, 0.9289]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_comp_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8324, 0.7941, 0.7012]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9016, 0.8985, 0.9089]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_comp_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
