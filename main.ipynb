{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from utilities import get_model_distilbert, record_activations\n",
    "\n",
    "mask_layer = 5\n",
    "text_tag = \"sentence\"\n",
    "compliment = True\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the dataset\n",
    "dataset_all = load_dataset(\"stanfordnlp/sst2\")\n",
    "# Select the train split\n",
    "dataset_all = dataset_all['train']\n",
    "model = get_model_distilbert(\"distilbert-base-uncased-finetuned-sst-2-english\", mask_layer)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "all_fc_vals = []\n",
    "batch_size = 32  # You can adjust this based on your GPU memory\n",
    "for j in range(0,2):\n",
    "    dataset = dataset_all.filter(lambda x: x['label'] in [j])\n",
    "    fc_vals = record_activations(dataset, model, tokenizer, text_tag='sentence', batch_size=256, mask_layer=mask_layer)\n",
    "    all_fc_vals.append(fc_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage:  0.2\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n",
      "'World' is already a single token (ID: 10603)\n",
      "'Sports' is already a single token (ID: 18153)\n",
      "'Business' is already a single token (ID: 24749)\n",
      "\n",
      "Added 1 new tokens to the tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d1/grad/mha361/anaconda3/envs/memit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1707745/1257197563.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16e97cedcd14edbaeda4ed2b0711d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a3a82909c24a8ebe3122f01fabb526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:591: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item['input_ids']).to(device)\n",
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:592: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item['attention_mask']).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9682 Confidence :  0.9625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db260de288bb4880b7d6b34f4966469f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m all_fc_vals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m--> 153\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     fc_vals \u001b[38;5;241m=\u001b[39m evaluate_gpt2_classification(lab, model, dataset, tokenizer)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy : \u001b[39m\u001b[38;5;124m'\u001b[39m, fc_vals[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence : \u001b[39m\u001b[38;5;124m'\u001b[39m, fc_vals[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:3771\u001b[0m, in \u001b[0;36mDataset.filter\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 3771\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_indices_from_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3779\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3793\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3798\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3799\u001b[0m new_dataset\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:3167\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3163\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3164\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3165\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3166\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3168\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3169\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:3558\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3554\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3555\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3556\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3558\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3566\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3567\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:3427\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3426\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3427\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3429\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3430\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3431\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:6538\u001b[0m, in \u001b[0;36mget_indices_from_mask_function\u001b[0;34m(function, batched, with_indices, with_rank, input_columns, indices_mapping, *args, **fn_kwargs)\u001b[0m\n\u001b[1;32m   6536\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys()))])\n\u001b[1;32m   6537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[0;32m-> 6538\u001b[0m     example \u001b[38;5;241m=\u001b[39m {key: batch[key][i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m   6539\u001b[0m     additional_args \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   6540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/arrow_dataset.py:6538\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   6536\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys()))])\n\u001b[1;32m   6537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[0;32m-> 6538\u001b[0m     example \u001b[38;5;241m=\u001b[39m {key: \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m   6539\u001b[0m     additional_args \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   6540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/formatting/formatting.py:279\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    277\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m--> 279\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format\u001b[38;5;241m.\u001b[39mremove(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/formatting/formatting.py:382\u001b[0m, in \u001b[0;36mLazyBatch.format\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/formatting/formatting.py:448\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 448\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/datasets/formatting/formatting.py:148\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "import random\n",
    "import numpy as np\n",
    "from utilities import evaluate_gpt2_classification as evaluate_gpt2_classification, mask_range_gpt,compute_masks, reset_gpt\n",
    "import torch  \n",
    "\n",
    "dataset_name = \"fancyzhx/dbpedia_14\"\n",
    "\n",
    "text_tag = \"text\"\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "\n",
    "\n",
    "tables = []\n",
    "layer = 11\n",
    "# for layer in range(0,12):\n",
    "per = 0.2\n",
    "print(\"Percentage: \", per)\n",
    "num_classes = 4\n",
    "\n",
    "# tao = 2.5\n",
    "\n",
    "lab = \"label\"\n",
    "# tao = torch.inf\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(dataset)\n",
    "# Set random seed\n",
    "seed_value = 42  # or any other integer\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():  # PyTorch-specific\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "special_tokens_dict = {}\n",
    "new_tokens = []\n",
    "label2text = dataset['train'].features[lab].names\n",
    "\n",
    "for label in label2text:\n",
    "    # Create special token format (with and without space)\n",
    "    special_token = f'{label}'\n",
    "    \n",
    "    # Check if the label is already a single token in the tokenizer\n",
    "    label_tokens = tokenizer.encode(label, add_special_tokens=False)\n",
    "    is_single_token = len(label_tokens) == 1\n",
    "    \n",
    "    if is_single_token:\n",
    "        print(f\"'{label}' is already a single token (ID: {label_tokens[0]})\")\n",
    "    \n",
    "    # Add both versions to new tokens list\n",
    "    new_tokens.extend([special_token])\n",
    "\n",
    "# Add the tokens to the tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"\\nAdded {num_added_tokens} new tokens to the tokenizer\")\n",
    "\n",
    "special_tokens = {\n",
    "    'pad_token': '<|pad|>',\n",
    "    'sep_token': '<|sep|>',\n",
    "    'eos_token': '<|eos|>'\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "def format_data(examples):\n",
    "    formatted_texts = []\n",
    "    for text, label in zip(examples[text_tag], examples[lab]):\n",
    "        # Convert label to string\n",
    "        \n",
    "        tok_text = tokenizer.encode(text, max_length=400, truncation=True)\n",
    "        text = tokenizer.decode(tok_text)\n",
    "        label_str = dataset['train'].features[lab].int2str(label)\n",
    "        formatted_text = f\"Classify emotion: {text}{tokenizer.sep_token}\"#{label_str}{tokenizer.eos_token}\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "    return {'formatted_text': formatted_texts}\n",
    "\n",
    "def tokenize_and_prepare(examples):\n",
    "\n",
    "    # Tokenize with batch processing\n",
    "    tokenized = tokenizer(\n",
    "        examples['formatted_text'],\n",
    "        padding='max_length',\n",
    "        max_length=408,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Clone input_ids to create labels\n",
    "    labels = tokenized['input_ids'].clone()\n",
    "    \n",
    "    # Find the position of sep_token\n",
    "    sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "    sep_positions = (labels == sep_token_id).nonzero(as_tuple=True)\n",
    "    \n",
    "    # Mask all tokens with -100 except for the token right after sep_token\n",
    "    labels[:] = -100  # Mask all initially\n",
    "    for batch_idx, sep_pos in zip(*sep_positions):\n",
    "        if sep_pos + 1 < labels.size(1):\n",
    "            labels[batch_idx, sep_pos + 1] = tokenized['input_ids'][batch_idx, sep_pos + 1]\n",
    "    \n",
    "    # Set padding tokens to -100\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "# Process the dataset\n",
    "formatted_dataset = dataset.map(format_data, batched=True)\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_and_prepare, \n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel as gt\n",
    "from models.gpt2 import GPT2LMHeadModel\n",
    "# Load pre-trained GPT-2 model\n",
    "model1 = gt.from_pretrained('gpt2')\n",
    "\n",
    "model1.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model1.config.m_layer = layer\n",
    "import os\n",
    "\n",
    "base_path = os.path.join(\"model_weights\", dataset_name)\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "weights_path = os.path.join(base_path, \"weights.pth\")\n",
    "\n",
    "model = GPT2LMHeadModel(model1.config)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "dataset_all = tokenized_dataset['train']\n",
    "\n",
    "all_fc_vals = []\n",
    "for j in range(0,num_classes):\n",
    "    dataset = dataset_all.filter(lambda x: x['label'] in [j])\n",
    "    fc_vals = evaluate_gpt2_classification(lab, model, dataset, tokenizer)\n",
    "    print('Accuracy : ', fc_vals[0], 'Confidence : ', fc_vals[1])\n",
    "    fc_vals = fc_vals[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utilities import compute_masks\n",
    "\n",
    "output_widgets = []\n",
    "j = 10\n",
    "max_all = []\n",
    "for i, v in enumerate(all_fc_vals):\n",
    "    v = np.array(v)\n",
    "    m = np.mean(np.abs(v), axis=0)\n",
    "    s = np.std(v, axis=0)\n",
    "    mini = np.min(v, axis=0)\n",
    "    maxi = np.max(v, axis=0)\n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max = compute_masks(v, 0.30)\n",
    "    max_all.append(mask_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print number of differences between the two classes\n",
    "print(torch.sum(max_all[0]!=max_all[1]))\n",
    "print(max_all[0]==max_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, v in enumerate(all_fc_vals):\n",
    "    v = np.array(v)\n",
    "    m = np.mean(np.abs(v), axis=0)\n",
    "    s = np.std(v, axis=0)\n",
    "    \n",
    "    \n",
    "    min_val = np.min(v, axis=0)\n",
    "    max_val = np.max(v, axis=0)    \n",
    "    \n",
    "    s = (s-min_val) / (max_val - min_val)\n",
    "    # m = (m-min_val) / (max_val - min_val)\n",
    "\n",
    "    # Create a new figure for each set of values\n",
    "    out = Output()\n",
    "    with out:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot the mean\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(m, 'bo', markersize=4)\n",
    "        plt.title(f'Mean of Activations - Set {i+1}')\n",
    "        plt.xlabel('Activation Index')\n",
    "        plt.ylabel('Mean Value')\n",
    "        plt.ylim(0, np.max(m))  # Ensure y-axis starts at 0\n",
    "\n",
    "        # Plot the standard deviation\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(s, 'ro', markersize=4)\n",
    "        plt.title(f'Standard Deviation of Activations - Set {i+1}')\n",
    "        plt.xlabel('Activation Index')\n",
    "        plt.ylabel('Standard Deviation')\n",
    "        plt.ylim(0, np.max(s))  # Ensure y-axis starts at 0\n",
    "\n",
    "        # Show the plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "VBox(output_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, v in enumerate(all_fc_vals):\n",
    "    v = np.array(v)\n",
    "    m = np.mean(np.abs(v), axis=0)\n",
    "    s = np.std(v, axis=0)\n",
    "    \n",
    "    min_val = np.min(v, axis=0)\n",
    "    max_val = np.max(v, axis=0)\n",
    "    s = (s-min_val) / (max_val - min_val)\n",
    "    # m = (m-min_val) / (max_val - min_val)\n",
    "    # Create a new figure for each set of values\n",
    "    out = Output()\n",
    "    with out:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Plot the mean with markers\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(768)),\n",
    "            y=m,\n",
    "            mode='markers',\n",
    "            name='Mean',\n",
    "            marker=dict(size=3, color='blue')\n",
    "        ))\n",
    "\n",
    "        # Plot the standard deviation with markers\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(768)),\n",
    "            y=s,\n",
    "            mode='markers',\n",
    "            name='Std Dev',\n",
    "            marker=dict(size=3, color='red')\n",
    "        ))\n",
    "\n",
    "        # Add lines connecting corresponding points\n",
    "        for j in range(768):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[j, j],\n",
    "                y=[m[j], s[j]],\n",
    "                mode='lines',\n",
    "                line=dict(color='gray', width=0.5),\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Mean and Standard Deviation of Activations - Set {i+1}',\n",
    "            xaxis_title='Activation Index',\n",
    "            yaxis_title='Value',\n",
    "            yaxis=dict(range=[0, max(np.max(m), np.max(s)) * 1.1]),\n",
    "            height=600,\n",
    "            width=1000,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "    \n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "# VBox(output_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "import importlib\n",
    "import utilities\n",
    "\n",
    "importlib.reload(utilities)\n",
    "from utilities import compute_masks\n",
    "from IPython.display import display\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, fc1 in enumerate(all_fc_vals):\n",
    "    fc1 = np.array(fc1)\n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_std_high_max = compute_masks(fc1, 0.15)\n",
    "    mask_std = mask_std_high_max\n",
    "    \n",
    "    m = np.mean(np.abs(fc1), axis=0)\n",
    "    s = np.std(fc1, axis=0)\n",
    "    min_val = np.min(fc1, axis=0)\n",
    "    max_val = np.max(fc1, axis=0)\n",
    "    \n",
    "    # Normalize std and mean\n",
    "    s_norm = (s - min_val) / (max_val - min_val)\n",
    "    m_norm = m#(m - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create indices for different masks\n",
    "    indices_max = np.where(mask_max == 0)[0]\n",
    "    indices_std = np.where(mask_std == 0)[0]\n",
    "    indices_intersection = np.intersect1d(indices_max, indices_std)\n",
    "    indices_max_minus_std = np.setdiff1d(indices_max, indices_std)\n",
    "    indices_std_minus_max = np.setdiff1d(indices_std, indices_max)\n",
    "    \n",
    "    # Count the indices in each set\n",
    "    count_max = len(indices_max)\n",
    "    count_std = len(indices_std)\n",
    "    count_intersection = len(indices_intersection)\n",
    "    count_max_minus_std = len(indices_max_minus_std)\n",
    "    count_std_minus_max = len(indices_std_minus_max)\n",
    "    \n",
    "    out = Output()\n",
    "    with out:\n",
    "        # Create subplots with counts in titles\n",
    "        fig = make_subplots(rows=2, cols=3, \n",
    "                            subplot_titles=(f\"Max Mask (Count: {count_max})\", \n",
    "                                            f\"Std Mask (Count: {count_std})\", \n",
    "                                            f\"Intersection (Count: {count_intersection})\",\n",
    "                                            f\"Max - Std (Count: {count_max_minus_std})\", \n",
    "                                            f\"Std - Max (Count: {count_std_minus_max})\"))\n",
    "        \n",
    "        # Helper function to add traces\n",
    "        def add_traces(indices, row, col):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices,\n",
    "                    y=m_norm[indices],\n",
    "                    mode='markers',\n",
    "                    name='Mean',\n",
    "                    marker=dict(size=3, color='blue'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices,\n",
    "                    y=s_norm[indices],\n",
    "                    mode='markers',\n",
    "                    name='Std Dev',\n",
    "                    marker=dict(size=3, color='red'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            for j in indices:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[j, j],\n",
    "                        y=[m_norm[j], s_norm[j]],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='gray', width=0.5),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        \n",
    "        # Add traces for all plots\n",
    "        add_traces(indices_max, 1, 1)\n",
    "        add_traces(indices_std, 1, 2)\n",
    "        add_traces(indices_intersection, 1, 3)\n",
    "        add_traces(indices_max_minus_std, 2, 1)\n",
    "        add_traces(indices_std_minus_max, 2, 2)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Mean and Standard Deviation of Activations - Class {i+1}',\n",
    "            height=1200,\n",
    "            width=1800,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update x and y axis labels for all subplots\n",
    "        for row in range(1, 3):\n",
    "            for col in range(1, 4):\n",
    "                if row == 2 and col == 3:\n",
    "                    continue  # Skip the empty subplot\n",
    "                fig.update_xaxes(title_text=\"Activation Index\", row=row, col=col)\n",
    "                fig.update_yaxes(title_text=\"Normalized Value\", row=row, col=col)\n",
    "        \n",
    "        display(fig)\n",
    "    \n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "from utilities import compute_masks\n",
    "from IPython.display import display\n",
    "\n",
    "output_widgets = []\n",
    "\n",
    "for i, fc1 in enumerate(all_fc_vals):\n",
    "    fc1 = np.array(fc1)\n",
    "    mask_max, mask_std = compute_masks(fc1, 0.15)\n",
    "    \n",
    "    m = np.mean(np.abs(fc1), axis=0)\n",
    "    s = np.std(fc1, axis=0)\n",
    "    min_val = np.min(fc1, axis=0)\n",
    "    max_val = np.max(fc1, axis=0)\n",
    "    \n",
    "    # Normalize std and mean\n",
    "    s_norm = (s - min_val) / (max_val - min_val)\n",
    "    m_norm = m#(m - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create indices for different masks\n",
    "    indices_max = np.where(mask_max == 0)[0]\n",
    "    indices_std = np.where(mask_std == 0)[0]\n",
    "    indices_intersection = np.intersect1d(indices_max, indices_std)\n",
    "    indices_max_minus_std = np.setdiff1d(indices_max, indices_std)\n",
    "    indices_std_minus_max = np.setdiff1d(indices_std, indices_max)\n",
    "    \n",
    "    # Count the indices in each set\n",
    "    count_all = len(m_norm)\n",
    "    count_max = len(indices_max)\n",
    "    count_std = len(indices_std)\n",
    "    count_intersection = len(indices_intersection)\n",
    "    count_max_minus_std = len(indices_max_minus_std)\n",
    "    count_std_minus_max = len(indices_std_minus_max)\n",
    "    \n",
    "    out = Output()\n",
    "    with out:\n",
    "        # Create subplots with counts in titles\n",
    "        fig = make_subplots(rows=2, cols=3, \n",
    "                            subplot_titles=(f\"All Activations (Count: {count_all})\",\n",
    "                                            f\"Max Mask (Count: {count_max})\", \n",
    "                                            f\"Std Mask (Count: {count_std})\", \n",
    "                                            f\"Intersection (Count: {count_intersection})\",\n",
    "                                            f\"Max - Std (Count: {count_max_minus_std})\", \n",
    "                                            f\"Std - Max (Count: {count_std_minus_max})\"))\n",
    "        \n",
    "        # Helper function to add traces\n",
    "        def add_traces(indices, row, col):\n",
    "            indices_list = list(indices)  # Convert range or numpy array to list\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices_list,\n",
    "                    y=m_norm[indices_list],\n",
    "                    mode='markers',\n",
    "                    name='Mean',\n",
    "                    marker=dict(size=3, color='blue'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=indices_list,\n",
    "                    y=s_norm[indices_list],\n",
    "                    mode='markers',\n",
    "                    name='Std Dev',\n",
    "                    marker=dict(size=3, color='red'),\n",
    "                    showlegend=(row == 1 and col == 1)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            for j in indices_list:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[j, j],\n",
    "                        y=[m_norm[j], s_norm[j]],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='gray', width=0.5),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        \n",
    "        # Add traces for all activations\n",
    "        add_traces(range(len(m_norm)), 1, 1)\n",
    "        \n",
    "        # Add traces for other plots\n",
    "        add_traces(indices_max, 1, 2)\n",
    "        add_traces(indices_std, 1, 3)\n",
    "        add_traces(indices_intersection, 2, 1)\n",
    "        add_traces(indices_max_minus_std, 2, 2)\n",
    "        add_traces(indices_std_minus_max, 2, 3)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Mean and Standard Deviation of Activations - Class {i+1}',\n",
    "            height=1200,\n",
    "            width=1800,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update x and y axis labels for all subplots\n",
    "        for row in range(1, 3):\n",
    "            for col in range(1, 4):\n",
    "                fig.update_xaxes(title_text=\"Activation Index\", row=row, col=col)\n",
    "                fig.update_yaxes(title_text=\"Normalized Value\", row=row, col=col)\n",
    "        \n",
    "        display(fig)\n",
    "    \n",
    "    output_widgets.append(out)\n",
    "\n",
    "# Display all figures in a vertical box\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from ipywidgets import VBox, Output\n",
    "from utilities import compute_masks\n",
    "from IPython.display import display\n",
    "\n",
    "def create_index_tracking_plot(indices_per_class, title):\n",
    "    num_classes = len(indices_per_class)\n",
    "    all_indices = sorted(set.union(*[set(indices) for indices in indices_per_class]))\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Create a color scale\n",
    "    color_scale = px.colors.diverging.RdYlGn_r  # Red to Yellow to Green color scale\n",
    "\n",
    "    # Add edges for indices present in multiple classes\n",
    "    for idx in all_indices:\n",
    "        classes_with_idx = [i for i, indices in enumerate(indices_per_class) if idx in indices]\n",
    "        if len(classes_with_idx) > 1:\n",
    "            x = [idx] * len(classes_with_idx)\n",
    "            y = classes_with_idx\n",
    "            color_index = (len(classes_with_idx) - 1) / (num_classes - 1)  # Normalize to [0, 1]\n",
    "            edge_color = px.colors.sample_colorscale(color_scale, [color_index])[0]\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode='lines',\n",
    "                line=dict(color=edge_color, width=2),\n",
    "                hoverinfo='text',\n",
    "                hovertext=f'Index: {idx}<br>Present in {len(classes_with_idx)} classes',\n",
    "                showlegend=False\n",
    "            ))\n",
    "    \n",
    "    # Add scatter plots for each class\n",
    "    for class_idx, indices in enumerate(indices_per_class):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=indices,\n",
    "            y=[class_idx] * len(indices),\n",
    "            mode='markers',\n",
    "            name=f'Class {class_idx + 1}',\n",
    "            marker=dict(size=4, symbol='circle', color='black'),\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f'Index: {idx}<br>Class: {class_idx + 1}' for idx in indices]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='Activation Index',\n",
    "        yaxis_title='Class',\n",
    "        yaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=list(range(num_classes)),\n",
    "            ticktext=[f'Class {i+1}' for i in range(num_classes)]\n",
    "        ),\n",
    "        hovermode='closest',\n",
    "        width=1500,\n",
    "        height=800,\n",
    "        plot_bgcolor='white',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    \n",
    "    # Add color bar\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            colorscale=color_scale,\n",
    "            showscale=True,\n",
    "            cmin=1,\n",
    "            cmax=num_classes,\n",
    "            colorbar=dict(\n",
    "                title='Number of Classes',\n",
    "                tickvals=list(range(1, num_classes+1)),\n",
    "                ticktext=list(range(1, num_classes+1))\n",
    "            )\n",
    "        ),\n",
    "        hoverinfo='none',\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Collect indices for each class\n",
    "max_indices_per_class = []\n",
    "std_indices_per_class = []\n",
    "\n",
    "for fc1 in all_fc_vals:\n",
    "    mask_max, mask_std = compute_masks(fc1, 0.15)\n",
    "    max_indices_per_class.append(np.where(mask_max == 0)[0])\n",
    "    std_indices_per_class.append(np.where(mask_std == 0)[0])\n",
    "\n",
    "# Create and display visualizations\n",
    "output_widgets = []\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "    fig_max = create_index_tracking_plot(max_indices_per_class, 'Max Mask Indices Across Classes')\n",
    "    display(fig_max)\n",
    "output_widgets.append(out)\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "    fig_std = create_index_tracking_plot(std_indices_per_class, 'Std Mask Indices Across Classes')\n",
    "    display(fig_std)\n",
    "output_widgets.append(out)\n",
    "\n",
    "# Display all visualizations\n",
    "# display(VBox(output_widgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "# from model_distill_bert import getmodel\n",
    "from utilities import compute_accuracy, compute_masks, mask_distillbert, get_model_distilbert, record_activations\n",
    "\n",
    "batch_size = 256\n",
    "mask_layer = 5\n",
    "text_tag = \"text\"\n",
    "compliment = True\n",
    "results_table = PrettyTable()\n",
    "if(compliment):\n",
    "   results_table.field_names = results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"Base Complement Acc\", \"Base Compliment Conf\", \"STD Accuracy\", \"STD Confidence\", \"STD compliment ACC\", \"STD compliment Conf\", \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\", \"Total Masked\", \"Intersedction\"]#, \"Same as Max\"]#\"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"\n",
    "# results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"STD Accuracy\", \"STD Confidence\", \"Same as Max\"]#, \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"]\n",
    "\n",
    "class_labels = []\n",
    "base_accuracies = []\n",
    "base_confidences = []\n",
    "base_comp_acc = []\n",
    "base_comp_conf = []\n",
    "std_masked_counts = []\n",
    "std_accuracies = []\n",
    "std_confidences = []\n",
    "std_comp_acc = []\n",
    "std_comp_conf = []\n",
    "max_masked_counts = []\n",
    "max_accuracies = []\n",
    "max_confidences = []\n",
    "max_comp_acc = []\n",
    "max_comp_conf = []\n",
    "diff_from_max = []\n",
    "total_masked = []\n",
    "\n",
    "dataset_list = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"2O24dpower2024/distilbert-base-uncased-finetuned-emotion\")\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset_all = load_dataset(\"dair-ai/emotion\")\n",
    "dataset_all = dataset_all['train']\n",
    "\n",
    "for j in range(0,7):\n",
    "    # model = get_model_distilbert(\"esuriddick/distilbert-base-uncased-finetuned-emotion\", mask_layer)\n",
    "    \n",
    "    model = get_model_distilbert(\"2O24dpower2024/distilbert-base-uncased-finetuned-emotion\", mask_layer)\n",
    "    dataset = dataset_all.filter(lambda x: x['label'] in [j])\n",
    "    dataset_complement = dataset_all.filter(lambda x: x['label'] not in [j])\n",
    "    \n",
    "    if(j==6):\n",
    "        dataset = dataset_all\n",
    "\n",
    "    class_labels.append(f\"Class {j}\")\n",
    "    acc = compute_accuracy(dataset, model, tokenizer, text_tag, batch_size=batch_size)\n",
    "    dataset_list.append(acc[2])\n",
    "    print(\"Class \",j, \"base accuracy: \", acc[0], acc[1])\n",
    "    base_accuracies.append(acc[0])\n",
    "    base_confidences.append(acc[1])\n",
    "    aug_dataset = acc[2]\n",
    "    if(compliment):\n",
    "        acc = compute_accuracy(dataset_complement, model, tokenizer, text_tag , batch_size=batch_size)\n",
    "        print(\"Class \",j, \"complement base accuracy: \", acc[0], acc[1])\n",
    "        base_comp_acc.append(acc[0])\n",
    "        base_comp_conf.append(acc[1])\n",
    "        aug_dataset.extend(acc[2])\n",
    "        \n",
    "\n",
    "    #record the activations of the first fully connected layer, CLS tokken\n",
    "    print(\"Recording activations...\")\n",
    "    # progress_bar = tqdm(total=len(dataset))\n",
    "    # model.to(device)\n",
    "    # model.eval()\n",
    "    # fc_vals = []\n",
    "    # with torch.no_grad():\n",
    "    #     for i in range(len(dataset)):\n",
    "    #         text = dataset[i]['sentence']\n",
    "    #         inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    #         outputs = model(**inputs)\n",
    "    #         fc_vals.append(outputs[1][mask_layer+1][:, 0].squeeze().cpu().numpy())\n",
    "    #         progress_bar.update(1)\n",
    "    #     progress_bar.close()\n",
    "\n",
    "    fc_vals = record_activations(dataset, model, tokenizer, text_tag=text_tag, mask_layer=mask_layer, batch_size=batch_size)\n",
    "\n",
    "        \n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max = compute_masks(fc_vals,0.50)\n",
    "    mask_std = mask_std_high_max\n",
    "    print(\"Masking STD...\")\n",
    "    model = mask_distillbert(model,mask_std)\n",
    "    t = int(mask_std.shape[0]-torch.count_nonzero(mask_std))\n",
    "    print(\"Total Masked :\", t)\n",
    "    total_masked.append(t)\n",
    "    diff_from_max.append(int((torch.logical_or(mask_std, mask_max) == 0).sum().item()))\n",
    "    acc = compute_accuracy(dataset, model, tokenizer, text_tag, batch_size=batch_size, in_aug_dataset=aug_dataset[:len(dataset)]) \n",
    "    dataset_list.append(acc[2])\n",
    "    print(\"accuracy after masking STD: \", acc[0], acc[1])\n",
    "    std_accuracies.append(acc[0])\n",
    "    std_confidences.append(acc[1])\n",
    "    if(compliment):\n",
    "        acc = compute_accuracy(dataset_complement, model, tokenizer, text_tag, batch_size=batch_size, in_aug_dataset=aug_dataset[len(dataset):])\n",
    "        print(\"accuracy after masking STD on complement: \", acc[0], acc[1])\n",
    "        std_comp_acc.append(acc[0])\n",
    "        std_comp_conf.append(acc[1])\n",
    "\n",
    "    print(\"Masking MAX...\")\n",
    "    model = mask_distillbert(model,mask_max)\n",
    "    t = int(mask_max.shape[0]-torch.count_nonzero(mask_max))\n",
    "    print(\"Total Masked :\", t)\n",
    "    # total_masked.append(t)\n",
    "    acc = compute_accuracy(dataset, model, tokenizer, text_tag, batch_size=batch_size, in_aug_dataset=aug_dataset[:len(dataset)])\n",
    "    dataset_list.append(acc[2])\n",
    "    print(\"accuracy after masking MAX: \", acc[0], acc[1])\n",
    "    max_accuracies.append(acc[0])\n",
    "    max_confidences.append(acc[1])\n",
    "    acc = compute_accuracy(dataset_complement, model, tokenizer, text_tag, batch_size=batch_size, in_aug_dataset=aug_dataset[len(dataset):])\n",
    "    print(\"accuracy after masking MAX on complement: \", acc[0], acc[1])\n",
    "    max_comp_acc.append(acc[0])\n",
    "    max_comp_conf.append(acc[1])\n",
    "    if(compliment):\n",
    "        results_table.add_row([\n",
    "            class_labels[j],\n",
    "            base_accuracies[j],\n",
    "            base_confidences[j],\n",
    "            base_comp_acc[j],\n",
    "            base_comp_conf[j],\n",
    "            std_accuracies[j],\n",
    "            std_confidences[j],\n",
    "            std_comp_acc[j],\n",
    "            std_comp_conf[j],\n",
    "            max_accuracies[j],\n",
    "            max_confidences[j],\n",
    "            max_comp_acc[j],\n",
    "            max_comp_conf[j],\n",
    "            total_masked[j],\n",
    "            diff_from_max[j]\n",
    "        ])\n",
    "    # results_table.add_row([\n",
    "    #     class_labels[j],\n",
    "    #     base_accuracies[j],\n",
    "    #     base_confidences[j],\n",
    "    #     std_accuracies[j],\n",
    "    #     std_confidences[j],\n",
    "    #     # max_accuracies[j],\n",
    "    #     # max_confidences[j],\n",
    "    #     diff_from_max[j]\n",
    "    # ])\n",
    "\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Assuming you already have your DataFrame\n",
    "# df = pd.DataFrame(dataset_list[1])\n",
    "\n",
    "def display_df(dataframe, rows_per_page=10):\n",
    "    # Convert the dataframe to HTML\n",
    "    html = dataframe.to_html(classes='table table-striped')\n",
    "    \n",
    "    # Add Bootstrap CSS\n",
    "    html = f\"\"\"\n",
    "    <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\">\n",
    "    <div class=\"container\">\n",
    "        {html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add pagination\n",
    "    total_rows = len(dataframe)\n",
    "    total_pages = (total_rows - 1) // rows_per_page + 1\n",
    "    \n",
    "    pagination_html = f\"\"\"\n",
    "    <nav>\n",
    "        <ul class=\"pagination justify-content-center\">\n",
    "            <li class=\"page-item\"><a class=\"page-link\" href=\"#\" id=\"prev-page\">Previous</a></li>\n",
    "            <li class=\"page-item\"><span class=\"page-link\" id=\"current-page\">1 / {total_pages}</span></li>\n",
    "            <li class=\"page-item\"><a class=\"page-link\" href=\"#\" id=\"next-page\">Next</a></li>\n",
    "        </ul>\n",
    "    </nav>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add JavaScript for pagination functionality\n",
    "    js = f\"\"\"\n",
    "    <script>\n",
    "        var currentPage = 1;\n",
    "        var rowsPerPage = {rows_per_page};\n",
    "        var totalPages = {total_pages};\n",
    "        \n",
    "        function showPage(page) {{\n",
    "            var rows = document.querySelectorAll('table.table tbody tr');\n",
    "            for (var i = 0; i < rows.length; i++) {{\n",
    "                if (i >= (page - 1) * rowsPerPage && i < page * rowsPerPage) {{\n",
    "                    rows[i].style.display = '';\n",
    "                }} else {{\n",
    "                    rows[i].style.display = 'none';\n",
    "                }}\n",
    "            }}\n",
    "            document.getElementById('current-page').textContent = page + ' / ' + totalPages;\n",
    "        }}\n",
    "        \n",
    "        document.getElementById('prev-page').addEventListener('click', function(e) {{\n",
    "            e.preventDefault();\n",
    "            if (currentPage > 1) {{\n",
    "                currentPage--;\n",
    "                showPage(currentPage);\n",
    "            }}\n",
    "        }});\n",
    "        \n",
    "        document.getElementById('next-page').addEventListener('click', function(e) {{\n",
    "            e.preventDefault();\n",
    "            if (currentPage < totalPages) {{\n",
    "                currentPage++;\n",
    "                showPage(currentPage);\n",
    "            }}\n",
    "        }});\n",
    "        \n",
    "        showPage(1);\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine all HTML and JavaScript\n",
    "    full_html = html + pagination_html + js\n",
    "    \n",
    "    # Display the result\n",
    "    display(HTML(full_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.iloc[::-1]\n",
    "display_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"esuriddick/distilbert-base-uncased-finetuned-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 & .867 & .685 & .868 & .639 & .733 & .016 & .868 & .460 & .567 & .009 & .865 & .113 \\\\\n",
      "Class 1 & 1.000 & .808 & .868 & .638 & .700 & .009 & .863 & .424 & .633 & .009 & .863 & .105 \\\\\n",
      "Class 2 & .900 & .589 & .868 & .640 & .533 & .014 & .867 & .415 & .533 & .011 & .860 & .104 \\\\\n",
      "Class 3 & 1.000 & .749 & .868 & .639 & .700 & .017 & .869 & .498 & .700 & .011 & .872 & .108 \\\\\n",
      "Class 4 & .967 & .754 & .868 & .639 & .700 & .014 & .869 & .490 & .567 & .010 & .868 & .114 \\\\\n",
      "Class 5 & 1.000 & .860 & .868 & .638 & 1.000 & .010 & .871 & .465 & 1.000 & .010 & .869 & .113 \\\\\n",
      "Class 6 & .700 & .529 & .869 & .640 & .500 & .014 & .872 & .441 & .467 & .009 & .873 & .110 \\\\\n",
      "Class 7 & .967 & .771 & .868 & .639 & .933 & .017 & .869 & .477 & .767 & .010 & .870 & .106 \\\\\n",
      "Class 8 & .867 & .662 & .868 & .639 & .633 & .012 & .864 & .450 & .633 & .010 & .870 & .111 \\\\\n",
      "Class 9 & .900 & .738 & .868 & .639 & .467 & .011 & .867 & .462 & .433 & .009 & .867 & .106 \\\\\n",
      "Class 10 & 1.000 & .703 & .868 & .639 & .167 & .010 & .873 & .361 & .133 & .008 & .868 & .108 \\\\\n",
      "Class 11 & .967 & .554 & .868 & .640 & .300 & .012 & .862 & .424 & .200 & .009 & .861 & .108 \\\\\n",
      "Class 12 & .867 & .634 & .868 & .639 & .667 & .012 & .868 & .458 & .633 & .009 & .867 & .112 \\\\\n",
      "Class 13 & 1.000 & .694 & .868 & .639 & .667 & .013 & .865 & .415 & .667 & .010 & .865 & .105 \\\\\n",
      "Class 14 & .967 & .776 & .868 & .639 & .800 & .009 & .868 & .434 & .800 & .009 & .868 & .111 \\\\\n",
      "Class 15 & 1.000 & .736 & .868 & .639 & .867 & .012 & .864 & .431 & .800 & .010 & .868 & .110 \\\\\n",
      "Class 16 & .867 & .538 & .868 & .640 & .167 & .011 & .869 & .416 & .133 & .009 & .864 & .102 \\\\\n",
      "Class 17 & 1.000 & .824 & .868 & .638 & 1.000 & .011 & .863 & .425 & .967 & .010 & .860 & .108 \\\\\n",
      "Class 18 & 1.000 & .794 & .868 & .638 & .900 & .012 & .867 & .466 & .733 & .010 & .867 & .108 \\\\\n",
      "Class 19 & .967 & .722 & .868 & .639 & .200 & .011 & .865 & .439 & .100 & .009 & .862 & .108 \\\\\n",
      "Class 20 & 1.000 & .673 & .868 & .639 & .700 & .013 & .869 & .442 & .600 & .010 & .866 & .103 \\\\\n",
      "Class 21 & 1.000 & .738 & .868 & .639 & .633 & .013 & .862 & .443 & .500 & .010 & .864 & .106 \\\\\n",
      "Class 22 & .900 & .764 & .868 & .639 & .833 & .010 & .865 & .445 & .833 & .009 & .865 & .108 \\\\\n",
      "Class 23 & .967 & .640 & .868 & .639 & .600 & .014 & .870 & .474 & .567 & .009 & .867 & .111 \\\\\n",
      "Class 24 & 1.000 & .816 & .868 & .638 & .867 & .011 & .871 & .484 & .767 & .009 & .870 & .107 \\\\\n",
      "Class 25 & .967 & .753 & .868 & .639 & .867 & .012 & .870 & .448 & .700 & .010 & .871 & .110 \\\\\n",
      "Class 26 & .933 & .746 & .868 & .639 & .900 & .012 & .870 & .515 & .900 & .010 & .869 & .110 \\\\\n",
      "Class 27 & .833 & .567 & .869 & .640 & .267 & .012 & .873 & .411 & .267 & .009 & .871 & .113 \\\\\n",
      "Class 28 & .933 & .668 & .868 & .639 & .600 & .013 & .868 & .453 & .467 & .009 & .865 & .110 \\\\\n",
      "Class 29 & 1.000 & .793 & .868 & .638 & .733 & .013 & .865 & .462 & .567 & .009 & .867 & .110 \\\\\n",
      "Class 30 & 1.000 & .660 & .868 & .639 & .533 & .011 & .870 & .434 & .567 & .009 & .866 & .109 \\\\\n",
      "Class 31 & .967 & .778 & .868 & .639 & .900 & .013 & .867 & .474 & .800 & .009 & .866 & .112 \\\\\n",
      "Class 32 & .967 & .811 & .868 & .638 & .900 & .010 & .866 & .460 & .933 & .010 & .868 & .107 \\\\\n",
      "Class 33 & 1.000 & .850 & .868 & .638 & 1.000 & .011 & .870 & .495 & .967 & .010 & .866 & .108 \\\\\n",
      "Class 34 & 1.000 & .804 & .868 & .638 & 1.000 & .011 & .863 & .468 & .967 & .009 & .857 & .108 \\\\\n",
      "Class 35 & .967 & .725 & .868 & .639 & .667 & .012 & .866 & .418 & .600 & .010 & .870 & .109 \\\\\n",
      "Class 36 & 1.000 & .821 & .868 & .638 & 1.000 & .010 & .866 & .475 & 1.000 & .010 & .870 & .110 \\\\\n",
      "Class 37 & 1.000 & .770 & .868 & .639 & .867 & .011 & .863 & .408 & .733 & .010 & .867 & .110 \\\\\n",
      "Class 38 & 1.000 & .832 & .868 & .638 & .733 & .014 & .865 & .476 & .467 & .009 & .862 & .110 \\\\\n",
      "Class 39 & 1.000 & .829 & .868 & .638 & .967 & .011 & .869 & .518 & .967 & .009 & .866 & .107 \\\\\n",
      "Class 40 & 1.000 & .785 & .868 & .638 & .900 & .015 & .871 & .458 & .700 & .010 & .864 & .110 \\\\\n",
      "Class 41 & .867 & .600 & .868 & .640 & .500 & .012 & .866 & .421 & .500 & .009 & .862 & .107 \\\\\n",
      "Class 42 & .514 & .290 & .947 & .717 & .036 & .006 & .947 & .326 & .037 & .007 & .948 & .123 \\\\\n",
      "Class 43 & .833 & .568 & .869 & .640 & .500 & .013 & .864 & .420 & .467 & .009 & .868 & .105 \\\\\n",
      "Class 44 & .933 & .729 & .868 & .639 & .800 & .014 & .868 & .459 & .733 & .010 & .867 & .104 \\\\\n",
      "Class 45 & 1.000 & .854 & .868 & .638 & 1.000 & .014 & .868 & .563 & 1.000 & .010 & .868 & .108 \\\\\n",
      "Class 46 & .800 & .603 & .869 & .639 & .333 & .011 & .872 & .422 & .300 & .009 & .865 & .108 \\\\\n",
      "Class 47 & .967 & .785 & .868 & .638 & .833 & .013 & .866 & .475 & .700 & .010 & .863 & .104 \\\\\n",
      "Class 48 & .933 & .662 & .868 & .639 & .600 & .011 & .866 & .383 & .600 & .009 & .868 & .106 \\\\\n",
      "Class 49 & .800 & .616 & .869 & .639 & .467 & .014 & .869 & .422 & .433 & .009 & .870 & .105 \\\\\n",
      "Class 50 & 1.000 & .746 & .868 & .639 & .467 & .010 & .867 & .447 & .467 & .009 & .870 & .110 \\\\\n",
      "Class 51 & 1.000 & .790 & .868 & .638 & .900 & .017 & .867 & .493 & .667 & .010 & .869 & .110 \\\\\n",
      "Class 52 & 1.000 & .830 & .868 & .638 & 1.000 & .015 & .864 & .467 & .833 & .010 & .867 & .106 \\\\\n",
      "Class 53 & .867 & .647 & .868 & .639 & .567 & .011 & .870 & .438 & .567 & .009 & .877 & .111 \\\\\n",
      "Class 54 & .900 & .607 & .868 & .639 & .367 & .012 & .869 & .378 & .333 & .010 & .866 & .108 \\\\\n",
      "Class 55 & .967 & .617 & .868 & .639 & .267 & .009 & .866 & .402 & .267 & .009 & .868 & .112 \\\\\n",
      "Class 56 & .900 & .648 & .868 & .639 & .567 & .012 & .874 & .414 & .533 & .010 & .868 & .107 \\\\\n",
      "Class 57 & .900 & .592 & .868 & .640 & .533 & .014 & .864 & .394 & .367 & .009 & .869 & .106 \\\\\n",
      "Class 58 & .933 & .716 & .868 & .639 & .833 & .011 & .869 & .456 & .700 & .009 & .867 & .109 \\\\\n",
      "Class 59 & 1.000 & .871 & .868 & .638 & 1.000 & .014 & .864 & .510 & .833 & .009 & .866 & .109 \\\\\n",
      "Class 60 & .800 & .594 & .869 & .640 & .400 & .012 & .872 & .421 & .300 & .009 & .868 & .110 \\\\\n",
      "Class 61 & 1.000 & .729 & .868 & .639 & .400 & .015 & .862 & .425 & .200 & .009 & .866 & .109 \\\\\n",
      "Class 62 & .900 & .742 & .868 & .639 & .900 & .012 & .865 & .505 & .867 & .009 & .862 & .108 \\\\\n",
      "Class 63 & .933 & .736 & .868 & .639 & .767 & .022 & .869 & .515 & .633 & .010 & .865 & .110 \\\\\n",
      "Class 64 & .900 & .720 & .868 & .639 & .867 & .013 & .871 & .493 & .800 & .009 & .873 & .110 \\\\\n",
      "Class 65 & 1.000 & .703 & .868 & .639 & .500 & .014 & .870 & .440 & .467 & .010 & .867 & .109 \\\\\n",
      "Class 66 & .967 & .772 & .868 & .639 & .800 & .021 & .867 & .538 & .767 & .011 & .869 & .111 \\\\\n",
      "Class 67 & 1.000 & .715 & .868 & .639 & .733 & .010 & .868 & .414 & .700 & .009 & .869 & .109 \\\\\n",
      "Class 68 & .800 & .584 & .869 & .640 & .433 & .009 & .870 & .384 & .433 & .009 & .869 & .109 \\\\\n",
      "Class 69 & .967 & .765 & .868 & .639 & .833 & .014 & .866 & .493 & .733 & .010 & .864 & .105 \\\\\n",
      "Class 70 & .733 & .550 & .869 & .640 & .467 & .013 & .868 & .417 & .400 & .009 & .864 & .103 \\\\\n",
      "Class 71 & .867 & .584 & .868 & .640 & .333 & .010 & .866 & .402 & .333 & .008 & .868 & .108 \\\\\n",
      "Class 72 & 1.000 & .771 & .868 & .639 & .833 & .014 & .867 & .432 & .633 & .010 & .870 & .111 \\\\\n",
      "Class 73 & 1.000 & .779 & .868 & .639 & .867 & .011 & .866 & .464 & .733 & .009 & .867 & .110 \\\\\n",
      "Class 74 & 1.000 & .798 & .868 & .638 & .700 & .011 & .862 & .427 & .667 & .009 & .862 & .105 \\\\\n",
      "Class 75 & .867 & .478 & .868 & .640 & .500 & .010 & .866 & .393 & .500 & .010 & .868 & .110 \\\\\n",
      "Class 76 & 1.000 & .774 & .868 & .639 & .800 & .013 & .865 & .437 & .500 & .009 & .863 & .112 \\\\\n",
      "Class 77 & .967 & .660 & .868 & .639 & .567 & .013 & .865 & .410 & .433 & .010 & .868 & .103 \\\\\n",
      "Class 78 & .900 & .657 & .868 & .639 & .567 & .015 & .868 & .416 & .333 & .010 & .868 & .110 \\\\\n",
      "Class 79 & .867 & .655 & .868 & .639 & .600 & .015 & .869 & .446 & .533 & .010 & .867 & .110 \\\\\n",
      "Class 80 & 1.000 & .792 & .868 & .638 & .833 & .015 & .868 & .463 & .667 & .010 & .867 & .109 \\\\\n",
      "Class 81 & 1.000 & .836 & .868 & .638 & .833 & .011 & .868 & .471 & .800 & .010 & .866 & .111 \\\\\n",
      "Class 82 & .967 & .697 & .868 & .639 & .733 & .011 & .869 & .363 & .733 & .010 & .869 & .108 \\\\\n",
      "Class 83 & .967 & .796 & .868 & .638 & .833 & .010 & .871 & .444 & .833 & .009 & .869 & .105 \\\\\n",
      "Class 84 & .967 & .728 & .868 & .639 & .767 & .014 & .869 & .453 & .600 & .010 & .868 & .103 \\\\\n",
      "Class 85 & 1.000 & .728 & .868 & .639 & .767 & .011 & .871 & .465 & .700 & .010 & .867 & .112 \\\\\n",
      "Class 86 & .967 & .720 & .868 & .639 & .700 & .013 & .867 & .436 & .633 & .010 & .867 & .111 \\\\\n",
      "Class 87 & 1.000 & .658 & .868 & .639 & .333 & .011 & .874 & .416 & .333 & .009 & .870 & .112 \\\\\n",
      "Class 88 & 1.000 & .846 & .868 & .638 & 1.000 & .020 & .868 & .508 & .867 & .010 & .875 & .110 \\\\\n",
      "Class 89 & .800 & .684 & .869 & .639 & .800 & .011 & .867 & .518 & .767 & .009 & .867 & .107 \\\\\n",
      "Class 90 & .900 & .610 & .868 & .639 & .233 & .010 & .870 & .378 & .233 & .009 & .869 & .110 \\\\\n",
      "Class 91 & 1.000 & .692 & .868 & .639 & .433 & .016 & .865 & .449 & .300 & .010 & .863 & .105 \\\\\n",
      "Class 92 & 1.000 & .868 & .868 & .638 & 1.000 & .012 & .868 & .513 & .933 & .010 & .871 & .111 \\\\\n",
      "Class 93 & .967 & .693 & .868 & .639 & .533 & .011 & .862 & .385 & .367 & .009 & .863 & .109 \\\\\n",
      "Class 94 & .933 & .808 & .868 & .638 & .933 & .013 & .867 & .485 & .933 & .009 & .864 & .106 \\\\\n",
      "Class 95 & .967 & .788 & .868 & .638 & .900 & .011 & .866 & .444 & .767 & .009 & .866 & .107 \\\\\n",
      "Class 96 & 1.000 & .758 & .868 & .639 & .633 & .011 & .859 & .387 & .633 & .010 & .861 & .104 \\\\\n",
      "Class 97 & .900 & .671 & .868 & .639 & .500 & .016 & .870 & .450 & .267 & .009 & .863 & .106 \\\\\n",
      "Class 98 & 1.000 & .830 & .868 & .638 & .900 & .011 & .868 & .479 & .900 & .010 & .872 & .108 \\\\\n",
      "Class 99 & .967 & .715 & .868 & .639 & .700 & .011 & .865 & .418 & .633 & .009 & .868 & .109 \\\\\n",
      "Class 100 & 1.000 & .875 & .868 & .638 & 1.000 & .015 & .867 & .528 & .967 & .010 & .865 & .104 \\\\\n",
      "Class 101 & .933 & .700 & .868 & .639 & .567 & .013 & .867 & .434 & .533 & .009 & .862 & .108 \\\\\n",
      "Class 102 & .867 & .675 & .868 & .639 & .433 & .011 & .870 & .416 & .300 & .009 & .870 & .113 \\\\\n",
      "Class 103 & 1.000 & .811 & .868 & .638 & .867 & .010 & .867 & .413 & .833 & .010 & .862 & .111 \\\\\n",
      "Class 104 & .967 & .644 & .868 & .639 & .567 & .013 & .868 & .436 & .567 & .010 & .871 & .108 \\\\\n",
      "Class 105 & .967 & .663 & .868 & .639 & .700 & .012 & .874 & .428 & .633 & .010 & .870 & .113 \\\\\n",
      "Class 106 & .967 & .646 & .868 & .639 & .367 & .017 & .866 & .419 & .167 & .010 & .862 & .104 \\\\\n",
      "Class 107 & .800 & .546 & .869 & .640 & .367 & .013 & .874 & .451 & .333 & .009 & .872 & .111 \\\\\n",
      "Class 108 & .967 & .770 & .868 & .639 & .867 & .009 & .868 & .457 & .767 & .009 & .872 & .109 \\\\\n",
      "Class 109 & .967 & .730 & .868 & .639 & .733 & .011 & .869 & .456 & .733 & .009 & .868 & .107 \\\\\n",
      "Class 110 & .933 & .665 & .868 & .639 & .333 & .010 & .871 & .431 & .267 & .009 & .870 & .106 \\\\\n",
      "Class 111 & .933 & .654 & .868 & .639 & .733 & .014 & .870 & .467 & .633 & .009 & .874 & .111 \\\\\n",
      "Class 112 & .867 & .547 & .868 & .640 & .400 & .012 & .866 & .414 & .400 & .009 & .871 & .111 \\\\\n",
      "Class 113 & .967 & .806 & .868 & .638 & .967 & .010 & .865 & .507 & .967 & .009 & .866 & .109 \\\\\n",
      "Class 114 & .900 & .633 & .868 & .639 & .333 & .011 & .873 & .399 & .333 & .009 & .868 & .109 \\\\\n",
      "Class 115 & .933 & .626 & .868 & .639 & .267 & .012 & .862 & .379 & .233 & .010 & .866 & .106 \\\\\n",
      "Class 116 & 1.000 & .800 & .868 & .638 & 1.000 & .009 & .867 & .431 & .900 & .009 & .866 & .113 \\\\\n",
      "Class 117 & .967 & .743 & .868 & .639 & .700 & .011 & .864 & .432 & .467 & .009 & .870 & .108 \\\\\n",
      "Class 118 & 1.000 & .726 & .868 & .639 & .700 & .014 & .865 & .451 & .700 & .009 & .862 & .100 \\\\\n",
      "Class 119 & 1.000 & .800 & .868 & .638 & .800 & .013 & .864 & .442 & .633 & .010 & .864 & .109 \\\\\n",
      "Class 120 & .967 & .795 & .868 & .638 & .933 & .014 & .863 & .476 & .800 & .009 & .864 & .108 \\\\\n",
      "Class 121 & .933 & .660 & .868 & .639 & .333 & .010 & .866 & .366 & .367 & .009 & .868 & .111 \\\\\n",
      "Class 122 & 1.000 & .794 & .868 & .638 & .967 & .012 & .865 & .445 & .800 & .009 & .863 & .106 \\\\\n",
      "Class 123 & .933 & .720 & .868 & .639 & .433 & .012 & .868 & .436 & .400 & .009 & .867 & .108 \\\\\n",
      "Class 124 & .967 & .720 & .868 & .639 & .800 & .011 & .870 & .424 & .767 & .009 & .868 & .110 \\\\\n",
      "Class 125 & .867 & .599 & .868 & .640 & .567 & .012 & .871 & .443 & .567 & .010 & .869 & .110 \\\\\n",
      "Class 126 & .967 & .572 & .868 & .640 & .500 & .012 & .864 & .429 & .433 & .009 & .860 & .105 \\\\\n",
      "Class 127 & 1.000 & .827 & .868 & .638 & 1.000 & .011 & .868 & .494 & .967 & .010 & .869 & .110 \\\\\n",
      "Class 128 & 1.000 & .807 & .868 & .638 & .833 & .019 & .866 & .476 & .433 & .009 & .869 & .107 \\\\\n",
      "Class 129 & 1.000 & .766 & .868 & .639 & .900 & .015 & .868 & .445 & .667 & .010 & .866 & .112 \\\\\n",
      "Class 130 & 1.000 & .808 & .868 & .638 & .933 & .018 & .869 & .490 & .767 & .010 & .866 & .108 \\\\\n",
      "Class 131 & .900 & .677 & .868 & .639 & .533 & .011 & .873 & .401 & .433 & .009 & .870 & .108 \\\\\n",
      "Class 132 & 1.000 & .767 & .868 & .639 & 1.000 & .015 & .866 & .485 & .733 & .009 & .864 & .110 \\\\\n",
      "Class 133 & .900 & .722 & .868 & .639 & .700 & .012 & .869 & .430 & .633 & .010 & .869 & .106 \\\\\n",
      "Class 134 & .933 & .604 & .868 & .639 & .400 & .016 & .868 & .447 & .367 & .009 & .870 & .112 \\\\\n",
      "Class 135 & 1.000 & .833 & .868 & .638 & 1.000 & .016 & .867 & .528 & .967 & .010 & .868 & .106 \\\\\n",
      "Class 136 & .967 & .695 & .868 & .639 & .467 & .011 & .867 & .425 & .467 & .010 & .867 & .108 \\\\\n",
      "Class 137 & 1.000 & .804 & .868 & .638 & .767 & .013 & .862 & .437 & .633 & .010 & .862 & .106 \\\\\n",
      "Class 138 & .933 & .758 & .868 & .639 & .800 & .011 & .866 & .433 & .833 & .009 & .868 & .108 \\\\\n",
      "Class 139 & .967 & .728 & .868 & .639 & .533 & .013 & .865 & .463 & .467 & .009 & .865 & .112 \\\\\n",
      "Class 140 & 1.000 & .788 & .868 & .638 & .633 & .010 & .870 & .440 & .567 & .009 & .868 & .110 \\\\\n",
      "Class 141 & .967 & .764 & .868 & .639 & .633 & .018 & .869 & .501 & .533 & .009 & .867 & .106 \\\\\n",
      "Class 142 & 1.000 & .822 & .868 & .638 & .900 & .010 & .865 & .447 & .867 & .009 & .858 & .105 \\\\\n",
      "Class 143 & .967 & .718 & .868 & .639 & .567 & .011 & .868 & .441 & .567 & .010 & .869 & .111 \\\\\n",
      "Class 144 & .900 & .545 & .868 & .640 & .333 & .011 & .862 & .407 & .333 & .009 & .863 & .110 \\\\\n",
      "Class 145 & .900 & .650 & .868 & .639 & .300 & .014 & .871 & .445 & .233 & .010 & .865 & .110 \\\\\n",
      "Class 146 & .900 & .726 & .868 & .639 & .867 & .015 & .868 & .478 & .700 & .010 & .864 & .110 \\\\\n",
      "Class 147 & 1.000 & .730 & .868 & .639 & .533 & .012 & .867 & .412 & .433 & .010 & .870 & .107 \\\\\n",
      "Class 148 & .867 & .572 & .868 & .640 & .467 & .010 & .862 & .392 & .467 & .009 & .860 & .103 \\\\\n",
      "Class 149 & .967 & .738 & .868 & .639 & .600 & .011 & .865 & .378 & .600 & .010 & .867 & .112 \\\\\n"
     ]
    }
   ],
   "source": [
    "data = '''Class 0\t0.8667\t0.6855\t0.8684\t0.6391\t0.7333\t0.0161\t0.8675\t0.4598\t0.5667\t0.0092\t0.8645\t0.1126\t384\t199\n",
    "Class 1\t1\t0.808\t0.8676\t0.6384\t0.7\t0.0093\t0.8631\t0.4242\t0.6333\t0.0088\t0.8634\t0.1054\t384\t194\n",
    "Class 2\t0.9\t0.589\t0.8682\t0.6396\t0.5333\t0.0137\t0.8665\t0.4151\t0.5333\t0.0106\t0.8596\t0.1042\t384\t210\n",
    "Class 3\t1\t0.7492\t0.8676\t0.6387\t0.7\t0.0167\t0.8691\t0.4978\t0.7\t0.0109\t0.8717\t0.1076\t384\t184\n",
    "Class 4\t0.9667\t0.754\t0.8678\t0.6387\t0.7\t0.0142\t0.8691\t0.4898\t0.5667\t0.0104\t0.8675\t0.1135\t384\t204\n",
    "Class 5\t1\t0.8596\t0.8676\t0.6381\t1\t0.0097\t0.8709\t0.4649\t1\t0.0097\t0.8691\t0.1131\t384\t177\n",
    "Class 6\t0.7\t0.5288\t0.8693\t0.6399\t0.5\t0.0141\t0.8717\t0.4407\t0.4667\t0.0093\t0.8728\t0.1102\t384\t186\n",
    "Class 7\t0.9667\t0.7714\t0.8678\t0.6386\t0.9333\t0.0173\t0.8693\t0.4771\t0.7667\t0.0097\t0.8697\t0.1061\t384\t187\n",
    "Class 8\t0.8667\t0.6623\t0.8684\t0.6392\t0.6333\t0.0121\t0.864\t0.4495\t0.6333\t0.0097\t0.8697\t0.1107\t384\t191\n",
    "Class 9\t0.9\t0.7383\t0.8682\t0.6388\t0.4667\t0.0115\t0.8671\t0.4616\t0.4333\t0.0093\t0.8669\t0.1064\t384\t195\n",
    "Class 10\t1\t0.7031\t0.8676\t0.639\t0.1667\t0.0097\t0.8726\t0.3615\t0.1333\t0.0084\t0.8678\t0.1083\t384\t203\n",
    "Class 11\t0.9667\t0.5542\t0.8678\t0.6398\t0.3\t0.0124\t0.8616\t0.4243\t0.2\t0.0092\t0.8607\t0.1081\t384\t192\n",
    "Class 12\t0.8667\t0.6339\t0.8684\t0.6394\t0.6667\t0.0117\t0.8682\t0.4583\t0.6333\t0.0091\t0.8667\t0.1123\t384\t198\n",
    "Class 13\t1\t0.694\t0.8676\t0.639\t0.6667\t0.0132\t0.8654\t0.4148\t0.6667\t0.0098\t0.8654\t0.1054\t384\t196\n",
    "Class 14\t0.9667\t0.7763\t0.8678\t0.6386\t0.8\t0.0094\t0.868\t0.4339\t0.8\t0.0091\t0.868\t0.1107\t384\t191\n",
    "Class 15\t1\t0.7357\t0.8676\t0.6388\t0.8667\t0.0122\t0.8644\t0.4311\t0.8\t0.0099\t0.8684\t0.11\t384\t189\n",
    "Class 16\t0.8667\t0.5376\t0.8684\t0.6399\t0.1667\t0.011\t0.8693\t0.416\t0.1333\t0.0085\t0.8636\t0.1023\t384\t194\n",
    "Class 17\t1\t0.8239\t0.8676\t0.6383\t1\t0.0109\t0.8627\t0.425\t0.9667\t0.0102\t0.8603\t0.108\t384\t182\n",
    "Class 18\t1\t0.7937\t0.8676\t0.6385\t0.9\t0.0116\t0.8665\t0.4662\t0.7333\t0.0096\t0.8671\t0.1085\t384\t183\n",
    "Class 19\t0.9667\t0.7218\t0.8678\t0.6389\t0.2\t0.0115\t0.8651\t0.4389\t0.1\t0.0089\t0.8616\t0.1081\t384\t204\n",
    "Class 20\t1\t0.6735\t0.8676\t0.6392\t0.7\t0.0129\t0.8687\t0.4422\t0.6\t0.01\t0.8656\t0.1034\t384\t196\n",
    "Class 21\t1\t0.7379\t0.8676\t0.6388\t0.6333\t0.0128\t0.862\t0.4434\t0.5\t0.01\t0.8644\t0.1059\t384\t203\n",
    "Class 22\t0.9\t0.7642\t0.8682\t0.6387\t0.8333\t0.0104\t0.8654\t0.4449\t0.8333\t0.0088\t0.8653\t0.108\t384\t193\n",
    "Class 23\t0.9667\t0.6396\t0.8678\t0.6393\t0.6\t0.0139\t0.8702\t0.4739\t0.5667\t0.0091\t0.8671\t0.1112\t384\t203\n",
    "Class 24\t1\t0.8164\t0.8676\t0.6384\t0.8667\t0.0108\t0.8709\t0.4844\t0.7667\t0.0095\t0.8697\t0.1068\t384\t202\n",
    "Class 25\t0.9667\t0.753\t0.8678\t0.6387\t0.8667\t0.0122\t0.8695\t0.4475\t0.7\t0.0098\t0.8709\t0.1104\t384\t190\n",
    "Class 26\t0.9333\t0.7457\t0.868\t0.6388\t0.9\t0.0122\t0.8697\t0.5148\t0.9\t0.0099\t0.8691\t0.1098\t384\t183\n",
    "Class 27\t0.8333\t0.5667\t0.8686\t0.6397\t0.2667\t0.0121\t0.8733\t0.4111\t0.2667\t0.0087\t0.8709\t0.1127\t384\t210\n",
    "Class 28\t0.9333\t0.668\t0.868\t0.6392\t0.6\t0.0131\t0.8676\t0.453\t0.4667\t0.0095\t0.8651\t0.1098\t384\t190\n",
    "Class 29\t1\t0.7926\t0.8676\t0.6385\t0.7333\t0.0127\t0.8647\t0.4622\t0.5667\t0.0095\t0.8665\t0.1098\t384\t197\n",
    "Class 30\t1\t0.6601\t0.8676\t0.6392\t0.5333\t0.0105\t0.8698\t0.4343\t0.5667\t0.0094\t0.866\t0.1087\t384\t206\n",
    "Class 31\t0.9667\t0.7784\t0.8678\t0.6386\t0.9\t0.0126\t0.8667\t0.4741\t0.8\t0.0094\t0.866\t0.1118\t384\t209\n",
    "Class 32\t0.9667\t0.8114\t0.8678\t0.6384\t0.9\t0.0104\t0.8662\t0.4603\t0.9333\t0.0096\t0.8676\t0.1073\t384\t206\n",
    "Class 33\t1\t0.8504\t0.8676\t0.6382\t1\t0.0109\t0.8698\t0.4952\t0.9667\t0.01\t0.8658\t0.1079\t384\t176\n",
    "Class 34\t1\t0.8042\t0.8676\t0.6384\t1\t0.0106\t0.8625\t0.4679\t0.9667\t0.0095\t0.8574\t0.1077\t384\t200\n",
    "Class 35\t0.9667\t0.7253\t0.8678\t0.6389\t0.6667\t0.0118\t0.8656\t0.4178\t0.6\t0.0096\t0.87\t0.1091\t384\t196\n",
    "Class 36\t1\t0.8209\t0.8676\t0.6383\t1\t0.0101\t0.866\t0.475\t1\t0.0099\t0.8704\t0.1101\t384\t196\n",
    "Class 37\t1\t0.77\t0.8676\t0.6386\t0.8667\t0.0111\t0.8633\t0.4078\t0.7333\t0.0098\t0.8665\t0.1095\t384\t204\n",
    "Class 38\t1\t0.832\t0.8676\t0.6383\t0.7333\t0.0137\t0.8651\t0.4764\t0.4667\t0.0092\t0.8618\t0.1098\t384\t196\n",
    "Class 39\t1\t0.8287\t0.8676\t0.6383\t0.9667\t0.0107\t0.8687\t0.5185\t0.9667\t0.0095\t0.8658\t0.1073\t384\t197\n",
    "Class 40\t1\t0.7854\t0.8676\t0.6385\t0.9\t0.0147\t0.8709\t0.4575\t0.7\t0.0099\t0.8644\t0.1099\t384\t196\n",
    "Class 41\t0.8667\t0.5996\t0.8684\t0.6396\t0.5\t0.0119\t0.8656\t0.4208\t0.5\t0.0095\t0.8622\t0.1074\t384\t168\n",
    "Class 42\t0.514\t0.29\t0.9471\t0.717\t0.036\t0.0056\t0.9467\t0.3255\t0.037\t0.007\t0.9476\t0.1226\t384\t195\n",
    "Class 43\t0.8333\t0.5676\t0.8686\t0.6397\t0.5\t0.0128\t0.864\t0.4204\t0.4667\t0.0094\t0.8675\t0.1051\t384\t185\n",
    "Class 44\t0.9333\t0.7291\t0.868\t0.6388\t0.8\t0.0142\t0.868\t0.4585\t0.7333\t0.0099\t0.8665\t0.1041\t384\t212\n",
    "Class 45\t1\t0.8544\t0.8676\t0.6382\t1\t0.014\t0.8682\t0.5627\t1\t0.0102\t0.8676\t0.1082\t384\t186\n",
    "Class 46\t0.8\t0.603\t0.8687\t0.6395\t0.3333\t0.0115\t0.8718\t0.4216\t0.3\t0.0095\t0.8651\t0.1081\t384\t190\n",
    "Class 47\t0.9667\t0.7855\t0.8678\t0.6385\t0.8333\t0.0135\t0.8656\t0.4746\t0.7\t0.0098\t0.8631\t0.1038\t384\t185\n",
    "Class 48\t0.9333\t0.6621\t0.868\t0.6392\t0.6\t0.0107\t0.866\t0.3827\t0.6\t0.0092\t0.868\t0.1064\t384\t188\n",
    "Class 49\t0.8\t0.6157\t0.8687\t0.6395\t0.4667\t0.014\t0.8693\t0.4216\t0.4333\t0.009\t0.8695\t0.1051\t384\t201\n",
    "Class 50\t1\t0.7456\t0.8676\t0.6388\t0.4667\t0.0097\t0.8669\t0.4467\t0.4667\t0.0092\t0.8698\t0.1098\t384\t210\n",
    "Class 51\t1\t0.7898\t0.8676\t0.6385\t0.9\t0.0172\t0.8673\t0.4929\t0.6667\t0.0103\t0.8686\t0.11\t384\t195\n",
    "Class 52\t1\t0.8303\t0.8676\t0.6383\t1\t0.0155\t0.8636\t0.4668\t0.8333\t0.0103\t0.8673\t0.1061\t384\t189\n",
    "Class 53\t0.8667\t0.6475\t0.8684\t0.6393\t0.5667\t0.0114\t0.8704\t0.4375\t0.5667\t0.0093\t0.8773\t0.1114\t384\t193\n",
    "Class 54\t0.9\t0.607\t0.8682\t0.6395\t0.3667\t0.0118\t0.8686\t0.3784\t0.3333\t0.0096\t0.8656\t0.1081\t384\t182\n",
    "Class 55\t0.9667\t0.6171\t0.8678\t0.6395\t0.2667\t0.0089\t0.8664\t0.402\t0.2667\t0.0087\t0.8675\t0.1115\t384\t212\n",
    "Class 56\t0.9\t0.6483\t0.8682\t0.6393\t0.5667\t0.0118\t0.8744\t0.4138\t0.5333\t0.0102\t0.8676\t0.107\t384\t192\n",
    "Class 57\t0.9\t0.5923\t0.8682\t0.6396\t0.5333\t0.0142\t0.8642\t0.3944\t0.3667\t0.0092\t0.8693\t0.1064\t384\t191\n",
    "Class 58\t0.9333\t0.7161\t0.868\t0.6389\t0.8333\t0.0111\t0.8687\t0.4555\t0.7\t0.0093\t0.8665\t0.1094\t384\t186\n",
    "Class 59\t1\t0.8709\t0.8676\t0.6381\t1\t0.0137\t0.8644\t0.5103\t0.8333\t0.0095\t0.866\t0.1089\t384\t188\n",
    "Class 60\t0.8\t0.5944\t0.8687\t0.6396\t0.4\t0.0123\t0.8715\t0.4215\t0.3\t0.0093\t0.8676\t0.11\t384\t200\n",
    "Class 61\t1\t0.7286\t0.8676\t0.6388\t0.4\t0.0149\t0.8616\t0.4251\t0.2\t0.0092\t0.8656\t0.1088\t384\t186\n",
    "Class 62\t0.9\t0.7418\t0.8682\t0.6388\t0.9\t0.012\t0.8645\t0.5055\t0.8667\t0.0088\t0.8618\t0.1084\t384\t201\n",
    "Class 63\t0.9333\t0.7355\t0.868\t0.6388\t0.7667\t0.022\t0.8689\t0.5147\t0.6333\t0.0102\t0.8654\t0.1096\t384\t189\n",
    "Class 64\t0.9\t0.7202\t0.8682\t0.6389\t0.8667\t0.013\t0.8711\t0.4932\t0.8\t0.0091\t0.8728\t0.1097\t384\t200\n",
    "Class 65\t1\t0.7029\t0.8676\t0.639\t0.5\t0.014\t0.8698\t0.4401\t0.4667\t0.0101\t0.8671\t0.1091\t384\t209\n",
    "Class 66\t0.9667\t0.7721\t0.8678\t0.6386\t0.8\t0.021\t0.8671\t0.538\t0.7667\t0.011\t0.8693\t0.1111\t384\t187\n",
    "Class 67\t1\t0.7152\t0.8676\t0.6389\t0.7333\t0.0096\t0.8678\t0.4136\t0.7\t0.0095\t0.8687\t0.109\t384\t192\n",
    "Class 68\t0.8\t0.5838\t0.8687\t0.6396\t0.4333\t0.0088\t0.87\t0.3841\t0.4333\t0.0089\t0.8689\t0.109\t384\t191\n",
    "Class 69\t0.9667\t0.7655\t0.8678\t0.6386\t0.8333\t0.014\t0.8664\t0.4929\t0.7333\t0.0096\t0.864\t0.1047\t384\t206\n",
    "Class 70\t0.7333\t0.5502\t0.8691\t0.6398\t0.4667\t0.0133\t0.8676\t0.4171\t0.4\t0.0086\t0.864\t0.1032\t384\t194\n",
    "Class 71\t0.8667\t0.5836\t0.8684\t0.6396\t0.3333\t0.0104\t0.8664\t0.4019\t0.3333\t0.0081\t0.868\t0.1081\t384\t209\n",
    "Class 72\t1\t0.7706\t0.8676\t0.6386\t0.8333\t0.0138\t0.8667\t0.432\t0.6333\t0.01\t0.8698\t0.1108\t384\t199\n",
    "Class 73\t1\t0.7789\t0.8676\t0.6386\t0.8667\t0.0115\t0.8656\t0.4639\t0.7333\t0.0093\t0.8665\t0.1098\t384\t187\n",
    "Class 74\t1\t0.7985\t0.8676\t0.6385\t0.7\t0.0108\t0.862\t0.4273\t0.6667\t0.0095\t0.8618\t0.1054\t384\t200\n",
    "Class 75\t0.8667\t0.4779\t0.8684\t0.6402\t0.5\t0.01\t0.8662\t0.3932\t0.5\t0.0096\t0.8684\t0.11\t384\t212\n",
    "Class 76\t1\t0.7738\t0.8676\t0.6386\t0.8\t0.0126\t0.8647\t0.4367\t0.5\t0.0093\t0.8631\t0.1118\t384\t200\n",
    "Class 77\t0.9667\t0.6602\t0.8678\t0.6392\t0.5667\t0.0129\t0.8645\t0.4103\t0.4333\t0.0103\t0.8675\t0.1027\t384\t189\n",
    "Class 78\t0.9\t0.6567\t0.8682\t0.6392\t0.5667\t0.0146\t0.8678\t0.4158\t0.3333\t0.0097\t0.868\t0.1098\t384\t209\n",
    "Class 79\t0.8667\t0.6551\t0.8684\t0.6393\t0.6\t0.0147\t0.8687\t0.4458\t0.5333\t0.01\t0.8665\t0.1102\t384\t210\n",
    "Class 80\t1\t0.7924\t0.8676\t0.6385\t0.8333\t0.015\t0.8676\t0.4631\t0.6667\t0.0098\t0.8667\t0.1094\t384\t198\n",
    "Class 81\t1\t0.8355\t0.8676\t0.6383\t0.8333\t0.0115\t0.8676\t0.4713\t0.8\t0.0099\t0.8664\t0.1111\t384\t198\n",
    "Class 82\t0.9667\t0.6965\t0.8678\t0.639\t0.7333\t0.0105\t0.8691\t0.3627\t0.7333\t0.0098\t0.8686\t0.1085\t384\t193\n",
    "Class 83\t0.9667\t0.7965\t0.8678\t0.6385\t0.8333\t0.0099\t0.8711\t0.4438\t0.8333\t0.0091\t0.8689\t0.1053\t384\t183\n",
    "Class 84\t0.9667\t0.7283\t0.8678\t0.6388\t0.7667\t0.0139\t0.8686\t0.4526\t0.6\t0.0099\t0.868\t0.1027\t384\t202\n",
    "Class 85\t1\t0.7278\t0.8676\t0.6389\t0.7667\t0.0112\t0.8707\t0.4645\t0.7\t0.0098\t0.8669\t0.1119\t384\t193\n",
    "Class 86\t0.9667\t0.7204\t0.8678\t0.6389\t0.7\t0.0127\t0.8665\t0.4365\t0.6333\t0.0098\t0.8667\t0.1111\t384\t193\n",
    "Class 87\t1\t0.6578\t0.8676\t0.6392\t0.3333\t0.0106\t0.8744\t0.4161\t0.3333\t0.0091\t0.8695\t0.1121\t384\t196\n",
    "Class 88\t1\t0.8461\t0.8676\t0.6382\t1\t0.0203\t0.8676\t0.5081\t0.8667\t0.0098\t0.8748\t0.1097\t384\t185\n",
    "Class 89\t0.8\t0.6843\t0.8687\t0.6391\t0.8\t0.011\t0.8673\t0.5183\t0.7667\t0.009\t0.8669\t0.107\t384\t193\n",
    "Class 90\t0.9\t0.6098\t0.8682\t0.6395\t0.2333\t0.0104\t0.8698\t0.3781\t0.2333\t0.0094\t0.8693\t0.1099\t384\t220\n",
    "Class 91\t1\t0.6915\t0.8676\t0.6391\t0.4333\t0.0156\t0.8649\t0.4487\t0.3\t0.0096\t0.8631\t0.1047\t384\t189\n",
    "Class 92\t1\t0.8676\t0.8676\t0.6381\t1\t0.0122\t0.8675\t0.5131\t0.9333\t0.0103\t0.8707\t0.111\t384\t210\n",
    "Class 93\t0.9667\t0.6933\t0.8678\t0.639\t0.5333\t0.0112\t0.8616\t0.3852\t0.3667\t0.0094\t0.8631\t0.1086\t384\t192\n",
    "Class 94\t0.9333\t0.8079\t0.868\t0.6384\t0.9333\t0.0131\t0.8667\t0.4849\t0.9333\t0.0093\t0.8642\t0.1064\t384\t201\n",
    "Class 95\t0.9667\t0.788\t0.8678\t0.6385\t0.9\t0.0106\t0.866\t0.4435\t0.7667\t0.0093\t0.8662\t0.1068\t384\t209\n",
    "Class 96\t1\t0.7579\t0.8676\t0.6387\t0.6333\t0.0108\t0.8585\t0.3865\t0.6333\t0.01\t0.8607\t0.1042\t384\t197\n",
    "Class 97\t0.9\t0.6711\t0.8682\t0.6392\t0.5\t0.0159\t0.87\t0.4495\t0.2667\t0.0091\t0.8633\t0.106\t384\t209\n",
    "Class 98\t1\t0.8303\t0.8676\t0.6383\t0.9\t0.0108\t0.8682\t0.4789\t0.9\t0.0102\t0.8715\t0.1083\t384\t197\n",
    "Class 99\t0.9667\t0.7154\t0.8678\t0.6389\t0.7\t0.0106\t0.8647\t0.4179\t0.6333\t0.009\t0.868\t0.1093\t384\t188\n",
    "Class 100\t1\t0.8755\t0.8676\t0.638\t1\t0.0151\t0.8671\t0.528\t0.9667\t0.0098\t0.8653\t0.1036\t384\t209\n",
    "Class 101\t0.9333\t0.6996\t0.868\t0.639\t0.5667\t0.0132\t0.8673\t0.4339\t0.5333\t0.0089\t0.862\t0.1085\t384\t199\n",
    "Class 102\t0.8667\t0.6747\t0.8684\t0.6391\t0.4333\t0.0115\t0.8697\t0.4165\t0.3\t0.0089\t0.87\t0.1127\t384\t212\n",
    "Class 103\t1\t0.8107\t0.8676\t0.6384\t0.8667\t0.01\t0.8665\t0.4131\t0.8333\t0.0098\t0.862\t0.1105\t384\t195\n",
    "Class 104\t0.9667\t0.6437\t0.8678\t0.6393\t0.5667\t0.0131\t0.8684\t0.4363\t0.5667\t0.0097\t0.8709\t0.1077\t384\t202\n",
    "Class 105\t0.9667\t0.6634\t0.8678\t0.6392\t0.7\t0.0119\t0.8737\t0.4279\t0.6333\t0.0102\t0.8702\t0.1128\t384\t193\n",
    "Class 106\t0.9667\t0.6461\t0.8678\t0.6393\t0.3667\t0.0174\t0.8664\t0.4191\t0.1667\t0.0099\t0.8622\t0.1043\t384\t199\n",
    "Class 107\t0.8\t0.5461\t0.8687\t0.6398\t0.3667\t0.0126\t0.8735\t0.4513\t0.3333\t0.0094\t0.8717\t0.1106\t384\t201\n",
    "Class 108\t0.9667\t0.7696\t0.8678\t0.6386\t0.8667\t0.009\t0.8682\t0.4573\t0.7667\t0.0086\t0.8722\t0.1093\t384\t202\n",
    "Class 109\t0.9667\t0.7304\t0.8678\t0.6388\t0.7333\t0.0106\t0.8689\t0.456\t0.7333\t0.0095\t0.8684\t0.1067\t384\t188\n",
    "Class 110\t0.9333\t0.6648\t0.868\t0.6392\t0.3333\t0.0102\t0.8706\t0.4313\t0.2667\t0.0087\t0.8702\t0.1064\t384\t218\n",
    "Class 111\t0.9333\t0.6545\t0.868\t0.6393\t0.7333\t0.0144\t0.8704\t0.4668\t0.6333\t0.0092\t0.8739\t0.1108\t384\t187\n",
    "Class 112\t0.8667\t0.5469\t0.8684\t0.6398\t0.4\t0.012\t0.8658\t0.4142\t0.4\t0.0088\t0.8709\t0.1105\t384\t188\n",
    "Class 113\t0.9667\t0.8063\t0.8678\t0.6384\t0.9667\t0.0097\t0.8654\t0.5067\t0.9667\t0.0089\t0.866\t0.1087\t384\t199\n",
    "Class 114\t0.9\t0.6331\t0.8682\t0.6394\t0.3333\t0.0105\t0.8728\t0.3988\t0.3333\t0.0095\t0.8676\t0.1091\t384\t186\n",
    "Class 115\t0.9333\t0.6261\t0.868\t0.6394\t0.2667\t0.0117\t0.8618\t0.3792\t0.2333\t0.0098\t0.8662\t0.1065\t384\t183\n",
    "Class 116\t1\t0.8005\t0.8676\t0.6385\t1\t0.0095\t0.8669\t0.4306\t0.9\t0.0093\t0.8658\t0.1133\t384\t179\n",
    "Class 117\t0.9667\t0.7434\t0.8678\t0.6388\t0.7\t0.0113\t0.8644\t0.4322\t0.4667\t0.0095\t0.8698\t0.1079\t384\t194\n",
    "Class 118\t1\t0.7257\t0.8676\t0.6389\t0.7\t0.0143\t0.8645\t0.4512\t0.7\t0.0093\t0.8622\t0.1003\t384\t200\n",
    "Class 119\t1\t0.8004\t0.8676\t0.6385\t0.8\t0.0132\t0.864\t0.4422\t0.6333\t0.0101\t0.8642\t0.1088\t384\t189\n",
    "Class 120\t0.9667\t0.7947\t0.8678\t0.6385\t0.9333\t0.0138\t0.8633\t0.4764\t0.8\t0.0091\t0.864\t0.108\t384\t200\n",
    "Class 121\t0.9333\t0.6605\t0.868\t0.6392\t0.3333\t0.01\t0.8662\t0.3663\t0.3667\t0.0092\t0.8675\t0.111\t384\t225\n",
    "Class 122\t1\t0.7937\t0.8676\t0.6385\t0.9667\t0.0121\t0.8653\t0.445\t0.8\t0.0094\t0.8634\t0.1063\t384\t188\n",
    "Class 123\t0.9333\t0.7195\t0.868\t0.6389\t0.4333\t0.0117\t0.8675\t0.436\t0.4\t0.0093\t0.8673\t0.1085\t384\t192\n",
    "Class 124\t0.9667\t0.7197\t0.8678\t0.6389\t0.8\t0.0107\t0.8698\t0.4238\t0.7667\t0.0094\t0.8676\t0.1104\t384\t202\n",
    "Class 125\t0.8667\t0.5987\t0.8684\t0.6396\t0.5667\t0.0123\t0.8707\t0.4426\t0.5667\t0.0097\t0.8691\t0.1097\t384\t191\n",
    "Class 126\t0.9667\t0.5716\t0.8678\t0.6397\t0.5\t0.0121\t0.8644\t0.4291\t0.4333\t0.0091\t0.8603\t0.1051\t384\t192\n",
    "Class 127\t1\t0.8266\t0.8676\t0.6383\t1\t0.0106\t0.8682\t0.4938\t0.9667\t0.0098\t0.8686\t0.1095\t384\t193\n",
    "Class 128\t1\t0.8069\t0.8676\t0.6384\t0.8333\t0.0186\t0.8656\t0.4761\t0.4333\t0.0095\t0.8689\t0.1069\t384\t200\n",
    "Class 129\t1\t0.7665\t0.8676\t0.6386\t0.9\t0.0153\t0.8684\t0.445\t0.6667\t0.0101\t0.8662\t0.1116\t384\t180\n",
    "Class 130\t1\t0.8077\t0.8676\t0.6384\t0.9333\t0.0179\t0.8686\t0.4905\t0.7667\t0.0098\t0.866\t0.1081\t384\t196\n",
    "Class 131\t0.9\t0.6773\t0.8682\t0.6391\t0.5333\t0.0113\t0.8729\t0.4005\t0.4333\t0.0095\t0.8698\t0.1085\t384\t196\n",
    "Class 132\t1\t0.7671\t0.8676\t0.6386\t1\t0.015\t0.866\t0.4853\t0.7333\t0.0091\t0.8638\t0.1097\t384\t197\n",
    "Class 133\t0.9\t0.7215\t0.8682\t0.6389\t0.7\t0.0118\t0.8691\t0.4302\t0.6333\t0.0097\t0.8686\t0.1061\t384\t196\n",
    "Class 134\t0.9333\t0.6043\t0.868\t0.6395\t0.4\t0.0162\t0.8675\t0.4471\t0.3667\t0.0091\t0.8695\t0.1119\t384\t199\n",
    "Class 135\t1\t0.8334\t0.8676\t0.6383\t1\t0.0163\t0.8669\t0.5282\t0.9667\t0.0102\t0.8676\t0.1059\t384\t200\n",
    "Class 136\t0.9667\t0.6949\t0.8678\t0.639\t0.4667\t0.0112\t0.8673\t0.4252\t0.4667\t0.0096\t0.8669\t0.108\t384\t202\n",
    "Class 137\t1\t0.8039\t0.8676\t0.6384\t0.7667\t0.0128\t0.8623\t0.4372\t0.6333\t0.0098\t0.8622\t0.1063\t384\t192\n",
    "Class 138\t0.9333\t0.7579\t0.868\t0.6387\t0.8\t0.0109\t0.8664\t0.4332\t0.8333\t0.0094\t0.8684\t0.1079\t384\t193\n",
    "Class 139\t0.9667\t0.7284\t0.8678\t0.6388\t0.5333\t0.0126\t0.8647\t0.4628\t0.4667\t0.0093\t0.8645\t0.1119\t384\t193\n",
    "Class 140\t1\t0.7881\t0.8676\t0.6385\t0.6333\t0.0104\t0.8698\t0.4396\t0.5667\t0.0092\t0.8676\t0.1095\t384\t178\n",
    "Class 141\t0.9667\t0.7638\t0.8678\t0.6387\t0.6333\t0.0177\t0.8693\t0.501\t0.5333\t0.0094\t0.8669\t0.1061\t384\t191\n",
    "Class 142\t1\t0.8219\t0.8676\t0.6383\t0.9\t0.0099\t0.8653\t0.4469\t0.8667\t0.0095\t0.858\t0.1049\t384\t200\n",
    "Class 143\t0.9667\t0.7176\t0.8678\t0.6389\t0.5667\t0.0109\t0.8678\t0.4413\t0.5667\t0.0096\t0.8687\t0.1106\t384\t190\n",
    "Class 144\t0.9\t0.5452\t0.8682\t0.6399\t0.3333\t0.0109\t0.8616\t0.4068\t0.3333\t0.0093\t0.8631\t0.11\t384\t190\n",
    "Class 145\t0.9\t0.6503\t0.8682\t0.6393\t0.3\t0.0139\t0.8709\t0.4449\t0.2333\t0.0099\t0.8645\t0.1097\t384\t203\n",
    "Class 146\t0.9\t0.7262\t0.8682\t0.6389\t0.8667\t0.0155\t0.8682\t0.478\t0.7\t0.0096\t0.864\t0.1099\t384\t193\n",
    "Class 147\t1\t0.7295\t0.8676\t0.6388\t0.5333\t0.012\t0.8669\t0.4116\t0.4333\t0.0096\t0.8702\t0.1067\t384\t186\n",
    "Class 148\t0.8667\t0.5723\t0.8684\t0.6397\t0.4667\t0.0099\t0.862\t0.3923\t0.4667\t0.0087\t0.86\t0.1029\t384\t195\n",
    "Class 149\t0.9667\t0.738\t0.8678\t0.6388\t0.6\t0.0107\t0.8654\t0.3782\t0.6\t0.0097\t0.8667\t0.1118\t384\t199'''\n",
    "\n",
    "def format_latex_row(line):\n",
    "    # Split the input line into components\n",
    "    parts = line.split()\n",
    "    \n",
    "    # Extract values, skipping the last two columns (384 and intersection)\n",
    "    values = parts[2:-2]\n",
    "    \n",
    "    # Format class name\n",
    "    class_name = f\"{parts[0]} {parts[1]}\"\n",
    "    \n",
    "    # Convert values to 3 decimal format\n",
    "    formatted_values = [f\"{float(val):.3f}\" for val in values]\n",
    "    \n",
    "    # Remove leading zeros\n",
    "    formatted_values = [val.replace('0.', '.') for val in formatted_values]\n",
    "    \n",
    "    # Combine into LaTeX format\n",
    "    return f\"{class_name} & \" + \" & \".join(formatted_values) + \" \\\\\\\\\"\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "for line in data.split('\\n'):\n",
    "    print(format_latex_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
