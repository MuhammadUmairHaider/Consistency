{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage:  0.3\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n",
      "'World' is already a single token (ID: 10603)\n",
      "'Sports' is already a single token (ID: 18153)\n",
      "'Business' is already a single token (ID: 24749)\n",
      "\n",
      "Added 1 new tokens to the tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d1/grad/mha361/anaconda3/envs/memit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_162071/699109830.py:245: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "import random\n",
    "import numpy as np\n",
    "from utilities import evaluate_gpt2_classification as evaluate_gpt2_classification, mask_range_gpt,compute_masks, reset_gpt, compute_mask_probe, mask_gpt2\n",
    "import torch  \n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_name = \"fancyzhx/ag_news\"\n",
    "\n",
    "text_tag = \"text\"\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "\n",
    "\n",
    "tables = []\n",
    "layer = 6\n",
    "# for i in tqdm(range(1, 21)):\n",
    "per = 0.3\n",
    "print(\"Percentage: \", per)\n",
    "num_classes = 4\n",
    "\n",
    "# tao = 2.5\n",
    "\n",
    "lab = \"label\"\n",
    "# tao = torch.inf\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# print(dataset['train'].features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################Filter dataset####################\n",
    "from datasets import DatasetDict, Dataset, Features, ClassLabel, Value\n",
    "import pandas as pd\n",
    "\n",
    "def sample_balanced_dataset(dataset_dict, max_train_per_class=800, max_test_per_class=200):\n",
    "    \"\"\"\n",
    "    Sample a balanced subset while preserving the original feature structure including ClassLabel.\n",
    "    \"\"\"\n",
    "    # Store original features\n",
    "    original_features = dataset_dict['train'].features\n",
    "    \n",
    "    # Convert to pandas for sampling\n",
    "    train_df = dataset_dict['train'].to_pandas()\n",
    "    test_df = dataset_dict['test'].to_pandas()\n",
    "    \n",
    "    # Group by label\n",
    "    train_groups = train_df.groupby('label')\n",
    "    test_groups = test_df.groupby('label')\n",
    "    \n",
    "    sampled_train_dfs = []\n",
    "    sampled_test_dfs = []\n",
    "    \n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(\"\\nLabel | Label Name | Train Samples | Test Samples | Final Train | Final Test\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    label_names = original_features['label'].names\n",
    "    for idx, label_name in enumerate(label_names):\n",
    "        train_group = train_groups.get_group(idx)\n",
    "        test_group = test_groups.get_group(idx) if idx in test_groups.groups else pd.DataFrame()\n",
    "        \n",
    "        # Sample with replacement if needed\n",
    "        train_replace = len(train_group) < max_train_per_class\n",
    "        test_replace = len(test_group) < max_test_per_class\n",
    "        \n",
    "        sampled_train = train_group.sample(\n",
    "            n=min(len(train_group), max_train_per_class),\n",
    "            replace=train_replace,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        if not test_group.empty:\n",
    "            sampled_test = test_group.sample(\n",
    "                n=min(len(test_group), max_test_per_class),\n",
    "                replace=test_replace,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            sampled_test = pd.DataFrame(columns=test_df.columns)\n",
    "        \n",
    "        sampled_train_dfs.append(sampled_train)\n",
    "        sampled_test_dfs.append(sampled_test)\n",
    "        \n",
    "        print(f\"{idx:5d} | {label_name:10s} | {len(train_group):12d} | \"\n",
    "              f\"{len(test_group):11d} | {len(sampled_train):10d} | {len(sampled_test):9d}\")\n",
    "    \n",
    "    # Concatenate all sampled dataframes\n",
    "    final_train_df = pd.concat(sampled_train_dfs, ignore_index=True)\n",
    "    final_test_df = pd.concat(sampled_test_dfs, ignore_index=True)\n",
    "    \n",
    "    # Convert back to datasets while preserving the original features\n",
    "    final_train_dataset = Dataset.from_pandas(final_train_df, features=original_features)\n",
    "    final_test_dataset = Dataset.from_pandas(final_test_df, features=original_features)\n",
    "    \n",
    "    # Create new DatasetDict\n",
    "    sampled_dataset = DatasetDict({\n",
    "        'train': final_train_dataset,\n",
    "        'test': final_test_dataset\n",
    "    })\n",
    "    \n",
    "    print(\"\\nFinal dataset sizes:\")\n",
    "    print(f\"Train: {len(final_train_dataset)} samples\")\n",
    "    print(f\"Test: {len(final_test_dataset)} samples\")\n",
    "    \n",
    "    # Verify feature structure is preserved\n",
    "    print(\"\\nVerifying feature structure:\")\n",
    "    print(sampled_dataset['train'].features)\n",
    "    \n",
    "    return sampled_dataset\n",
    "\n",
    "# dataset = sample_balanced_dataset(dataset, max_train_per_class=800, max_test_per_class=200)\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "# Set random seed\n",
    "seed_value = 42  # or any other integer\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():  # PyTorch-specific\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "special_tokens_dict = {}\n",
    "new_tokens = []\n",
    "label2text = dataset['train'].features[lab].names\n",
    "\n",
    "for label in label2text:\n",
    "    # Create special token format (with and without space)\n",
    "    special_token = f'{label}'\n",
    "    \n",
    "    # Check if the label is already a single token in the tokenizer\n",
    "    label_tokens = tokenizer.encode(label, add_special_tokens=False)\n",
    "    is_single_token = len(label_tokens) == 1\n",
    "    \n",
    "    if is_single_token:\n",
    "        print(f\"'{label}' is already a single token (ID: {label_tokens[0]})\")\n",
    "    \n",
    "    # Add both versions to new tokens list\n",
    "    new_tokens.extend([special_token])\n",
    "\n",
    "# Add the tokens to the tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"\\nAdded {num_added_tokens} new tokens to the tokenizer\")\n",
    "\n",
    "special_tokens = {\n",
    "    'pad_token': '<|pad|>',\n",
    "    'sep_token': '<|sep|>',\n",
    "    'eos_token': '<|eos|>'\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "def format_data(examples):\n",
    "    formatted_texts = []\n",
    "    for text, label in zip(examples[text_tag], examples[lab]):\n",
    "        # Convert label to string\n",
    "        \n",
    "        tok_text = tokenizer.encode(text, max_length=400, truncation=True)\n",
    "        text = tokenizer.decode(tok_text)\n",
    "        label_str = dataset['train'].features[lab].int2str(label)\n",
    "        formatted_text = f\"Classify emotion: {text}{tokenizer.sep_token}\"#{label_str}{tokenizer.eos_token}\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "    return {'formatted_text': formatted_texts}\n",
    "\n",
    "def tokenize_and_prepare(examples):\n",
    "\n",
    "    # Tokenize with batch processing\n",
    "    tokenized = tokenizer(\n",
    "        examples['formatted_text'],\n",
    "        padding='max_length',\n",
    "        max_length=408,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Clone input_ids to create labels\n",
    "    labels = tokenized['input_ids'].clone()\n",
    "    \n",
    "    # Find the position of sep_token\n",
    "    sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "    sep_positions = (labels == sep_token_id).nonzero(as_tuple=True)\n",
    "    \n",
    "    # Mask all tokens with -100 except for the token right after sep_token\n",
    "    labels[:] = -100  # Mask all initially\n",
    "    for batch_idx, sep_pos in zip(*sep_positions):\n",
    "        if sep_pos + 1 < labels.size(1):\n",
    "            labels[batch_idx, sep_pos + 1] = tokenized['input_ids'][batch_idx, sep_pos + 1]\n",
    "    \n",
    "    # Set padding tokens to -100\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "dataset = dataset.filter(lambda x: x[lab] != -1)\n",
    "# Process the dataset\n",
    "formatted_dataset = dataset.map(format_data, batched=True)\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_and_prepare, \n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel as gt\n",
    "from models.gpt2 import GPT2LMHeadModel\n",
    "# Load pre-trained GPT-2 model\n",
    "model1 = gt.from_pretrained('gpt2')\n",
    "\n",
    "model1.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model1.config.m_layer = layer\n",
    "import os\n",
    "\n",
    "base_path = os.path.join(\"model_weights\", dataset_name)\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "weights_path = os.path.join(base_path, \"weights.pth\")\n",
    "\n",
    "model = GPT2LMHeadModel(model1.config)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording activations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23dc9c253cb949819abd591cb27e2057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:693: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item['input_ids']).to(device)\n",
      "/mounts/u-amo-d1/grad/mha361/work/probless/Sentiment/utilities.py:694: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item['attention_mask']).to(device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbc2da78d7240cb81edd167dd75bdeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0 base accuracy:  0.9547 0.9509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aa79e96c4140a4ac9d78a4d0edbcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0 complement base accuracy:  0.9412 0.9276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa7307328034577932b98b7f99f27ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ef8c7ded97412cbadfef71f37708a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  1 base accuracy:  0.9863 0.9811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee5a87f85704087bc40b20d86a97c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  1 complement base accuracy:  0.9307 0.9175\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f2d16ee7ff4afea33144d69c07a2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651dd51d37dc403490c7ca3c5cd6da67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  2 base accuracy:  0.8974 0.8858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433a0bec51c5449eb2d21b859b9b641f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  2 complement base accuracy:  0.9604 0.9493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc32de8eb8041d88e22a141e9200005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b07738aded47f0a524bf1eea2058b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  3 base accuracy:  0.94 0.9159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67bb472cd2b458d9cc2c2e9c92f98cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  3 complement base accuracy:  0.9461 0.9392\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.tensor\")\n",
    "\n",
    "batch_size = 128\n",
    "# mask_layer = 5\n",
    "compliment = True\n",
    "results_table = PrettyTable()\n",
    "if(compliment):\n",
    "    results_table.field_names = results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"Base Complement Acc\", \"Base Compliment Conf\", \"STD Accuracy\", \"STD Confidence\", \"STD compliment ACC\", \"STD compliment Conf\", \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\", \"Total Masked\", \"Intersection\"]#, \"Same as Max\"]#\"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"\n",
    "\n",
    "class_labels = []\n",
    "base_accuracies = []\n",
    "base_confidences = []\n",
    "base_comp_acc = []\n",
    "base_comp_conf = []\n",
    "std_masked_counts = []\n",
    "std_accuracies = []\n",
    "std_confidences = []\n",
    "std_comp_acc = []\n",
    "std_comp_conf = []\n",
    "max_masked_counts = []\n",
    "max_accuracies = []\n",
    "max_confidences = []\n",
    "max_comp_acc = []\n",
    "max_comp_conf = []\n",
    "diff_from_max = []\n",
    "total_masked = []\n",
    "\n",
    "#merge test and train set and then shuffle and make splits\n",
    "\n",
    "# First merge and shuffle\n",
    "# tokenized_dataset = concatenate_datasets([tokenized_dataset['train'], tokenized_dataset['test']]).shuffle(seed=42)#.select(range(100))\n",
    "\n",
    "# Get the total length\n",
    "# dataset_length = len(tokenized_dataset)\n",
    "\n",
    "\n",
    "# Calculate split index\n",
    "# split_index = int(dataset_length * 0.2)  # 80% for training\n",
    "\n",
    "# Create the splits using dataset slicing\n",
    "tokenized_dataset1 = tokenized_dataset['test']#.shuffle().select(range(200))\n",
    "recording_dataset = tokenized_dataset['train']#.shuffle().select(range(200))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "all_fc_vals = []\n",
    "base_accuracies = []\n",
    "base_confidences = []\n",
    "base_comp_acc = []\n",
    "base_comp_conf = []\n",
    "print(\"Recording activations...\")\n",
    "for j in range(0,num_classes):\n",
    "    dataset_recording = recording_dataset.filter(lambda x: x[lab] in [j])\n",
    "    dataset = tokenized_dataset1.filter(lambda x: x[lab] in [j])\n",
    "    dataset_complement = tokenized_dataset1.filter(lambda x: x[lab] not in [j])\n",
    "    fc_vals = evaluate_gpt2_classification(lab, model, dataset_recording, tokenizer, batch_size)\n",
    "    fc_vals = fc_vals[2]\n",
    "    all_fc_vals.append(np.array(fc_vals))\n",
    "    \n",
    "    \n",
    "    \n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset, tokenizer, batch_size)\n",
    "    \n",
    "    base_accuracies.append(acc[0])\n",
    "    base_confidences.append(acc[1])\n",
    "    \n",
    "    print(\"Class \",j, \"base accuracy: \", acc[0], acc[1])\n",
    "    \n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset_complement, tokenizer)\n",
    "    \n",
    "    base_comp_acc.append(acc[0])\n",
    "    base_comp_conf.append(acc[1])\n",
    "    \n",
    "    print(\"Class \",j, \"complement base accuracy: \", acc[0], acc[1])\n",
    "    \n",
    "    \n",
    "results_table = PrettyTable()\n",
    "if(compliment):\n",
    "    results_table.field_names = results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"Base Complement Acc\", \"Base Compliment Conf\", \"STD Accuracy\", \"STD Confidence\", \"STD compliment ACC\", \"STD compliment Conf\", \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\", \"Total Masked\", \"Intersection\"]#, \"Same as Max\"]#\"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"\n",
    "\n",
    "class_labels = []\n",
    "# base_accuracies = []\n",
    "# base_confidences = []\n",
    "# base_comp_acc = []\n",
    "# base_comp_conf = []\n",
    "std_masked_counts = []\n",
    "std_accuracies = []\n",
    "std_confidences = []\n",
    "std_comp_acc = []\n",
    "std_comp_conf = []\n",
    "max_masked_counts = []\n",
    "max_accuracies = []\n",
    "max_confidences = []\n",
    "max_comp_acc = []\n",
    "max_comp_conf = []\n",
    "diff_from_max = []\n",
    "total_masked = []\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0 base accuracy:  0.9547 0.9509\n",
      "Class  0 complement base accuracy:  0.9412 0.9276\n",
      "Total Masked : 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56c0cb25c4f427d83e8a6896902a912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking STD:  0.1532 0.0969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37c330c4f7b4c179451830a20b19da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking STD on complement:  0.3591 0.3181\n",
      "Masking MAX...\n",
      "Total Masked : 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22dc0c04fe846e087c5b96efff32443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking MAX:  0.0005 0.0006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0d2a222b194b97842aa9cffb78d785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking MAX on complement:  0.2879 0.272\n",
      "Class  1 base accuracy:  0.9863 0.9811\n",
      "Class  1 complement base accuracy:  0.9307 0.9175\n",
      "Total Masked : 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604666ca86904918b3e09a7f4471948a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking STD:  0.0442 0.0342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c30d14d4554664a4822956cb979521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking STD on complement:  0.3802 0.3377\n",
      "Masking MAX...\n",
      "Total Masked : 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886b53841d754f90982db9991f98714d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking MAX:  0.0016 0.0006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2949ebb4ac79494eb230d036ff50e457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking MAX on complement:  0.2186 0.1749\n",
      "Class  2 base accuracy:  0.8974 0.8858\n",
      "Class  2 complement base accuracy:  0.9604 0.9493\n",
      "Total Masked : 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a44730b9f5343759c6e934239b5a6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking STD:  0.0747 0.0555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c42d6e56f648d2b331c4d10165d077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking STD on complement:  0.3539 0.3224\n",
      "Masking MAX...\n",
      "Total Masked : 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931cbc3b89664e0e8d66d5077126a028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking MAX:  0.0063 0.0029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727b6c02805e4813aac60a6244160eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking MAX on complement:  0.2496 0.2468\n",
      "Class  3 base accuracy:  0.94 0.9159\n",
      "Class  3 complement base accuracy:  0.9461 0.9392\n",
      "Total Masked : 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44524d83c8414fd19e80c5ef63706715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking STD:  0.6679 0.6526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a2fd3915fd468cb31eb732578d9260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking STD on complement:  0.1242 0.0838\n",
      "Masking MAX...\n",
      "Total Masked : 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ca82d4cf604ffc86d7dd6c8df80bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking MAX:  0.7589 0.7403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd34ca8bcca409e852d9d6d777ca805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after masking MAX on complement:  0.0125 0.0063\n",
      "+---------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+--------------+----------------+--------------------+---------------------+\n",
      "|  Class  | Base Accuracy | Base Confidence | Base Complement Acc | Base Compliment Conf | STD Accuracy | STD Confidence | STD compliment ACC | STD compliment Conf | MAX Accuracy | MAX Confidence | Max compliment acc | Max compliment conf |\n",
      "+---------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+--------------+----------------+--------------------+---------------------+\n",
      "| Class 0 |     0.9547    |      0.9509     |        0.9412       |        0.9276        |    0.1532    |     0.0969     |       0.3591       |        0.3181       |    0.0005    |     0.0006     |       0.2879       |        0.272        |\n",
      "| Class 1 |     0.9863    |      0.9811     |        0.9307       |        0.9175        |    0.0442    |     0.0342     |       0.3802       |        0.3377       |    0.0016    |     0.0006     |       0.2186       |        0.1749       |\n",
      "| Class 2 |     0.8974    |      0.8858     |        0.9604       |        0.9493        |    0.0747    |     0.0555     |       0.3539       |        0.3224       |    0.0063    |     0.0029     |       0.2496       |        0.2468       |\n",
      "| Class 3 |      0.94     |      0.9159     |        0.9461       |        0.9392        |    0.6679    |     0.6526     |       0.1242       |        0.0838       |    0.7589    |     0.7403     |       0.0125       |        0.0063       |\n",
      "+---------+---------------+-----------------+---------------------+----------------------+--------------+----------------+--------------------+---------------------+--------------+----------------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import utilities\n",
    "import importlib\n",
    "importlib.reload(utilities)\n",
    "from utilities import compute_masks\n",
    "\n",
    "compliment = True\n",
    "results_table = PrettyTable()\n",
    "if(compliment):\n",
    "    results_table.field_names = results_table.field_names = [\"Class\", \"Base Accuracy\", \"Base Confidence\", \"Base Complement Acc\", \"Base Compliment Conf\", \"STD Accuracy\", \"STD Confidence\", \"STD compliment ACC\", \"STD compliment Conf\", \"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"]#, \"Same as Max\"]#\"MAX Accuracy\", \"MAX Confidence\", \"Max compliment acc\", \"Max compliment conf\"\n",
    "\n",
    "per = 0.5\n",
    "for j in range(0,num_classes):\n",
    "    fc_vals = all_fc_vals[j]\n",
    "    model = reset_gpt(model)\n",
    "    model = mask_gpt2(model, torch.ones(768).to('cuda'))\n",
    "    dataset = tokenized_dataset1.filter(lambda x: x[lab] in [j])\n",
    "    dataset_recording = recording_dataset.filter(lambda x: x[lab] in [j])\n",
    "    dataset_complement = tokenized_dataset1.filter(lambda x: x[lab] not in [j])\n",
    "    \n",
    "\n",
    "    class_labels.append(f\"Class {j}\")\n",
    "    # acc = evaluate_gpt2_classification(lab, model, dataset, tokenizer)\n",
    "    print(\"Class \",j, \"base accuracy: \", base_accuracies[j], base_confidences[j])\n",
    "    if(compliment):\n",
    "        print(\"Class \",j, \"complement base accuracy: \", base_comp_acc[j], base_comp_conf[j])\n",
    "\n",
    "        \n",
    "    mask_max, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max, mask_max_random_off, random_mask = compute_masks(fc_vals,per)\n",
    "    mask_max2, mask_std, mask_intersection, mask_max_low_std, mask_max_high_std, mask_std_high_max, mask_max_random_off, random_mask = compute_masks(fc_vals,1)\n",
    "    \n",
    "    all_fc_vals_pass = all_fc_vals.copy()\n",
    "    # all_fc_vals_pass.pop(j)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    tao = 3.5\n",
    "    model = mask_range_gpt(model, mask_max2, fc_vals, tao, all_fc_vals_pass)        \n",
    "    t = int(mask_std.shape[0]-torch.count_nonzero(mask_max))\n",
    "    print(\"Total Masked :\", t)\n",
    "    # total_masked.append(t)\n",
    "    \n",
    "    \n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset, tokenizer) \n",
    "    print(\"accuracy after masking STD: \", acc[0], acc[1])\n",
    "    std_accuracies.append(acc[0])\n",
    "    std_confidences.append(acc[1])\n",
    "    if(compliment):\n",
    "        acc = evaluate_gpt2_classification(lab, model, dataset_complement, tokenizer)\n",
    "        print(\"accuracy after masking STD on complement: \", acc[0], acc[1])\n",
    "        std_comp_acc.append(acc[0])\n",
    "        std_comp_conf.append(acc[1])\n",
    "    model = reset_gpt(model)\n",
    "\n",
    "    print(\"Masking MAX...\")\n",
    "    tao = torch.inf\n",
    "    \n",
    "    # model = mask_distillbert(model,mask_max) \n",
    "    # model = mask_range_gpt(model, mask_max, fc_vals, tao, all_fc_vals_pass)\n",
    "    \n",
    "    # model = mask_gpt2(model, mask_max)\n",
    "    model = mask_range_gpt(model, mask_max, fc_vals, tao, all_fc_vals_pass)    \n",
    "    t = int(mask_max.shape[0]-torch.count_nonzero(mask_max))\n",
    "    print(\"Total Masked :\", t)\n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset, tokenizer)\n",
    "    print(\"accuracy after masking MAX: \", acc[0], acc[1])\n",
    "    max_accuracies.append(acc[0])\n",
    "    max_confidences.append(acc[1])\n",
    "    acc = evaluate_gpt2_classification(lab, model, dataset_complement, tokenizer)\n",
    "    print(\"accuracy after masking MAX on complement: \", acc[0], acc[1])\n",
    "    max_comp_acc.append(acc[0])\n",
    "    max_comp_conf.append(acc[1])\n",
    "    if(compliment):\n",
    "        results_table.add_row([\n",
    "            class_labels[j],\n",
    "            base_accuracies[j],\n",
    "            base_confidences[j],\n",
    "            base_comp_acc[j],\n",
    "            base_comp_conf[j],\n",
    "            std_accuracies[j],\n",
    "            std_confidences[j],\n",
    "            std_comp_acc[j],\n",
    "            std_comp_conf[j],\n",
    "            max_accuracies[j],\n",
    "            max_confidences[j],\n",
    "            max_comp_acc[j],\n",
    "            max_comp_conf[j],\n",
    "            # total_masked[j],\n",
    "            # diff_from_max[j]\n",
    "        ])            \n",
    "# print(\"Layer \", mask_layer)\n",
    "print(results_table)\n",
    "#     tables.append(results_table)\n",
    "#     # print(\"Layer \", mask_layer)\n",
    "#     print(\"Average Base Accuracy: \",round(sum(base_accuracies)/len(base_accuracies), 4))\n",
    "#     print(\"Average Base Confidence: \", round(sum(base_confidences)/len(base_confidences), 4))\n",
    "#     print(\"Average MAX Accuracy: \", round(sum(max_accuracies)/len(max_accuracies), 4))\n",
    "#     print(\"Average MAX Confidence: \", round(sum(max_confidences)/len(max_confidences), 4))\n",
    "#     print(\"Average MAX Complement Accuracy: \", round(sum(max_comp_acc)/len(max_comp_acc), 4))\n",
    "#     print(\"Average MAX Complement Confidence: \", round(sum(max_comp_conf)/len(max_comp_conf), 4))\n",
    "\n",
    "# per = 0.1\n",
    "# for table in tables:\n",
    "    \n",
    "#     print(f\"\\nAnalysis for Percentage: {per:.2f}\")\n",
    "#     print(table)\n",
    "#     print(\"\\n\")\n",
    "#     per += 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear PyTorch GPU cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% Trimmed Mean across all classes for each metric:\n",
      "Base Accuracy: 0.9872\n",
      "Base Confidence: 0.9842\n",
      "Base Complement Acc: 0.9926\n",
      "Base Compliment Conf: 0.9811\n",
      "MAX Accuracy: 0.9381\n",
      "MAX Confidence: 0.1577\n",
      "Max compliment acc: 0.9929\n",
      "Max compliment conf: 0.9919\n",
      "\n",
      "Classes sorted by Base Accuracy:\n",
      "Class  Base Accuracy\n",
      "  WRB            1.0\n",
      "   ``            1.0\n",
      "    .            1.0\n",
      "    #            1.0\n",
      "   ''            1.0\n",
      "-LRB-            1.0\n",
      "   EX            1.0\n",
      "  WP$            1.0\n",
      "  PDT            1.0\n",
      " PRP$            1.0\n",
      "\n",
      "Comparison of Base vs MAX metrics (10% trimmed means):\n",
      "Base Accuracy vs MAX Accuracy: 0.9872 vs 0.9381\n",
      "Base Confidence vs MAX Confidence: 0.9842 vs 0.1577\n",
      "\n",
      "Classes with biggest difference between Base and MAX Accuracy:\n",
      "Class  Base Accuracy  MAX Accuracy  Accuracy_Diff\n",
      " NNPS         0.9262        0.2746         0.6516\n",
      "  RBR         0.9191        0.5662         0.3529\n",
      "   UH         0.6667        0.3333         0.3334\n",
      "  RBS         0.8857        0.6000         0.2857\n",
      "  JJS         0.9725        0.7527         0.2198\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def parse_table(file_path):\n",
    "    # Read the content from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Extract the table rows\n",
    "    lines = content.strip().split('\\n')\n",
    "\n",
    "    # Find header and data rows\n",
    "    header_row = None\n",
    "    data_rows = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if '+--' in line:\n",
    "            continue\n",
    "        if header_row is None and '|' in line:\n",
    "            header_row = line\n",
    "        elif header_row is not None and '|' in line:\n",
    "            data_rows.append(line)\n",
    "\n",
    "    # Parse header\n",
    "    header_parts = [part.strip() for part in re.split(r'\\s*\\|\\s*', header_row) if part.strip()]\n",
    "\n",
    "    # Parse data rows\n",
    "    parsed_rows = []\n",
    "    for row in data_rows:\n",
    "        parts = [part.strip() for part in re.split(r'\\s*\\|\\s*', row) if part.strip()]\n",
    "        if len(parts) == len(header_parts):\n",
    "            parsed_rows.append(parts)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(parsed_rows, columns=header_parts)\n",
    "\n",
    "    # Convert numeric columns to float\n",
    "    numeric_columns = df.columns[1:]  # All columns except 'Class'\n",
    "    for col in numeric_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "def trimmed_mean(data, proportion=0.1):\n",
    "    \"\"\"Calculate the trimmed mean by removing the specified proportion from both ends.\"\"\"\n",
    "    data = [x for x in data if pd.notna(x)]  # Remove NaN values\n",
    "    if not data:\n",
    "        return np.nan\n",
    "    \n",
    "    n = len(data)\n",
    "    k = int(round(proportion * n))\n",
    "    return np.mean(sorted(data)[k:n-k])\n",
    "\n",
    "def main():\n",
    "    # Parse the table\n",
    "    df = parse_table('paste.txt')\n",
    "    \n",
    "    # Get metrics (all columns except 'Class')\n",
    "    metrics = [col for col in df.columns if col != 'Class']\n",
    "    \n",
    "    # Calculate 10% trimmed mean for each metric across all classes\n",
    "    print(\"10% Trimmed Mean across all classes for each metric:\")\n",
    "    for metric in metrics:\n",
    "        mean_value = trimmed_mean(df[metric].values)\n",
    "        print(f\"{metric}: {mean_value:.4f}\")\n",
    "    \n",
    "    # Print the sorted classes by Base Accuracy for reference\n",
    "    print(\"\\nClasses sorted by Base Accuracy:\")\n",
    "    sorted_df = df.sort_values(by='Base Accuracy', ascending=False)\n",
    "    print(sorted_df[['Class', 'Base Accuracy']].head(10).to_string(index=False))\n",
    "    \n",
    "    # Calculate mean comparison between Base and MAX metrics\n",
    "    print(\"\\nComparison of Base vs MAX metrics (10% trimmed means):\")\n",
    "    print(f\"Base Accuracy vs MAX Accuracy: {trimmed_mean(df['Base Accuracy'].values):.4f} vs {trimmed_mean(df['MAX Accuracy'].values):.4f}\")\n",
    "    print(f\"Base Confidence vs MAX Confidence: {trimmed_mean(df['Base Confidence'].values):.4f} vs {trimmed_mean(df['MAX Confidence'].values):.4f}\")\n",
    "    \n",
    "    # Calculate the difference between Base and MAX for each class\n",
    "    df['Accuracy_Diff'] = df['Base Accuracy'] - df['MAX Accuracy']\n",
    "    \n",
    "    # Print classes with biggest difference in accuracy\n",
    "    print(\"\\nClasses with biggest difference between Base and MAX Accuracy:\")\n",
    "    diff_df = df.sort_values(by='Accuracy_Diff', ascending=False)\n",
    "    print(diff_df[['Class', 'Base Accuracy', 'MAX Accuracy', 'Accuracy_Diff']].head(5).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Tau'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtao_abiliation.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     79\u001b[0m     data_text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 81\u001b[0m trimmed_means \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_trimmed_means\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Print results for each tau value\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tau, table \u001b[38;5;129;01min\u001b[39;00m trimmed_means\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m, in \u001b[0;36mcalculate_trimmed_means\u001b[0;34m(data_text, trim_percent)\u001b[0m\n\u001b[1;32m     65\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tau \u001b[38;5;129;01min\u001b[39;00m tau_values:\n\u001b[0;32m---> 68\u001b[0m     tau_results \u001b[38;5;241m=\u001b[39m trimmed_df[\u001b[43mtrimmed_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTau\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m tau]\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m     tau_results \u001b[38;5;241m=\u001b[39m tau_results\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTau\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Convert to formatted table\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/memit/lib/python3.9/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Tau'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_trimmed_means(data_text, trim_percent=0.1):\n",
    "    # Parse the data from the text\n",
    "    tau_values = []\n",
    "    metrics_data = {}\n",
    "    \n",
    "    lines = data_text.strip().split('\\n')\n",
    "    current_tau = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('For Tau:'):\n",
    "            current_tau = float(line.split(':')[1].strip())\n",
    "            tau_values.append(current_tau)\n",
    "            metrics_data[current_tau] = []\n",
    "        elif line.startswith('|  Class  | Base Accuracy') and current_tau is not None:\n",
    "            # This is the header line, extract column names\n",
    "            headers = [h.strip() for h in line.split('|')[1:-1]]\n",
    "        elif line.startswith('|  Class') and current_tau is not None:\n",
    "            # This is a data line\n",
    "            values = line.split('|')[1:-1]\n",
    "            class_name = values[0].strip()\n",
    "            metrics = [float(v.strip()) for v in values[1:]]\n",
    "            metrics_data[current_tau].append([class_name] + metrics)\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    all_data = []\n",
    "    \n",
    "    for tau in tau_values:\n",
    "        for row in metrics_data[tau]:\n",
    "            all_data.append([tau] + row)\n",
    "    \n",
    "    df = pd.DataFrame(all_data, columns=['Tau', 'Class'] + headers[1:])\n",
    "    \n",
    "    # Calculate trimmed means for each tau value and class\n",
    "    tau_classes = df.groupby(['Tau', 'Class'])\n",
    "    \n",
    "    # List of metric columns\n",
    "    metric_columns = df.columns[2:]\n",
    "    \n",
    "    # Create a new dataframe for trimmed means\n",
    "    trimmed_means = []\n",
    "    \n",
    "    for tau in tau_values:\n",
    "        tau_data = df[df['Tau'] == tau]\n",
    "        \n",
    "        for class_name in tau_data['Class'].unique():\n",
    "            class_data = tau_data[tau_data['Class'] == class_name]\n",
    "            \n",
    "            row = {'Tau': tau, 'Class': class_name}\n",
    "            \n",
    "            for metric in metric_columns:\n",
    "                # Calculate trimmed mean (removing 10% from each end)\n",
    "                values = class_data[metric].values\n",
    "                trimmed_mean = np.mean(np.sort(values)[int(len(values)*trim_percent):int(len(values)*(1-trim_percent))])\n",
    "                row[metric] = trimmed_mean\n",
    "            \n",
    "            trimmed_means.append(row)\n",
    "    \n",
    "    trimmed_df = pd.DataFrame(trimmed_means)\n",
    "    \n",
    "    # Format the results by tau value\n",
    "    results = {}\n",
    "    \n",
    "    for tau in tau_values:\n",
    "        tau_results = trimmed_df[trimmed_df['Tau'] == tau].sort_values('Class')\n",
    "        tau_results = tau_results.drop('Tau', axis=1)\n",
    "        \n",
    "        # Convert to formatted table\n",
    "        table = tabulate(tau_results, headers='keys', tablefmt='grid', floatfmt='.4f')\n",
    "        results[tau] = table\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "with open('tao_abiliation.txt', 'r') as f:\n",
    "    data_text = f.read()\n",
    "\n",
    "trimmed_means = calculate_trimmed_means(data_text)\n",
    "\n",
    "# Print results for each tau value\n",
    "for tau, table in trimmed_means.items():\n",
    "    print(f\"\\nFor Tau: {tau}\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tau  Base Accuracy  Base Confidence  Base Complement Acc  \\\n",
      "0  1.0          0.945           0.9337                0.945   \n",
      "1  1.3          0.945           0.9337                0.945   \n",
      "2  1.6          0.945           0.9337                0.945   \n",
      "3  1.9          0.945           0.9337                0.945   \n",
      "4  2.2          0.945           0.9337                0.945   \n",
      "5  2.5          0.945           0.9337                0.945   \n",
      "6  2.8          0.945           0.9337                0.945   \n",
      "7  3.1          0.945           0.9337                0.945   \n",
      "8  3.4          0.945           0.9337                0.945   \n",
      "9  3.7          0.945           0.9337                0.945   \n",
      "\n",
      "   Base Compliment Conf  STD Accuracy  STD Confidence  STD compliment ACC  \\\n",
      "0                0.9337        0.7632          0.7281              0.9375   \n",
      "1                0.9337        0.7124          0.6767              0.9338   \n",
      "2                0.9337        0.6684          0.6350              0.9337   \n",
      "3                0.9337        0.6337          0.6062              0.9326   \n",
      "4                0.9337        0.6158          0.5901              0.9323   \n",
      "5                0.9337        0.6076          0.5830              0.9311   \n",
      "6                0.9337        0.6007          0.5783              0.9264   \n",
      "7                0.9337        0.5972          0.5783              0.9221   \n",
      "8                0.9337        0.5964          0.5788              0.9141   \n",
      "9                0.9337        0.5973          0.5796              0.9080   \n",
      "\n",
      "   STD compliment Conf  \n",
      "0               0.9232  \n",
      "1               0.9144  \n",
      "2               0.9066  \n",
      "3               0.8967  \n",
      "4               0.8882  \n",
      "5               0.8834  \n",
      "6               0.8791  \n",
      "7               0.8776  \n",
      "8               0.8741  \n",
      "9               0.8723  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def parse_data(text):\n",
    "    # Find all tau sections\n",
    "    tau_sections = re.split(r'Tao:\\s+', text)[1:]  # Skip the first empty element\n",
    "    \n",
    "    # Initialize dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    for section in tau_sections:\n",
    "        # Extract tau value\n",
    "        tau_match = re.match(r'(\\d+\\.?\\d*)', section)\n",
    "        if not tau_match:\n",
    "            continue\n",
    "        \n",
    "        tau = float(tau_match.group(1))\n",
    "        \n",
    "        # Extract class data\n",
    "        class_data = []\n",
    "        for class_idx in range(4):\n",
    "            pattern = r'Class ' + str(class_idx) + r'\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+\\|\\s+([\\d\\.]+)\\s+'\n",
    "            match = re.search(pattern, section)\n",
    "            \n",
    "            if match:\n",
    "                row_data = [float(match.group(i)) for i in range(1, 9)]\n",
    "                class_data.append(row_data)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if class_data:\n",
    "            results[tau] = np.array(class_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_averages_table(parsed_data):\n",
    "    # Column names\n",
    "    columns = [\n",
    "        'Base Accuracy', 'Base Confidence', 'Base Complement Acc', 'Base Compliment Conf',\n",
    "        'STD Accuracy', 'STD Confidence', 'STD compliment ACC', 'STD compliment Conf'\n",
    "    ]\n",
    "    \n",
    "    # Calculate averages for each tau value\n",
    "    results = []\n",
    "    for tau, data in sorted(parsed_data.items()):\n",
    "        row = [tau]\n",
    "        row.extend([np.mean(data[:, i]) for i in range(data.shape[1])])\n",
    "        results.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results, columns=['Tau'] + columns)\n",
    "    return df.round(4)\n",
    "\n",
    "# Read data from file\n",
    "with open('tao_abiliation.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Parse the data\n",
    "parsed_data = parse_data(data)\n",
    "\n",
    "# Create and display the averages table\n",
    "result_table = create_averages_table(parsed_data)\n",
    "print(result_table)\n",
    "\n",
    "# Save to CSV\n",
    "result_table.to_csv('tau_averages.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
